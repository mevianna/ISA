# English Version

## General Ideas

Naive Bayes is a **supervised** machine learning algorithm used for **classification** [1].

> [!NOTE]
> - **Supervised learning** learns from data that already includes the correct answers (also called labeled examples). Based on these, it makes predictions about new data [3].
>
> - **Regression** problems predict numbers, like the price of a house. **Classification** problems are those that predict "labels" or "categories," such as whether an email fits into the "spam" or "not spam" category [3].

Naive Bayes makes predictions using probabilities learned from the training data. It estimates the probability of each class and the probability of each attribute appearing within each class. Then, to classify a new instance, it calculates the probability of it belonging to each class and chooses the most likely one [1]. To see this working in practice, let's go to an example.

### Example: Spam Email Classification

Let's use a classic example of email classification. Imagine that, during training with a large dataset, the algorithm observed the following:
- **Class probabilities:** 30% of emails are "spam" and 70% are "normal".
- **Attribute probabilities per class:**
    - The word "make" accounts for 24% of all words in spam emails, but only 5% of words in normal emails.
    - The word "money" accounts for 20% of all words in spam emails, but only 5% of words in normal emails.

Now, when a new email arrives with the text **"make money"**, Naive Bayes will use these probabilities to decide if it is spam or not:

- **It calculates the probability of a new instance belonging to each possible class based on its attributes**:
    - **It calculates the probability** of it being spam based on the words present.
    - **It calculates the probability** of it being a normal email based on the same words.
- **It compares the two** and chooses the class with the higher probability.

Since the words "make" and "money" are much more common in spam than in normal emails, the algorithm would classify this email as **SPAM**. Let's see how this works mathematically.

### The Naive Bayes Formula

Naive Bayes is based on **Bayes' Theorem** to calculate probabilities. To simplify, let's start with the simplest case, where we analyze only **one attribute**, such as an email containing only the word "make". Its formula is [1]:

$$P(C|X) = \frac{P(X|C) \times P(C)}{P(X)}$$

Where [1]:
- $P(C|X)$ - **Posterior Probability**: is the probability **that** an instance belongs to class $C$, given its observed attributes $X$. This is the value we want to calculate to make the classification.

- $P(X|C)$ - **Likelihood**: is the probability of observing the attributes $X$ in instances that we know belong to class $C$. It measures how "typical" or "likely" these attributes are for the class.
  
- $P(C)$ - **Prior Probability (Prior)**: is the overall probability of an instance belonging to class $C$. It is our knowledge **before** observing any specific attributes.
  
- $P(X)$ - **Evidence**: is the total probability of the attributes $X$ appearing, regardless of the class.

During training, the algorithm calculates $P(X|C)$ by observing how often each attribute appears in each class, and $P(C)$ by counting the proportion of each class in the dataset. $P(X)$ represents the total probability of the attribute appearing, but as we will see next, we don't need to calculate it to perform the classification.

### Numerical example: The word "make"

* First, let's list the values we already know:

**Prior probabilities**
- $P(\text{spam}) = 0.3$ (30% of words come from spam emails)
- $P(\text{normal}) = 0.7$ (70% of words come from normal emails)

**Likelihoods**
- $P(\text{"make"}|\text{spam}) = 0.24$ (24% of words in spam emails are "make")
- $P(\text{"make"}|\text{normal}) = 0.05$ (5% of words in normal emails are "make")

We want to calculate the posterior probability: is an email with only the word "make" spam or normal?

**Posterior for SPAM:**

$$P(\text{spam}|\text{"make"}) = \frac{P(\text{"make"}|\text{spam}) \times P(\text{spam})}{P(\text{"make"})}$$

$$P(\text{spam}|\text{"make"}) = \frac{0.24 \times 0.3}{P(\text{"make"})} = \frac{0.072}{P(\text{"make"})}$$

**Posterior for NORMAL:**

$$P(\text{normal}|\text{"make"}) = \frac{P(\text{"make"}|\text{normal}) \times P(\text{normal})}{P(\text{"make"})}$$

$$P(\text{normal}|\text{"make"}) = \frac{0.05 \times 0.7}{P(\text{"make"})} = \frac{0.035}{P(\text{"make"})}$$

To classify the email, we need to determine which of these values is greater: $P(\text{spam}|\text{"make"})$ or $P(\text{normal}|\text{"make"})$.

Note that the denominator $P(\text{"make"})$ is the same in both fractions. To find out which fraction is larger, we just need to compare the numerators $\text{Likelihood} \times \text{Prior}$: $0.072$ and $0.035$.

Since $0.072$ is greater than $0.035$, the probability for the **SPAM** class is higher, and the algorithm classifies the email as such.

### The Bayesian Process

Now that we have seen a concrete example, we can understand the **belief updating** process that Naive Bayes implements [2]:

1.  **Prior (Initial Knowledge)**: We start with expectations based on the frequency of classes in the training data (30% spam, 70% normal).
2.  **Likelihood (Observation)**: We measure how typical the observed attributes are for each class.
3.  **Posterior (Updated Belief)**: We combine the prior and likelihood to arrive at the final probability.
4.  **Decision**: We choose the class with the highest posterior probability.

### Classifier's formula

Therefore, the classification comes down to [2]:

$$\hat{C} = \arg\max_C [P(X|C) \times P(C)]$$

Or, using Bayesian terminology:

$$\hat{C} = \arg\max_C [\text{Likelihood} \times \text{Prior}]$$

Where $\hat{C}$ is the chosen class and $\arg\max_C$ means "find the class $C$ that maximizes the expression".

> [!NOTE]
> **What is the denominator P(X) for [1]?**
>
> You may have noticed that we ignored $P(\text{"make"})$ in the calculations. Since it is the same for all classes, it does not change which class will have the highest score. Therefore, for the simple task of *classification*, we can omit it.
>
> However, its role is that of a **normalization factor**. It would be necessary if we wanted to know the exact probability (a value between 0 and 1).

### Dealing with multiple attributes

When we have multiple attributes (like several words in an email), **Bayes' Theorem** expands to [2]:

$$P(C|X_1, X_2, ..., X_n) = \frac{P(C) \cdot P(X_1, X_2, ..., X_n|C)}{P(X_1, X_2, ..., X_n)}$$

- **Problem**: Calculating $P(X_1, X_2, ..., X_n|C)$ requires observing all possible combinations of attributes. For example, to find the joint probability of the words "make" AND "money" appearing together in spam emails, we would need specific data about this exact combination. With vocabularies of thousands of words, this becomes computationally infeasible [2].

- **The "naive" assumption**: To get around this problem, Naive Bayes assumes that the attributes are independent of each other. This means that the presence of one attribute does not influence the probability of another attribute appearing. Mathematically [2]:

$$P(C|X_1, X_2, ..., X_n) = \frac{P(C) \times \prod_{i=1}^{n} P(X_i|C)}{P(X_1, X_2, ..., X_n)}$$

Where the symbol $\prod$ (product) means that we multiply the probabilities of each individual attribute given the class C.

Therefore, the classifier becomes:

$$\hat{C} = \arg\max_C \left[ P(C) \times \prod_{i=1}^{n} P(X_i|C) \right]$$

Or, in other words:

$$(\text{chosen class}) = \arg\max_C \left[ \text{Prior} \times \left(\prod \text{Likelihoods}\right) \right]$$

>[!NOTE]
> - **What does independence mean [2]?** >
> - Two events are **independent** if the occurrence of one does not affect the probability of the other. For example, the outcome of a coin toss and tomorrow's temperature are independent.
>
> - On the other hand, many variables are **dependent**. For example, "being cloudy" and "raining" are dependent; if it is cloudy, the chance of rain increases.

> [!IMPORTANT]
> The independence assumption is not true in many problems; in the example, the presence of the word "make" may increase the chance of "money" appearing, but it greatly simplifies the algorithm and works very well in practice [4]. The name **Naive** for the algorithm comes from this simplification.

### Practical example: "make money"

Let's apply our formula to the email containing the words "make" and "money". Using the data from our previous example:

**For the SPAM class (Prior × Likelihoods):**

$$P(\text{spam}) \cdot P(\text{"make"}|\text{spam}) \cdot P(\text{"money"}|\text{spam})$$

$$= 0.3 \times 0.24 \times 0.20 = 0.0144$$

**For the NORMAL class (Prior × Likelihoods):**

$$P(\text{normal}) \cdot P(\text{"make"}|\text{normal}) \cdot P(\text{"money"}|\text{normal})$$

$$= 0.7 \times 0.05 \times 0.05 = 0.00175$$

Since $0.0144 > 0.00175$, the algorithm classifies the email as **SPAM**.

### Avoiding Underflow: Log-Probabilities

In practice, when there are many attributes (e.g., thousands of words), multiplying probabilities (values between 0 and 1) results in extremely small numbers, causing numerical errors (underflow). To solve this, we work with the logarithms of the probabilities instead of the direct probabilities [2].

Since the logarithm is a **monotonically increasing function** (that is, if A is greater than B, then $\log(A)$ will also be greater than $\log(B)$), finding the class with the highest log-probability is the same as finding the class with the highest probability.

The classifier's formula becomes [2]:

$$\hat{C} = \arg\max_C \left[\log P(C) + \sum_{i=1}^{n} \log P(X_i|C)\right]$$

Where the product $(\prod)$ becomes a summation $(\sum)$ due to the property: $\log(a \times b) = \log(a) + \log(b)$.

Example with our data:

**SPAM**: $\log(0.3) + \log(0.24) + \log(0.20) = -1.204 + (-1.427) + (-1.609) = -4.240$

**NORMAL**: $\log(0.7) + \log(0.05) + \log(0.05) = -0.357 + (-2.996) + (-2.996) = -6.349$

Since $-4.240 > -6.349$, we still classify it as **SPAM**.

## References

[1] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

[2] Izbicki, R., & Santos, T. M. (2020). *Aprendizado de máquina: uma abordagem estatística*. (1st ed.). Rafael Izbicki.

[3] Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

[4] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.

# Portuguese Version

## Ideias Gerais

Naive Bayes é um algoritmo de aprendizado de máquina **supervisionado** usado para **classificação** [1].

> [!NOTE]
> - O **aprendizado supervisionado** aprende a partir de dados que já incluem as respostas corretas (também chamados de exemplos rotulados). Com base nesses, ele faz previsões sobre novos dados [3].
>
> - Os problemas de **regressão** preveem números, como o preço de uma casa. Os de **classificação** são aqueles que preveem “rótulos” ou “categorias”, como se um email se encaixa na categoria “spam” ou “não spam” [3].


O Naive Bayes faz previsões usando probabilidades aprendidas dos dados de treinamento. Ele descobre a chance (probabilidade) de cada classe existir e a chance de cada atributo  aparecer em cada classe. Depois, para classificar uma nova instância, ele calcula a probabilidade de pertencer a cada classe e escolhe a mais provável [1]. Pra ver isso funcionando na prática, vamos a um exemplo.

### Exemplo: Classificação de Emails Spam

Vamos usar um exemplo clássico de classificação de emails. Imagine que, durante o treinamento com um grande conjunto de dados, o algoritmo observou o seguinte:
- **Probabilidade das classes:** 30% dos emails são "spam" e 70% são "normal".
- **Probabilidade dos atributos por classe:**
    - A palavra "ganhe" representa 24% de todas as palavras em emails spam, mas apenas 5% das palavras em emails normais.
    - A palavra "dinheiro" representa 20% de todas as palavras em emails spam, mas apenas 5% das palavras em emails normais.

Agora, quando um novo email chega com o texto **"ganhe dinheiro"**, o Naive Bayes usará essas probabilidades para decidir se é spam ou não:

- **Calcula a probabilidade de uma nova instância pertencer a cada classe possível baseado em seus atributos**:
    - **Calcula a probabilidade** de ser spam baseado nas palavras presentes.
    - **Calcula a probabilidade** de ser email normal baseado nas mesmas palavras.

- **Compara as duas** e escolhe a classe com maior probabilidade.

Como as palavras "ganhe" e "dinheiro" são muito mais comuns em spam do que em emails normais, o algoritmo classificaria este email como **SPAM**. Vamos ver como isso funciona matematicamente.

### A Fórmula do Naive Bayes

O Naive Bayes se fundamenta no **Teorema de Bayes** para calcular as probabilidades. Para simplificar, vamos começar com o caso mais simples, onde analisamos apenas **um atributo**, como um email que contém apenas a palavra "ganhe". Sua fórmula é [1]:

$$P(C|X) = \frac{P(X|C) \times P(C)}{P(X)}$$

Onde [1]:
- $P(C|X)$ - **Probabilidade A Posteriori**: é probabilidade de uma instância pertencer a uma classe $C$, dado que observamos seus atributos $X$. Este é o valor que queremos calcular para fazer a classificação.

- $P(X|C)$ - **Verossimilhança**: é a probabilidade de observarmos os atributos $X$ em instâncias que sabemos pertencer à classe $C$. Mede quão "típicos" ou "verossímeis" são esses atributos para a classe.
  
- $P(C)$ - **Probabilidade A Priori (Prior)**: é a probabilidade geral de uma instância pertencer à classe $C$. É nosso conhecimento **antes** de observar qualquer atributo específico.
  
- $P(X)$ - **Evidência**: é a probabilidade total dos atributos $X$ aparecerem, independente da classe.

Durante o treinamento, o algoritmo calcula $P(X|C)$ observando com que frequência cada atributo aparece em cada classe, e $P(C)$ contando a proporção de cada classe no dataset. Já $P(X)$ representa a probabilidade total do atributo aparecer, mas como veremos a seguir, não precisamos calculá-lo para fazer a classificação.

### Exemplo numérico: A palavra "ganhe"

* Primeiro, vamos listar os valores que já conhecemos:

**Probabilidades a priori**
- $P(\text{spam}) = 0.3$ (30% das palavras vêm de emails spam)
- $P(\text{normal}) = 0.7$ (70% das palavras vêm de emails normais)

**Verossimilhanças**
- $P(\text{"ganhe"}|\text{spam}) = 0.24$ (24% das palavras em emails spam são "ganhe")
- $P(\text{"ganhe"}|\text{normal}) = 0.05$ (5% das palavras em emails normais são "ganhe")

Queremos calcular a probabilidade a posteriori: um email com apenas a palavra "ganhe", é spam ou normal?

**Posterior para SPAM:**

$$P(\text{spam}|\text{"ganhe"}) = \frac{P(\text{"ganhe"}|\text{spam}) \times P(\text{spam})}{P(\text{"ganhe"})}$$

$$P(\text{spam}|\text{"ganhe"}) = \frac{0.24 \times 0.3}{P(\text{"ganhe"})} = \frac{0.072}{P(\text{"ganhe"})}$$

**Posterior para NORMAL:**

$$P(\text{normal}|\text{"ganhe"}) = \frac{P(\text{"ganhe"}|\text{normal}) \times P(\text{normal})}{P(\text{"ganhe"})}$$

$$P(\text{normal}|\text{"ganhe"}) = \frac{0.05 \times 0.7}{P(\text{"ganhe"})} = \frac{0.035}{P(\text{"ganhe"})}$$

Para classificar o email, precisamos determinar qual destes valores é maior: $P(\text{spam}|\text{"ganhe"})$ ou $P(\text{normal}|\text{"ganhe"})$.

Observe que o denominador $P(\text{"ganhe"})$ é o mesmo nas duas frações. Para descobrir qual fração é maior, basta comparar os numeradores $\text{Verossimilhança} \times \text{Prior}$: $0.072$ e $0.035$.

Como $0.072$ é maior que $0.035$, a probabilidade da classe **SPAM** é maior, e o algoritmo classifica o email como tal.

### O Processo Bayesiano

Agora que vimos um exemplo concreto, podemos entender o processo de **atualização de crenças** que o Naive Bayes implementa [2]:

1. **Prior (Conhecimento Inicial)**: Começamos com expectativas baseadas na frequência das classes no treinamento (30% spam, 70% normal)
2. **Verossimilhança (Observação)**: Medimos quão típicos são os atributos observados para cada classe 
3. **Posterior (Crença Atualizada)**: Combinamos prior e verossimilhança para chegar à probabilidade final
4. **Decisão**: Escolhemos a classe com maior probabilidade a posteriori

   
### Fórmula do classificador

Portanto a classificação se resume a [2]:

$$\hat{C} = \arg\max_C [P(X|C) \times P(C)]$$

Ou, usando a terminologia bayesiana:

$$\hat{C} = \arg\max_C [\text{Verossimilhança} \times \text{Prior}]$$

Onde $\hat{C}$ é a classe escolhida e $\arg\max_C$ significa "encontre a classe $C$ que maximiza a expressão".

> [!NOTE]
> **Para que serve o denominador P(X) [1]?**
>
> Você deve ter notado que ignoramos o $P(\text{"ganhe"})$ nos cálculos. Como ele é o mesmo para todas as classes, ele não altera qual classe terá a maior pontuação. Por isso, para a simples tarefa de *classificação*, podemos omiti-lo.
>
> No entanto, seu papel é o de um **fator de normalização**. Seria necessário se quiséssemos saber a probabilidade exata (um valor entre 0 e 1)



### Lidando com múltiplos atributos

Quando temos múltiplos atributos (como várias palavras em um email), o **Teorema de Bayes** se expande para [2]:

$$P(C|X_1, X_2, ..., X_n) = \frac{P(C) \cdot P(X_1, X_2, ..., X_n|C)}{P(X_1, X_2, ..., X_n)}$$

- **Problema**:  Calcular $P(X_1, X_2, ..., X_n|C)$ requer observar todas as possíveis combinações de atributos. Por exemplo, para descobrir a probabilidade conjunta de as palavras "ganhe" E "dinheiro" aparecerem juntas em emails spam, precisaríamos de dados específicos sobre essa combinação exata. Com vocabulários de milhares de palavras, isso se torna computacionalmente inviável [2].

- **A suposição "ingênua"**: Para contornar esse problema, o Naive Bayes assume que os atributos são independentes entre si. Isso significa que a presença de um atributo não influencia a probabilidade de outro atributo aparecer. Matematicamente [2]:

$$P(C|X_1, X_2, ..., X_n) = \frac{P(C) \times \prod_{i=1}^{n} P(X_i|C)}{P(X_1, X_2, ..., X_n)}$$

Onde o símbolo $\prod$ (produtório) significa que multiplicamos as probabilidades de cada atributo individual dado a classe C.

Portanto, o classificador se torna:

$$\hat{C} = \arg\max_C [P(C) \times \prod_{i=1}^{n} P(X_i|C)]$$

Ou, em outras palavras:

$$\hat{C} = \arg\max_C \left[ \text{Prior} \times \left(\prod \text{Verossimilhanças}\right) \right]$$

>[!NOTE]
> - **O que significa independência [2]?** 
>
> -  Dois eventos são **independentes** se a ocorrência de um não afeta a probabilidade do outro. Por exemplo, o resultado de um lançamento de moeda e a temperatura de amanhã são independentes.
>
> - Por outro lado, muitas variáveis são **dependentes**. Por exemplo, "estar nublado" e "chover" são dependentes, se está nublado, aumenta a chance de chover.


> [!IMPORTANT]
> A suposição de independência não é verdadeira em muitos problemas, no exemplo, a presença da palavra "ganhe" pode aumentar a chance de "dinheiro" aparecer, mas simplifica muito o algoritmo e funciona muito bem na prática [2]. O Nome **Naive** (ingênuo) para o algoritmo vem dessa simplificação.

### Exemplo prático: "ganhe dinheiro"

Vamos aplicar nossa fórmula ao email que contém as palavras "ganhe" e "dinheiro". Usando os dados do nosso exemplo anterior:

**Para a classe SPAM (Prior × Verossimilhanças):**

$$P(\text{spam}) \cdot P(\text{"ganhe"}|\text{spam}) \cdot P(\text{"dinheiro"}|\text{spam})$$

$$= 0.3 \times 0.24 \times 0.20 = 0.0144$$

**Para a classe NORMAL (Prior × Verossimilhanças):**

$$P(\text{normal}) \cdot P(\text{"ganhe"}|\text{normal}) \cdot P(\text{"dinheiro"}|\text{normal})$$

$$= 0.7 \times 0.05 \times 0.05 = 0.00175$$

Como $0.0144 > 0.00175$, o algoritmo classifica o email como **SPAM**.

### Evitando Underflow: Log-Probabilidades

Na prática, quando há muitos atributos (ex: milhares de palavras), multiplicar probabilidades (valores entre 0 e 1) resulta em números extremamente pequenos, causando erros numéricos (underflow). Para resolver isso, trabalhamos com logaritmos das probabilidades ao invés das probabilidades diretas [2].

Como o logaritmo é uma função **monótona crescente** (isto é, se a A é maior que B, então $\log(A)$ também será maior que $\log(B))$, encontrar a classe com maior log-probabilidade é o mesmo que encontrar a classe com maior probabilidade.

A fórmula do classificador se torna [2]:

$$\hat{C} = \arg\max_C [\log P(C) + \sum_{i=1}^{n} \log P(X_i|C)]$$

Onde o produtório $(\prod)$ se transforma em um somatório $(\sum)$ devido à propriedade: $\log(a \times b) = \log(a) + \log(b)$.

Exemplo com nossos dados:

**SPAM**: $\log(0.3) + \log(0.24) + \log(0.20) = -1.204 + (-1.427) + (-1.609) = -4.240$

**NORMAL**: $\log(0.7) + \log(0.05) + \log(0.05) = -0.357 + (-2.996) + (-2.996) = -6.349$

Como $-4.240 > -6.349$, ainda classificamos como **SPAM**.

## Referencias

[1] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

[2] Izbicki, R., & Santos, T. M. (2020). *Aprendizado de máquina: uma abordagem estatística*. (1st ed.). Rafael Izbicki.

[3] Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

[4] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.

