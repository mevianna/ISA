# English Version

# Portuguese Version

## Ideias Gerais

Naive Bayes é um algoritmo de aprendizado de máquina **supervisionado** usado para **classificação** [1].

> [!NOTE]
> - O **aprendizado supervisionado** aprende a partir de dados que já incluem as respostas corretas (também chamados de exemplos rotulados). Com base nesses, ele faz previsões sobre novos dados.
>
> - Os problemas de **regressão** preveem números, como o preço de uma casa. Os de **classificação** são aqueles que preveem “rótulos” ou “categorias”, como se um email se encaixa na categoria “spam” ou “não spam”.


O Naive Bayes faz previsões usando probabilidades aprendidas dos dados de treinamento. Ele descobre a chance (probabilidade) de cada classe existir e a chance de cada atributo  aparecer em cada classe. Depois, para classificar uma nova instância, ele calcula a probabilidade de pertencer a cada classe e escolhe a mais provável [1]. Pra ver isso funcionando na prática, vamos a um exemplo.

### Exemplo: Classificação de Emails Spam

Vamos usar um exemplo clássico de classificação de emails. Imagine que, durante o treinamento com um grande conjunto de dados, o algoritmo observou o seguinte:
- **Probabilidade das classes:** 30% dos emails são "spam" e 70% são "normal".
- **Probabilidade dos atributos por classe:**
    - A palavra "ganhe" representa 24% de todas as palavras em emails spam, mas apenas 5% das palavras em emails normais.
    - A palavra "dinheiro" representa 20% de todas as palavras em emails spam, mas apenas 5% das palavras em emails normais.

Agora, quando um novo email chega com o texto **"ganhe dinheiro"**, o Naive Bayes usará essas probabilidades para decidir se é spam ou não:

- **Calcula a probabilidade de uma nova instância pertencer a cada classe possível baseado em seus atributos**:
    - **Calcula a probabilidade** de ser spam baseado nas palavras presentes.
    - **Calcula a probabilidade** de ser email normal baseado nas mesmas palavras.

- **Compara as duas** e escolhe a classe com maior probabilidade.

Como as palavras "ganhe" e "dinheiro" são muito mais comuns em spam do que em emails normais, o algoritmo classificaria este email como **SPAM**. Vamos ver como isso funciona matematicamente.

### A Fórmula do Naive Bayes

O Naive Bayes se fundamenta no **Teorema de Bayes** para calcular as probabilidades. Para simplificar, vamos começar com o caso mais simples, onde analisamos apenas **um atributo**, como um email que contém apenas a palavra "ganhe". Sua fórmula é [1]:

$$P(C|X) = \frac{P(X|C) \times P(C)}{P(X)}$$

Onde:
- $P(C|X)$ - **Probabilidade A Posteriori**: é probabilidade de uma instância pertencer a uma classe $C$, dado que observamos seus atributos $X$. Este é o valor que queremos calcular para fazer a classificação.

- $P(X|C)$ - **Verossimilhança**: é a probabilidade de observarmos os atributos $X$ em instâncias que sabemos pertencer à classe $C$. Mede quão "típicos" ou "verossímeis" são esses atributos para a classe.
  
- $P(C)$ - **Probabilidade A Priori (Prior)**: é a probabilidade geral de uma instância pertencer à classe $C$. É nosso conhecimento **antes** de observar qualquer atributo específico.
  
- $P(X)$ - **Evidência**: é a probabilidade total dos atributos $X$ aparecerem, independente da classe.

Durante o treinamento, o algoritmo calcula $P(X|C)$ observando com que frequência cada atributo aparece em cada classe, e $P(C)$ contando a proporção de cada classe no dataset. Já $P(X)$ representa a probabilidade total do atributo aparecer, mas como veremos a seguir, não precisamos calculá-lo para fazer a classificação.

### Exemplo numérico: A palavra "ganhe"

* Primeiro, vamos listar os valores que já conhecemos:

**Probabilidades a priori**
- $P(\text{spam}) = 0.3$ (30% das palavras vêm de emails spam)
- $P(\text{normal}) = 0.7$ (70% das palavras vêm de emails normais)

**Verossimilhanças**
- $P(\text{"ganhe"}|\text{spam}) = 0.24$ (24% das palavras em emails spam são "ganhe")
- $P(\text{"ganhe"}|\text{normal}) = 0.05$ (5% das palavras em emails normais são "ganhe")

Queremos calcular a probabilidade a posteriori: um email com apenas a palavra "ganhe", é spam ou normal?

**Posterior para SPAM:**

$$P(\text{spam}|\text{"ganhe"}) = \frac{P(\text{"ganhe"}|\text{spam}) \times P(\text{spam})}{P(\text{"ganhe"})}$$

$$P(\text{spam}|\text{"ganhe"}) = \frac{0.24 \times 0.3}{P(\text{"ganhe"})} = \frac{0.072}{P(\text{"ganhe"})}$$

**Posterior para NORMAL:**

$$P(\text{normal}|\text{"ganhe"}) = \frac{P(\text{"ganhe"}|\text{normal}) \times P(\text{normal})}{P(\text{"ganhe"})}$$

$$P(\text{normal}|\text{"ganhe"}) = \frac{0.05 \times 0.7}{P(\text{"ganhe"})} = \frac{0.035}{P(\text{"ganhe"})}$$

Para classificar o email, precisamos determinar qual destes valores é maior: $P(\text{spam}|\text{"ganhe"})$ ou $P(\text{normal}|\text{"ganhe"})$.

Observe que o denominador $P(\text{"ganhe"})$ é o mesmo nas duas frações. Para descobrir qual fração é maior, basta comparar os numeradores $\text{Verossimilhança} \times \text{Prior}$: $0.072$ e $0.035$.

Como $0.072$ é maior que $0.035$, a probabilidade da classe **SPAM** é maior, e o algoritmo classifica o email como tal.

### O Processo Bayesiano

Agora que vimos um exemplo concreto, podemos entender o processo de **atualização de crenças** que o Naive Bayes implementa [2]:

1. **Prior (Conhecimento Inicial)**: Começamos com expectativas baseadas na frequência das classes no treinamento (30% spam, 70% normal)
2. **Verossimilhança (Observação)**: Medimos quão típicos são os atributos observados para cada classe 
3. **Posterior (Crença Atualizada)**: Combinamos prior e verossimilhança para chegar à probabilidade final
4. **Decisão**: Escolhemos a classe com maior probabilidade a posteriori

   
### Fórmula do classificador

Portanto a classificação se resume a [2]:

$$\hat{C} = \arg\max_C [P(X|C) \times P(C)]$$

Ou, usando a terminologia bayesiana:

$$\hat{C} = \arg\max_C [\text{Verossimilhança} \times \text{Prior}]$$

Onde $\hat{C}$ é a classe escolhida e $\arg\max_C$ significa "encontre a classe $C$ que maximiza a expressão".

> [!NOTE]
> **Para que serve o denominador P(X) [1]?**
>
> Você deve ter notado que ignoramos o $P(\text{"ganhe"})$ nos cálculos. Como ele é o mesmo para todas as classes, ele não altera qual classe terá a maior pontuação. Por isso, para a simples tarefa de *classificação*, podemos omiti-lo.
>
> No entanto, seu papel é o de um **fator de normalização**. Seria necessário se quiséssemos saber a probabilidade exata (um valor entre 0 e 1)



### Lidando com múltiplos atributos

Quando temos múltiplos atributos (como várias palavras em um email), o **Teorema de Bayes** se expande para [2]:

$$P(C|X_1, X_2, ..., X_n) = \frac{P(C) \cdot P(X_1, X_2, ..., X_n|C)}{P(X_1, X_2, ..., X_n)}$$

- **Problema**:  Calcular $P(X_1, X_2, ..., X_n|C)$ requer observar todas as possíveis combinações de atributos. Por exemplo, para descobrir a probabilidade conjunta de as palavras "ganhe" E "dinheiro" aparecerem juntas em emails spam, precisaríamos de dados específicos sobre essa combinação exata. Com vocabulários de milhares de palavras, isso se torna computacionalmente inviável [2].

- **A suposição "ingênua"**: Para contornar esse problema, o Naive Bayes assume que os atributos são independentes entre si. Isso significa que a presença de um atributo não influencia a probabilidade de outro atributo aparecer. Matematicamente [2]:

$$P(C|X_1, X_2, ..., X_n) = \frac{P(C) \times \prod_{i=1}^{n} P(X_i|C)}{P(X_1, X_2, ..., X_n)}$$

Onde o símbolo $\prod$ (produtório) significa que multiplicamos as probabilidades de cada atributo individual dado a classe C.

Portanto, o classificador se torna:

$$\hat{C} = \arg\max_C [P(C) \times \prod_{i=1}^{n} P(X_i|C)]$$

>[!NOTE]
> - **O que significa independência [2]?** 
>
> -  Dois eventos são **independentes** se a ocorrência de um não afeta a probabilidade do outro. Por exemplo, o resultado de um lançamento de moeda e a temperatura de amanhã são independentes.
>
> - Por outro lado, muitas variáveis são **dependentes**. Por exemplo, "estar nublado" e "chover" são dependentes, se está nublado, aumenta a chance de chover.


> [!IMPORTANT]
> A suposição de independência não é verdadeira em muitos problemas, no exemplo, a presença da palavra "ganhe" pode aumentar a chance de "dinheiro" aparecer, mas simplifica muito o algoritmo e funciona muito bem na prática [2]. O Nome **Naive** (ingênuo) para o algoritmo vem dessa simplificação.

### Exemplo prático: "ganhe dinheiro"

Vamos aplicar nossa fórmula ao email que contém as palavras "ganhe" e "dinheiro". Usando os dados do nosso exemplo anterior:

**Para a classe SPAM (Prior × Verossimilhanças):**

$$P(\text{spam}) \cdot P(\text{"ganhe"}|\text{spam}) \cdot P(\text{"dinheiro"}|\text{spam})$$

$$= 0.3 \times 0.24 \times 0.20 = 0.0144$$

**Para a classe NORMAL (Prior × Verossimilhanças):**

$$P(\text{normal}) \cdot P(\text{"ganhe"}|\text{normal}) \cdot P(\text{"dinheiro"}|\text{normal})$$

$$= 0.7 \times 0.05 \times 0.05 = 0.00175$$

Como $0.0144 > 0.00175$, o algoritmo classifica o email como **SPAM**.

### Evitando Underflow: Log-Probabilidades

Na prática, quando há muitos atributos (ex: milhares de palavras), multiplicar probabilidades (valores entre 0 e 1) resulta em números extremamente pequenos, causando erros numéricos (underflow). Para resolver isso, trabalhamos com logaritmos das probabilidades ao invés das probabilidades diretas [2].

Como o logaritmo é uma função **monótona crescente** (isto é, se a A é maior que B, então $\log(A)$ também será maior que $\log(B))$, encontrar a classe com maior log-probabilidade é o mesmo que encontrar a classe com maior probabilidade.

A fórmula do classificador se torna [2]:

$$\hat{C} = \arg\max_C [\log P(C) + \sum_{i=1}^{n} \log P(X_i|C)]$$

Onde o produtório $(\prod)$ se transforma em um somatório $(\sum)$ devido à propriedade: $\log(a \times b) = \log(a) + \log(b)$.

Exemplo com nossos dados:

**SPAM**: $\log(0.3) + \log(0.24) + \log(0.20) = -1.204 + (-1.427) + (-1.609) = -4.240$

**NORMAL**: $\log(0.7) + \log(0.05) + \log(0.05) = -0.357 + (-2.996) + (-2.996) = -6.349$

Como $-4.240 > -6.349$, ainda classificamos como **SPAM**.

## Referencias

[1] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

[2] Izbicki, R., & Santos, T. M. (2020). *Aprendizado de máquina: uma abordagem estatística*. (1st ed.). Rafael Izbicki.

