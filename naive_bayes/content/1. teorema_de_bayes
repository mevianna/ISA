---

## 1. Teorema de Bayes

O **Teorema de Bayes** é a espinha dorsal do classificador Naive Bayes. Ele descreve a probabilidade de um evento (uma classe) com base no conhecimento prévio de condições que podem estar relacionadas a esse evento (os atributos).

No contexto de aprendizado de máquina, para uma nova instância **A** (ou **x**) com atributos `a₁, ..., aₙ` (ou `x₁, ..., xₙ`), e um conjunto de classes **C**, o teorema é expresso como:

$$
P(\text{classe} \mid A) = \frac{P(A \mid \text{classe}) \times P(\text{classe})}{P(A)}
$$

Onde:
* `P(classe | A)` é a **probabilidade a posteriori** da classe, dado o conjunto de atributos A.
* `P(A | classe)` é a **verossimilhança** (likelihood) do conjunto de atributos A, dada a classe.
* `P(classe)` é a **probabilidade a priori** da classe.
* `P(A)` é a **probabilidade a priori** do conjunto de atributos A (também chamada de evidência).

Alternativamente, para um vetor de covariáveis **x** e uma classe `c ∈ C`:

- **Para x contínuo:**
  $$
  P(Y=c \mid x) = \frac{f(x \mid Y=c)P(Y=c)}{\sum_{s \in C} f(x \mid Y=s)P(Y=s)}
  $$
  Onde `f(x | Y=c)` é a densidade de probabilidade condicional.

- **Para X discreto:**
  $$
  P(Y=c \mid X=x) = \frac{P(X=x \mid Y=c)P(Y=c)}{\sum_{s \in C} P(X=x \mid Y=s)P(Y=s)}
  $$

---

## 2. A Suposição de Independência Condicional

A suposição fundamental e "ingênua" (*naive*) do classificador é que **todos os atributos são condicionalmente independentes**, dada a classe. Isso significa que, uma vez conhecida a classe, o valor de um atributo não oferece informação sobre o valor de outro.

Essa suposição simplifica drasticamente o cálculo da verossimilhança `P(a₁, ..., aₙ | classe)`:

$$
P(a_1, \dots, a_n \mid \text{classe}) = P(a_1 \mid \text{classe}) \times P(a_2 \mid \text{classe}) \times \dots \times P(a_n \mid \text{classe}) = \prod_{i=1}^{n} P(a_i \mid \text{classe})
$$

Para covariáveis `x = (x₁, ..., xₙ)`:
- **Se contínuas:** $f(x \mid Y=c) = \prod_{j=1}^{d} f(x_j \mid Y=c)$
- **Se discretas:** $P(X=x \mid Y=c) = \prod_{j=1}^{d} P(X_j=x_j \mid Y=c)$

Embora essa suposição seja raramente verdadeira em problemas reais, o classificador Naive Bayes frequentemente apresenta bom desempenho.

---

## 3. Formulação do Classificador

O objetivo é encontrar a classe mais provável para uma nova instância, selecionando a classe que maximiza a probabilidade a posteriori (**MAP** - Maximum A Posteriori).

$$
\text{classe}_{\text{MAP}} = \arg\max_{\text{classe}} P(\text{classe} \mid a_1, \dots, a_n)
$$

Como `P(a₁, ..., aₙ)` é constante para todas as classes, ele pode ser removido da maximização:

$$
\text{classe}_{\text{MAP}} = \arg\max_{\text{classe}} [P(a_1, \dots, a_n \mid \text{classe}) \times P(\text{classe})]
$$

Incorporando a suposição de independência condicional, a fórmula final do classificador se torna:

$$
\text{classe}_{\text{NaiveBayes}} = \arg\max_{\text{classe}} \left[ P(\text{classe}) \prod_{i=1}^{n} P(a_i \mid \text{classe}) \right]
$$

Para evitar **underflow numérico** ao multiplicar muitas probabilidades pequenas, é comum usar a soma dos logaritmos:

$$
\text{classe}_{\text{NaiveBayes}} = \arg\max_{\text{classe}} \left[ \log(P(\text{classe})) + \sum_{i=1}^{n} \log(P(a_i \mid \text{classe})) \right]
$$

---

## 4. Modelos de Probabilidade

A estimação do termo de verossimilhança `P(aᵢ | classe)` depende da natureza dos atributos.

### Modelo Gaussiano (Atributos Contínuos)
Assume-se que, para cada classe `c`, o atributo contínuo `Xⱼ` segue uma distribuição Gaussiana (Normal): $X_j \mid Y=c \sim \mathcal{N}(\mu_{j,c}, \sigma^2_{j,c})$. A densidade `f(xⱼ | Y=c)` é calculada usando a média `μ` e a variância `σ²` estimadas para essa classe e atributo.

### Modelo Multinomial (Atributos Discretos)
Usado para dados como contagens (ex: palavras em um texto). Assume-se que $X_j \mid Y=c \sim \text{Multinomial}(1, \theta_{j,c})$, onde `θ` é um vetor de probabilidades para as categorias do atributo. A probabilidade `P(aᵢ | classe)` é a frequência relativa do valor `aᵢ` dentro da classe `c`.

### Modelo Bernoulli
Similar ao Multinomial, mas os atributos são variáveis binárias independentes (ex: a palavra ocorre ou não).

---

## 5. Estimação de Parâmetros e Suavização

Os parâmetros `P(classe)` e `P(aᵢ | classe)` são estimados a partir dos dados de treinamento.

### Estimação de P(classe) (A Priori)
É a proporção de instâncias de treinamento que pertencem à classe `c`.
$$
\hat{P}(\text{classe}) = \frac{\text{Nº de instâncias da classe } c}{\text{Nº total de instâncias}}
$$

### Estimação de P(aᵢ | classe) (Verossimilhança)
- **Para Atributos Discretos (Multinomial):**
  $$
  \hat{P}(a_i \mid \text{classe}) = \frac{\text{Nº de instâncias da classe } c \text{ onde o atributo } i \text{ tem valor } a_i}{\text{Nº total de instâncias da classe } c}
  $$
- **Para Atributos Contínuos (Gaussiano):** Os parâmetros `μ` e `σ²` são a média e variância amostral das instâncias da classe `c`.
  $$
  \hat{\mu}_{j,c} = \frac{1}{|C_c|} \sum_{k \in C_c} X_{j,k}
  $$
  $$
  \hat{\sigma}^2_{j,c} = \frac{1}{|C_c|} \sum_{k \in C_c} (X_{j,k} - \hat{\mu}_{j,c})^2
  $$

### Suavização de Laplace (Add-one Smoothing)
Essa técnica é crucial para evitar **probabilidades zero**. Se um valor de atributo não foi visto em uma classe durante o treino, sua probabilidade seria 0, zerando todo o score da classe. A suavização adiciona uma pequena contagem (geralmente 1) a todas as frequências para evitar esse problema.

---

## 6. Processo de Classificação

Para classificar uma nova instância `A_nova = (a₁, ..., aₙ)`:

1.  Para cada classe `c ∈ C`:
    a. Calcule a probabilidade a priori `P(c)`.
    b. Para cada atributo `aᵢ`, calcule a verossimilhança condicional `P(aᵢ | c)`.
    c. Calcule o score final da classe, geralmente usando a soma dos logaritmos para estabilidade numérica:
       $$
       \text{Score}_{\log}(c) = \log(\hat{P}(c)) + \sum_{i=1}^{n} \log(\hat{P}(a_i \mid c))
       $$

2.  A classe predita para `A_nova` é aquela que possui o **maior score**.

---

## 7. Limitações do Modelo

* **Suposição de Independência Condicional**: Principal limitação, pois raramente é verdadeira na prática. Se os atributos forem altamente correlacionados, o desempenho pode ser prejudicado.
* **Problema da Frequência Zero**: Sem a suavização (como Laplace), se um valor de atributo não apareceu em uma classe durante o treino, essa classe terá uma probabilidade final de zero, o que é problemático.
* **Sensibilidade a Atributos Irrelevantes**: Uma grande quantidade de atributos irrelevantes pode "afogar" o sinal dos atributos importantes.
* **Desempenho com Atributos Contínuos**: O modelo Gaussiano assume que os dados seguem uma distribuição normal. Se essa suposição não for atendida, as estimativas de probabilidade podem ser imprecisas.

