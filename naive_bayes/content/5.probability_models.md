# English Version

## Probabilistic Models in Naive Bayes

Until now, our email classification examples have implicitly used the multinomial distribution. This is just one of several probability distributions we can use with Naive Bayes, each one suitable for different types of data and problems [4].

In Naive Bayes, the "distribution" is our **assumption** about how the data (attributes) behave within each class. It is fundamental to understand that, regardless of the chosen distribution, the classifier's decision rule is always the same: to find the class $C$ that maximizes the product between the prior probability and the likelihood.

The decision formula we saw in the introduction remains our foundation [2]:

$$\hat{C} = \arg\max_C [P(C) \times \prod_{i=1}^{n} P(X_i|C)]$$

or

$$\hat{C} = \arg\max_C \left[ \text{Prior} \times \left(\prod \text{Likelihoods}\right) \right]$$

In practice, to ensure numerical stability, we work with the sum of logarithms. Thus, the decision formula for **any** Naive Bayes classifier assumes the following universal structure [1]:

$$ \hat{C} = \arg\max_C \left[ \log P(C) + \sum_{i=1}^{n} \log P(X_i|C) \right] $$

This "sum of logarithms" structure is the practical template that we will see in all the following models. What differentiates them is **the way each term of the formula is calculated**.

### 1. What Remains Fixed: The Prior, $P(C)$

The **Prior Probability** is always calculated in the same way: it is the **overall frequency of each class in the training set**. Therefore, the term $\log P(C)$ in our formula **does not change**, regardless of the model we use for the attributes [2].

### 2. What Changes: The Likelihood, $P(X_i|C)$

The **Likelihood** is the part of the calculation that adapts to the nature of the data. TThe choice of model is, in essence, the choice of which mathematical tool to use to calculate this likelihood [3]:

- If the attributes are **counts** (like words in a text), we use the **Multinomial** approach.
- If the attributes are **presence/absence** (yes/no), we use the **Bernoulli** approach.
- If the attributes are **continuous** (like height or price), we use the **Gaussian** approach, which gives us a probability density $f(X_i|C)$ that is used within the logarithm.

Choosing the correct model to calculate the likelihood is fundamental to the classifier's success. With this structure in mind, let's now analyze in detail how each of these models works.

## References

[1] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

[2] Izbicki, R., & Santos, T. M. (2020). *Aprendizado de máquina: uma abordagem estatística*. (1st ed.). Rafael Izbicki.

[3] Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

[4] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.

# Portuguese Version

## **Modelos Probabilísticos no Naive Bayes**

Até agora, nossos exemplos de classificação de emails usaram implicitamente a distribuição multinomial. Esta é apenas uma das várias distribuições de probabilidade que podemos usar com Naive Bayes, cada uma adequada para diferentes tipos de dados e problemas [4].

No Naive Bayes, a "distribuição" é a nossa **suposição** sobre como os dados (atributos) se comportam dentro de cada classe. É fundamental entender que, independentemente da distribuição escolhida, a regra de decisão do classificador é sempre a mesma: encontrar a classe $C$ que maximiza o produto entre a probabilidade a priori e a verossimilhança.

A fórmula de decisão que vimos na introdução continua sendo nossa base [2]:

$$\hat{C} = \arg\max_C [P(C) \times \prod_{i=1}^{n} P(X_i|C)]$$

ou

$$\hat{C} = \arg\max_C \left[ \text{Prior} \times \left(\prod \text{Verossimilhanças}\right) \right]$$

Na prática, para garantir a estabilidade numérica, trabalhamos com a soma dos logaritmos. Assim, a fórmula de decisão para **qualquer** classificador Naive Bayes assume a seguinte estrutura universal [1]:

$$ \hat{C} = \arg\max_C \left[ \log P(C) + \sum_{i=1}^{n} \log P(X_i|C) \right] $$

Essa estrutura de "soma de logaritmos" é o template prático que veremos em todos os modelos a seguir. O que os diferencia é **a forma como cada termo da fórmula é calculado**.

### 1. O Que Permanece Fixo: O Prior, $P(C)$

A **Probabilidade a Priori** é sempre calculada da mesma forma: é a **frequência geral de cada classe no conjunto de treinamento**. Portanto, o termo $\log P(C)$ na nossa fórmula **não muda**, independentemente do modelo que usamos para os atributos [2].

### 2. O Que Muda: A Verossimilhança, $P(X_i|C)$

A **Verossimilhança** é a parte do cálculo que se adapta à natureza dos dados. A escolha do modelo é, na verdade, a escolha de qual ferramenta matemática usar para calcular essa verossimilhança [3]:

- Se os atributos são **contagens** (como palavras em um texto), usamos a abordagem **Multinomial**.
- Se os atributos são de **presença/ausência** (sim/não), usamos a abordagem **Bernoulli**.
- Se os atributos são **contínuos** (como altura ou preço), usamos a abordagem **Gaussiana**, que nos dá uma densidade de probabilidade $f(X_i|C)$ para usarmos dentro do logaritmo.

Escolher o modelo correto para calcular a verossimilhança é fundamental para o sucesso do classificador. Com essa estrutura em mente, vamos agora analisar em detalhe como cada um desses modelos funciona.

## Referências

[1] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

[2] Izbicki, R., & Santos, T. M. (2020). *Aprendizado de máquina: uma abordagem estatística*. (1st ed.). Rafael Izbicki.

[3] Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

[4] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.
