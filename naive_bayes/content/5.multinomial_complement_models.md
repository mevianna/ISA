# English Version

# Portuguese Version

## **Modelos Probabilísticos no Naive Bayes**

Até agora, nossos exemplos de classificação de emails usaram implicitamente a distribuição multinomial. Esta é apenas uma das várias distribuições de probabilidade que podemos usar com Naive Bayes, cada uma adequada para diferentes tipos de dados e problemas [1].

> [!NOTE]
> **O que é uma Distribuição de Probabilidade?**
> 
> Uma distribuição de probabilidade é uma função matemática que descreve a probabilidade de ocorrência de diferentes resultados possíveis para um experimento [2].
>
> No Naive Bayes, a "distribuição" é a nossa **suposição** sobre como os dados (atributos) se comportam dentro de cada classe. A escolha da distribuição define como calculamos a verossimilhança $P(X|C)$ [1]:
> - Se os atributos são **contagens** (como palavras em um texto), usamos a **Multinomial**.
> - Se os atributos são de **presença/ausência** (sim/não), usamos a **Bernoulli**.
> - Se os atributos são **contínuos** (como altura ou preço), usamos a **Gaussiana**.
>
> Escolher o modelo correto é fundamental para o sucesso do classificador.

Vamos começar formalizando o modelo que temos usado, a distribuição Multinomial, antes de apresentar suas variações e outras distribuições.

### Modelo Multinomial

O Multinomial Naive Bayes é o modelo ideal para atributos que representam **contagens** ou **frequências** de eventos, como o número de vezes que cada palavra aparece em um documento. Vamos agora formalizar a intuição por trás da fórmula que usamos na introdução.

#### Derivação Intuitiva da Verossimilhança $P(X|C)$

Para construir o classificador, precisamos calcular a verossimilhança $P(X|C)$: a probabilidade de um documento $X$ ter sido "gerado" por uma classe $C$.

A maneira mais simples de pensar nisso é imaginar que o modelo gera o documento **uma palavra de cada vez**. A suposição "ingênua" (Naive) é que a escolha de cada palavra é um evento **independente** das outras.

Se nosso documento for a sequência de palavras $X = (X_1, X_2, ..., X_n)$, a probabilidade de gerar essa sequência exata é simplesmente o produto das probabilidades de gerar cada palavra individualmente:

$$P(X|C) = P(X_1|C) \times P(X_2|C) \times \dots \times P(X_n|C) = \prod_{i=1}^{n} P(X_i|C)$$

Onde $X_i$ é a i-ésima palavra do documento. Por exemplo, para o email **"ganhe dinheiro ganhe"** ($n=3$), o cálculo para a classe *Spam* seria:

$$P(\text{"ganhe dinheiro ganhe"}|\text{Spam}) = P(\text{"ganhe"}|\text{Spam}) \times P(\text{"dinheiro"}|\text{Spam}) \times P(\text{"ganhe"}|\text{Spam})$$

Essa abordagem, baseada na probabilidade de uma sequência de eventos independentes, nos leva diretamente à fórmula que precisamos para o classificador.


#### A Fórmula do Classificador Multinomial

Agora, basta inserir nossa expressão de verossimilhança na regra de decisão de Bayes:

$$\hat{C} = \arg\max_C \left[ P(C) \times P(X|C) \right]$$

Substituindo $P(X|C)$ pela nossa derivação:

$$\hat{C} = \arg\max_C \left[ P(C) \times \prod_{i=1}^{n} P(X_i|C) \right]$$

Finalmente, aplicando o logaritmo para evitar problemas numéricos (underflow) e transformar a multiplicação em uma soma, chegamos à fórmula final que já conhecemos:

$$\hat{C} = \arg\max_C \left[ \log P(C) + \sum_{i=1}^{n} \log P(X_i|C) \right]$$

#### Suavização de Laplace

O último passo é saber como calcular cada $P(X_i|C)$. Como vimos, fazemos isso contando as frequências nos dados de treino e aplicando **Laplace Smoothing** para evitar o problema do zero:

$$P(X_i|C) = \frac{(\text{Contagem da palavra } X_i \text{ na classe } C) + 1}{(\text{Nº total de palavras na classe } C) +  |\text{vocabulário}|}$$


### Complement Naive Bayes (CNB)

Complement Naive Bayes (CNB) é uma adaptação do modelo Multinomial, projetado para lidar melhor com datasets desbalanceados. Em datasets desbalanceados, o Multinomial tende a favorecer classes majoritárias, pois elas têm maior probabilidade a priori $P(C)$. O CNB contorna isso focando em quão bem cada documento se distingue do **complemento** de cada classe.

Em vez de calcular a probabilidade de um documento pertencer à classe $C$, ele calcula quão incompatível o documento é com o **complemento** de $C$ (todas as outras classes) e escolhe a classe que é mais incompatível com seu complemento.

#### Classificador Complement Naive Bayes

A decisão é baseada em encontrar a classe que **minimiza** a seguinte expressão:

$$\hat{C} = \arg\min_C \sum_{i=1}^{n} \log P(X_i|\bar{C})$$

A estimativa de $P(X_i|\bar{C})$ segue a mesma lógica da verossimilhança multinomial, mas as contagens são feitas em todas as classes, *exceto* a classe $C$:

$$P(X_i|\bar{C}) = \frac{(\text{Contagem da palavra } X_i \text{ no complemento de } C) + 1}{(\text{Nº total de palavras no complemento de } C) + |\text{vocabulário}|}$$

Mais formalmente:

$$P(X_i|\bar{C}) = \frac{1 + \sum_{c \neq C} N_{ic}}{|\text{vocabulário}| + \sum_{c \neq C} \sum_{j=1}^{|\text{vocabulário}|} N_{jc}}$$

onde $N_{ic}$ é a contagem da palavra $i$ na classe $c$, e $\alpha$ é o parâmetro de suavização.

#### Exemplo Prático: Classificação de Críticas de Cinema

Vamos usar o cenário com três classes: **Comédia**, **Ação** e **Drama**, com a classe **Comédia** sendo minoritária. Recebemos a nova crítica: **"piada hilária e divertida"**. O CNB calculará a **log-probabilidade do complemento** para cada classe, este valor representa o quão provável é que a crítica pertença ao conjunto de classes do complemento.

Vamos supor que, após os cálculos, o algoritmo chegou aos seguintes valores:

* **Teste para "Comédia"**: (Calcula a log-probabilidade da crítica pertencer a {Ação, Drama})
    * $\sum_{i=1}^{n} \log P(X_i|\overline{\text{Comédia}})$ = **-25.8**

* **Teste para "Ação"**: (Calcula a log-probabilidade da crítica pertencer a {Comédia, Drama})
    * $\sum_{i=1}^{n} \log P(X_i|\overline{\text{Ação}})$ = **-12.5**

* **Teste para "Drama"**: (Calcula a log-probabilidade da crítica pertencer a {Comédia, Ação})
    * $\sum_{i=1}^{n} \log P(X_i|\overline{\text{Drama}})$ = **-14.2**

**Decisão:**

- O classificador usa $\arg\min$, ou seja, ele procura a classe que resultou na **menor log-probabilidade do complemento**. Comparando os resultados, $-25.8$ é o menor valor.

- O valor $-25.8$ para Comédia indica que "piada hilária e divertida" é **muito incompatível** com o conjunto {Ação, Drama}. Quanto menor (mais negativo) esse valor, maior a incompatibilidade com o complemento, logo maior a probabilidade de pertencer à classe testada.

- Portanto, o modelo classifica a crítica como **Comédia**. Ele conclui que a crítica é tão incompatível com o conjunto {Ação, Drama} que ela muito provavelmente pertence à classe "Comédia".

#### Vantagem em Datasets Desbalanceados

Esta abordagem é especialmente eficaz em cenários desbalanceados porque, ao focar no complemento, o algoritmo dá peso similar a todas as classes, independentemente de sua frequência no dataset de treino. Isso permite que classes minoritárias tenham uma chance mais justa de serem selecionadas quando realmente apropriado.

## Referências

[1] Murphy, K. P. (2012). Machine learning: A probabilistic perspective. MIT Press.

[2] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.
