# English Version

# Portuguese Version

## Ideias Gerais

Naive Bayes é um algoritmo de aprendizado de máquina **supervisionado** usado para **classificação**.[1]

> [!NOTE]
> - O **aprendizado supervisionado** aprende a partir de dados que já incluem as respostas corretas (também chamados de exemplos rotulados). Com base nesses, ele faz previsões sobre novos dados.
>
> - Os problemas de **regressão** preveem números, como o preço de uma casa. Os de **classificação** são aqueles que preveem “rótulos” ou “categorias”, como se um email se encaixa na categoria “spam” ou “não spam”.


O Naive Bayes faz previsões usando probabilidades aprendidas dos dados de treinamento. Ele descobre a chance (probabilidade) de cada classe existir e a chance de cada atributo  aparecer em cada classe. Depois, para classificar uma nova instância, ele calcula a probabilidade de pertencer a cada classe e escolhe a mais provável.[1] Pra ver isso funcionando na prática, vamos a um exemplo.

### Exemplo: Classificação de Emails Spam

Vamos usar um exemplo clássico de classificação de emails. Imagine que, durante o treinamento com um grande conjunto de dados, o algoritmo observou o seguinte:
- **Probabilidade das classes:** 30% dos emails são "spam" e 70% são "normal".
- **Probabilidade dos atributos por classe:**
    - A palavra "ganhe" aparece em 80% dos emails spam, mas em apenas 5% dos emails normais.
    - A palavra "dinheiro" aparece em 70% dos emails spam, mas em apenas 10% dos emails normais.

Agora, quando um novo email chega com o texto **"ganhe dinheiro"**, o Naive Bayes usará essas probabilidades para decidir se é spam ou não, ele:

- **Calcula a probabilidade de uma nova instância pertencer a cada classe possível baseado em seus atributos**:
    - **Calcula a probabilidade** de ser spam baseado nas palavras presentes.
    - **Calcula a probabilidade** de ser email normal baseado nas mesmas palavras.

- **Compara as duas** e escolhe a classe com maior probabilidade.

Como as palavras "ganhe" e "dinheiro" são muito mais comuns em spam do que em emails normais, o algoritmo classificaria este email como **SPAM**. Vamos ver como isso funciona matematicamente.

### A Fórmula do Naive Bayes

O Naive Bayes se fundamenta no **Teorema de Bayes** para calcular as probabilidades. Para simplificar, vamos começar com o caso mais simples, onde analisamos apenas **um atributo**, como um email que contém apenas a palavra "ganhe".

A fórmula é:

$$P(C|X) = \frac{P(X|C) \times P(C)}{P(X)}$$

Onde:
- $P(C|X)$ é probabilidade de uma instância pertencer a uma classe específica, dado que observamos seus atributos $X$. Este é o valor que queremos calcular para fazer a classificação.

- $P(X|C)$ é a probabilidade de observar o atributo $X$ em uma instância que sabemos que pertence à classe $C$.
  
- $P(C)$ é a probabilidade geral de uma instância pertencer à classe $C$.
- $P(X)$ é a probabilidade total do atributo $X$ aparecer, independente da classe.

Durante o treinamento, o algoritmo calcula $P(X|C)$ observando com que frequência cada atributo aparece em cada classe, e $P(C)$ contando a proporção de cada classe no dataset. Já $P(X)$ representa a probabilidade total do atributo aparecer, mas como veremos a seguir, não precisamos calculá-lo para fazer a classificação.

### Exemplo numérico: A palavra "ganhe"

* Primeiro, vamos listar os valores que já conhecemos:

    - $P(\text{spam}) = 0.3$ (30% dos emails são spam)
    - $P(\text{normal}) = 0.7$ (70% dos emails são normais)
    - $P(\text{"ganhe"}|\text{spam}) = 0.8$ (80% dos emails spam contêm "ganhe")
    - $P(\text{"ganhe"}|\text{normal}) = 0.05$ (5% dos emails normais contêm "ganhe")

Vamos aplicar os números do nosso exemplo. Queremos saber se um email com a palavra "ganhe" é spam ou normal.

**Cálculo para a classe SPAM:**

$$P(\text{spam}|\text{"ganhe"}) = \frac{P(\text{"ganhe"}|\text{spam}) \times P(\text{spam})}{P(\text{"ganhe"})}$$
$$P(\text{spam}|\text{"ganhe"}) = \frac{0.8 \times 0.3}{P(\text{"ganhe"})} = \frac{0.24}{P(\text{"ganhe"})}$$

**Cálculo para a classe NORMAL:**

$$P(\text{normal}|\text{"ganhe"}) = \frac{P(\text{"ganhe"}|\text{normal}) \times P(\text{normal})}{P(\text{"ganhe"})}$$
$$P(\text{normal}|\text{"ganhe"}) = \frac{0.05 \times 0.7}{P(\text{"ganhe"})} = \frac{0.035}{P(\text{"ganhe"})}$$

Para classificar o email, precisamos determinar qual destes valores é maior: $P(\text{spam}|\text{"ganhe"})$ ou $P(\text{normal}|\text{"ganhe"})$.

Observe que o denominador $P(\text{"ganhe"})$ é o mesmo nas duas frações. Para descobrir qual fração é maior, basta comparar os numeradores: $0.24$ e $0.035$.

Como $0.24$ é maior que $0.035$, a probabilidade da classe **SPAM** é maior, e o algoritmo classifica o email como tal.

>[!NOTE]
>Esta é a razão fundamental pela qual podemos ignorar o denominador $P(X)$ (probabilidade total do atributo $X$ aparecer) na prática da classificação. Como nosso objetivo é determinar qual classe tem a **maior** probabilidade, e o denominador é comum a todas elas, precisamos apenas comparar os numeradores.

### Fórmula do classificador

Portanto a classificação se resume a:

$$\hat{C} = \arg\max_C [P(X|C) \times P(C)]$$

Onde $\hat{C}$ é a classe escolhida e $\arg\max_C$ significa "encontre a classe $C$ que maximiza a expressão".


### Lidando com múltiplos atributos

Quando temos múltiplos atributos (como várias palavras em um email), o **Teorema de Bayes** se expande para:

$$P(C|X_1, X_2, ..., X_n) = \frac{P(C) \cdot P(X_1, X_2, ..., X_n|C)}{P(X_1, X_2, ..., X_n)}$$

- **Problema**:  Calcular $P(X_1, X_2, ..., X_n|C)$ requer observar todas as possíveis combinações de atributos. Por exemplo, para descobrir a probabilidade conjunta de as palavras "ganhe" E "dinheiro" aparecerem juntas em emails spam, precisaríamos de dados específicos sobre essa combinação exata. Com vocabulários de milhares de palavras, isso se torna computacionalmente inviável.

- **A suposição "ingênua"**: Para contornar esse problema, o Naive Bayes assume que os atributos são independentes entre si. Isso significa que a presença de um atributo não influencia a probabilidade de outro atributo aparecer. Matematicamente:

$$P(C|X_1, X_2, ..., X_n) = \frac{P(C) \times \prod_{i=1}^{n} P(X_i|C)}{P(X_1, X_2, ..., X_n)}$$

Onde o símbolo $\prod$ (produtório) significa que multiplicamos as probabilidades de cada atributo individual dado a classe C.

Portanto, o classificador se torna:

$$\hat{C} = \arg\max_C [P(C) \times \prod_{i=1}^{n} P(X_i|C)]$$

>[!NOTE]
> - **O que significa independência?**
>
> -  Dois eventos são **independentes** se a ocorrência de um não afeta a probabilidade do outro. Por exemplo, o resultado de um lançamento de moeda e a temperatura de amanhã são independentes.
>
> - Por outro lado, muitas variáveis são **dependentes**. Por exemplo, "estar nublado" e "chover" são dependentes, se está nublado, aumenta a chance de chover.


> [!IMPORTANT]
> A suposição de independência não é verdadeira em muitos problemas, no exemplo, a presença da palavra "ganhe" pode aumentar a chance de "dinheiro" aparecer, mas simplifica muito o algoritmo e funciona muito bem na prática.[2] O Nome **Naive** (ingênuo) para o algoritmo vem dessa simplificação.

### Exemplo prático: "ganhe dinheiro"

Vamos aplicar nossa fórmula ao email que contém as palavras "ganhe" e "dinheiro". Usando os dados do nosso exemplo anterior:

**Para a classe SPAM:**
$$P(\text{spam}) \cdot P(\text{"ganhe"}|\text{spam}) \cdot P(\text{"dinheiro"}|\text{spam})$$
$$= 0.3 \times 0.8 \times 0.7 = 0.168$$

**Para a classe NORMAL:**
$$P(\text{normal}) \cdot P(\text{"ganhe"}|\text{normal}) \cdot P(\text{"dinheiro"}|\text{normal})$$
$$= 0.7 \times 0.05 \times 0.1 = 0.0035$$

Como $0.168 > 0.0035$, o algoritmo classifica o email como **SPAM**, o que faz sentido, já que ambas as palavras são muito mais comuns em emails spam.

### Evitando Underflow: Log-Probabilidades

Na prática, quando há muitos atributos (ex: milhares de palavras), multiplicar probabilidades (valores entre 0 e 1) resulta em números extremamente pequenos, causando erros numéricos (underflow). Para resolver isso, trabalhamos com logaritmos das probabilidades ao invés das probabilidades diretas.[3]

Como o logaritmo é uma função **monótona crescente** (isto é, se a A é maior que B, então $\log(A)$ também será maior que $\log(B))$, encontrar a classe com maior log-probabilidade é o mesmo que encontrar a classe com maior probabilidade.

A fórmula do classificador se torna:

$$\hat{C} = \arg\max_C [\log P(C) + \sum_{i=1}^{n} \log P(X_i|C)]$$

Onde o produtório $(\prod)$ se transforma em um somatório $(\sum)$ devido à propriedade: $\log(a \times b) = \log(a) + \log(b)$.

Exemplo com nossos dados:

**SPAM**: $\log(0.3) + \log(0.8) + \log(0.7) = -1.204 + (-0.223) + (-0.357) = -1.784$

**NORMAL**: $\log(0.7) + \log(0.05) + \log(0.1) = -0.357 + (-2.996) + (-2.303) = -5.656$

Como $-1.784 > -5.656$, ainda classificamos como **SPAM**.

## Referencias

[1] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

[2] Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: Data mining, inference, and prediction*. (2nd ed.). Springer.

[3] Murphy, K. P. (2012). *Machine learning: A probabilistic perspective*. MIT Press.
