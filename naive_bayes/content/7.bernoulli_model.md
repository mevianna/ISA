## Modelo Bernoulli

O **Bernoulli Naive Bayes** é ideal para problemas em que os atributos são binários, ou seja, indicam **presença ou ausência** de uma característica, e não a frequência dela [4].

> [!NOTE]
>**Diferença do Multinomial**: Enquanto o modelo Multinomial conta **quantas vezes** cada palavra aparece em um documento, o Bernoulli se preocupa apenas com **se a palavra aparece ou não**. Além disso, o Bernoulli considera **tanto a presença quanto a ausência** de cada atributo na classificação. Uma palavra que **não aparece** em um documento também influencia a decisão do classificador.


### Exemplo

Vamos supor que temos um vocabulário com 3 palavras: "ganhe", "dinheiro", "agora". Um novo documento é então representado por um vetor binário.

| Palavra   | Documento 1 | Documento 2 |
|-----------|-------------|-------------|
| ganhe     | 1 (presente)| 0 (ausente) |
| dinheiro  | 1 (presente)| 1 (presente)|
| agora     | 0 (ausente) | 1 (presente)|

Mesmo que "dinheiro" apareça 10 vezes em um documento, o Bernoulli Naive Bayes a trata simplesmente como **presente (1)**. Além disso, o fato de "ganhe" **não aparecer** no Documento 2 é uma informação relevante para a classificação.


### Cálculo da verossimilhança

Para cada palavra $X_i$ do vocabulário, o modelo aprende a probabilidade de **presença** e **ausência** em cada classe, usando a Suavização de Laplace para evitar probabilidades zero [2].

$$P(X_i = 1|C) = \frac{N_{ic} + 1}{N_c + 2}$$

$$P(X_i = 0|C) = 1 - P(X_i = 1|C) = \frac{N_c - N_{ic} + 1}{N_c + 2}$$

- $N_{ic}$ = nº de documentos da classe $C$ que contêm a palavra $X_i$.
- $N_c$ = total de documentos da classe $C$.

> [!NOTE]
> **Por que `+2` no denominador [1]?**
> A intuição é que, para qualquer palavra, existem dois cenários possíveis em um e-mail: a palavra **está presente** ou **não está presente**. A suavização funciona adicionando `1` "documento fantasma" para *cada um* desses cenários, a fim de evitar probabilidades de zero em ambos os casos.
> * O `+1` no numerador representa o e-mail fantasma em que a palavra **está presente**.
> * O `+2` no denominador representa o total de e-mails fantasmas adicionados: `1` para o cenário **presente** e `1` para o cenário **ausente**

### Classificador

A classe de um novo documento é decidida pela seguinte fórmula [4]:

$$
\hat{C} = \arg\max_C \left[ \log P(C) + \sum_i \big( x_i \log P(X_i=1|C) + (1 - x_i) \log_P(X_i=0|C) \big) \right]
$$

> [!NOTE]
> **Nota sobre o somatório:** Diferente do modelo Multinomial, onde o somatório percorre apenas as palavras presentes no documento, no modelo Bernoulli o somatório $\sum_i$ percorre **todas as palavras do vocabulário**. É por isso que a ausência de palavras ($x_i=0$) também contribui para o cálculo final.


### O papel do $x_i$ na fórmula

Na equação acima, o termo $x_i$ representa se o atributo  $i$ está **presente (1)** ou **ausente (0)** no exemplo que estamos classificando. Ele atua como um **seletor** que decide qual probabilidade será usada:

- **Se $x_i=1$ (palavra presente)**: o primeiro termo $x_i \log P(X_i=1|C)$ contribui com $\log P(X_i=1|C)$, e o segundo termo $(1-x_i) \log P(X_i=0|C)$ se anula.

- **Se $x_i=0$ (palavra ausente)**: o primeiro termo se anula, e o segundo termo contribui com $\log P(X_i=0|C)$.

**Resultado**: Para cada palavra no vocabulário, o modelo sempre considera uma evidência, seja a probabilidade de presença (quando a palavra aparece) ou a probabilidade de ausência (quando não aparece).

## Referências

[1] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

[2] Izbicki, R., & Santos, T. M. (2020). *Aprendizado de máquina: uma abordagem estatística*. (1st ed.). Rafael Izbicki.

[3] Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

[4] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.
