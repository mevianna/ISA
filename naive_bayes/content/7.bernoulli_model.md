# English Version

## Bernoulli Model

**Bernoulli Naive Bayes** is ideal for problems where the attributes are binary, that is, they indicate the **presence or absence** of a feature, and not its frequency [4].

> [!NOTE]
> **Difference from Multinomial**: While the Multinomial model counts **how many times** each word appears in a document, the Bernoulli model is only concerned with **whether the word appears or not**. Furthermore, the Bernoulli model considers **both the presence and the absence** of each attribute in the classification. A word that **does not appear** in a document also influences the classifier's decision.

### Example

Let's suppose we have a vocabulary with 3 words: "make", "money", "now". A new document is then represented by a binary vector.

| Word    | Document 1 | Document 2 |
|---------|------------|------------|
| make    | 1 (present)| 0 (absent) |
| money   | 1 (present)| 1 (present)|
| now     | 0 (absent) | 1 (present)|

Even if "money" appears 10 times in a document, the Bernoulli Naive Bayes simply treats it as **present (1)**. Furthermore, the fact that "make" **does not appear** in Document 2 is relevant information for the classification.

### Likelihood calculation

For each word $X_i$ in the vocabulary, the model learns the probability of **presence** and **absence** in each class, using Laplace Smoothing to avoid zero probabilities [2].

$$P(X_i = 1|C) = \frac{N_{ic} + 1}{N_c + 2}$$

$$P(X_i = 0|C) = 1 - P(X_i = 1|C) = \frac{N_c - N_{ic} + 1}{N_c + 2}$$

- $N_{ic}$ = # of documents in class $C$ that contain the word $X_i$.
- $N_c$ = total # of documents in class $C$.

> [!NOTE]
> **Why `+2` in the denominator [1]?**
> The intuition is that, for any word, there are two possible scenarios in an email: the word **is present** or **it is not present**. Smoothing works by adding `1` "ghost document" for *each one* of these scenarios, in order to avoid zero probabilities in both cases.
> * The `+1` in the numerator represents the ghost email in which the word **is present**.
> * The `+2` in the denominator represents the total ghost emails added: `1` for the **present** scenario and `1` for the **absent** scenario.

### Classifier

The class of a new document is decided by the following formula [4]:

$$ \hat{C} = \arg\max_C \left[ \log P(C) + \sum_i \big( x_i \log P(X_i=1|C) + (1 - x_i) \log P(X_i=0|C) \big) \right] $$

> [!NOTE]
> **Note on the summation:** Unlike the Multinomial model, where the summation iterates only over the words present in the document, in the Bernoulli model the summation $\sum_i$ iterates over **all the words in the vocabulary**. This is why the absence of words ($x_i=0$) also contributes to the final calculation.

### The role of $x_i$ in the formula

In the equation above, the term $x_i$ represents if the attribute $i$ is **present (1)** or **absent (0)** in the example we are classifying. It acts as a **selector** that decides which probability will be used:

- **If $x_i=1$ (word present)**: the first term $x_i \log P(X_i=1|C)$ contributes with $\log P(X_i=1|C)$, and the second term $(1-x_i) \log P(X_i=0|C)$ becomes zero.
- **If $x_i=0$ (word absent)**: the first term becomes zero, and the second term contributes with $\log P(X_i=0|C)$.

**Result**: For each word in the vocabulary, the model always uses evidence—either from the probability of its presence (if the word appears) or the probability of its absence (if it does not).

## References

[1] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

[2] Izbicki, R., & Santos, T. M. (2020). *Aprendizado de máquina: uma abordagem estatística*. (1st ed.). Rafael Izbicki.

[3] Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

[4] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.

# Portuguese Version

## Modelo Bernoulli

O **Bernoulli Naive Bayes** é ideal para problemas em que os atributos são binários, ou seja, indicam **presença ou ausência** de uma característica, e não a frequência dela [4].

> [!NOTE]
>**Diferença do Multinomial**: Enquanto o modelo Multinomial conta **quantas vezes** cada palavra aparece em um documento, o Bernoulli se preocupa apenas com **se a palavra aparece ou não**. Além disso, o Bernoulli considera **tanto a presença quanto a ausência** de cada atributo na classificação. Uma palavra que **não aparece** em um documento também influencia a decisão do classificador.


### Exemplo

Vamos supor que temos um vocabulário com 3 palavras: "ganhe", "dinheiro", "agora". Um novo documento é então representado por um vetor binário.

| Palavra   | Documento 1 | Documento 2 |
|-----------|-------------|-------------|
| ganhe     | 1 (presente)| 0 (ausente) |
| dinheiro  | 1 (presente)| 1 (presente)|
| agora     | 0 (ausente) | 1 (presente)|

Mesmo que "dinheiro" apareça 10 vezes em um documento, o Bernoulli Naive Bayes a trata simplesmente como **presente (1)**. Além disso, o fato de "ganhe" **não aparecer** no Documento 2 é uma informação relevante para a classificação.


### Cálculo da verossimilhança

Para cada palavra $X_i$ do vocabulário, o modelo aprende a probabilidade de **presença** e **ausência** em cada classe, usando a Suavização de Laplace para evitar probabilidades zero [2].

$$P(X_i = 1|C) = \frac{N_{ic} + 1}{N_c + 2}$$

$$P(X_i = 0|C) = 1 - P(X_i = 1|C) = \frac{N_c - N_{ic} + 1}{N_c + 2}$$

- $N_{ic}$ = nº de documentos da classe $C$ que contêm a palavra $X_i$.
- $N_c$ = total de documentos da classe $C$.

> [!NOTE]
> **Por que `+2` no denominador [1]?**
> A intuição é que, para qualquer palavra, existem dois cenários possíveis em um e-mail: a palavra **está presente** ou **não está presente**. A suavização funciona adicionando `1` "documento fantasma" para *cada um* desses cenários, a fim de evitar probabilidades de zero em ambos os casos.
> * O `+1` no numerador representa o e-mail fantasma em que a palavra **está presente**.
> * O `+2` no denominador representa o total de e-mails fantasmas adicionados: `1` para o cenário **presente** e `1` para o cenário **ausente**

### Classificador

A classe de um novo documento é decidida pela seguinte fórmula [4]:

$$
\hat{C} = \arg\max_C \left[ \log P(C) + \sum_i \big( x_i \log P(X_i=1|C) + (1 - x_i) \log_P(X_i=0|C) \big) \right]
$$

> [!NOTE]
> **Nota sobre o somatório:** Diferente do modelo Multinomial, onde o somatório percorre apenas as palavras presentes no documento, no modelo Bernoulli o somatório $\sum_i$ percorre **todas as palavras do vocabulário**. É por isso que a ausência de palavras ($x_i=0$) também contribui para o cálculo final.


### O papel do $x_i$ na fórmula

Na equação acima, o termo $x_i$ representa se o atributo  $i$ está **presente (1)** ou **ausente (0)** no exemplo que estamos classificando. Ele atua como um **seletor** que decide qual probabilidade será usada:

- **Se $x_i=1$ (palavra presente)**: o primeiro termo $x_i \log P(X_i=1|C)$ contribui com $\log P(X_i=1|C)$, e o segundo termo $(1-x_i) \log P(X_i=0|C)$ se anula.

- **Se $x_i=0$ (palavra ausente)**: o primeiro termo se anula, e o segundo termo contribui com $\log P(X_i=0|C)$.

**Resultado**: Para cada palavra no vocabulário, o modelo sempre considera uma evidência, seja a probabilidade de presença (quando a palavra aparece) ou a probabilidade de ausência (quando não aparece).

## Referências

[1] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

[2] Izbicki, R., & Santos, T. M. (2020). *Aprendizado de máquina: uma abordagem estatística*. (1st ed.). Rafael Izbicki.

[3] Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

[4] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.
