# Entendendo o Teorema de Naive Bayes para Classificação

O algoritmo **Naive Bayes** fundamenta-se na aplicação do **Teorema de Bayes** para calcular as probabilidades necessárias à classificação de dados. No contexto de **, o teorema é utilizado para predizer a classe de uma nova instância \( b \) (ou \( x \)), composta por um conjunto de atributos \( (b_1, b_2, ..., b_n) \) ou \( (x_1, x_2, ..., x_d) \).

Ele descreve a **probabilidade de um evento** (neste caso, a atribuição de uma classe) com base no **conhecimento prévio** de condições que podem estar relacionadas a esse evento.



## A Fórmula Geral do Teorema de Bayes

A forma geral do Teorema de Bayes é expressa pela seguinte equação:

$$
P(\text{classe} \mid B) = \frac{P(B \mid \text{classe}) \cdot P(\text{classe})}{P(B)}
$$

### Onde:

- **\( P(\text{classe} \mid B) \)** — *Probabilidade a Posteriori*:  
  A probabilidade de uma instância pertencer a uma classe específica, dado que observamos seus atributos \( B \). Este é o valor que queremos calcular para fazer a classificação.

- **\( P(B \mid \text{classe}) \)** — *Verossimilhança (Likelihood)*:  
  A probabilidade de observarmos os atributos \( B \), sabendo que a instância pertence a uma determinada classe.

- **\( P(\text{classe}) \)** — *Probabilidade a Priori da Classe*:  
  A probabilidade geral de uma instância pertencer a essa classe, antes de observarmos seus atributos.

- **\( P(B) \)** — *Evidência*:  
  A probabilidade geral de observarmos os atributos \( B \), independentemente da classe.



## A Ideia Central: Hipótese e Evidência

A lógica por trás do teorema pode ser resumida assim:

> A probabilidade de uma **hipótese** ser verdadeira, dada uma **evidência**, depende de:
> - quão provável é essa evidência *se a hipótese for verdadeira*;
> - e quão provável era a hipótese *antes de vermos a evidência*.


##  Exemplo Prático: Classificação de E-mails

Vamos traduzir os conceitos para um exemplo comum: classificar um e-mail como **"spam"** ou **"não spam"**.

### Neste contexto:

- A **hipótese** é a classe (ex: o e-mail *é spam*);
- A **evidência** são os atributos do e-mail (ex: as palavras que ele contém, representadas por \( B \)).

### Aplicando a fórmula:

- **\( P(\text{classe} \mid B) \)**: Qual a probabilidade de o e-mail ser "spam" depois de vermos as palavras que ele contém?

- **\( P(B \mid \text{classe}) \)**: Se um e-mail é "spam", qual a probabilidade de ele ter exatamente essas palavras?

- **\( P(\text{classe}) \)**: Antes de olhar para o e-mail, qual era a chance de qualquer e-mail ser "spam"?

- **\( P(B) \)**: Qual a probabilidade geral de um e-mail qualquer ter esse conjunto de palavras, independente de ser spam ou não?



# Lidando com Múltiplos Atributos

Quando uma instância possui múltiplos atributos (ex: várias palavras em um e-mail), a fórmula é expandida para considerar todos:

$$
P(\text{classe} \mid b_1, b_2, ..., b_n) = \frac{P(b_1, b_2, ..., b_n \mid \text{classe}) \cdot P(\text{classe})}{P(b_1, b_2, ..., b_n)}
$$

Essa é a base do classificador **Naive Bayes**, que calcula essa probabilidade para cada classe possível e escolhe aquela com o maior valor.



##  Referências

- IZBICKI, Rafael; SANTOS, Tiago Mendonça dos. *Aprendizado de Máquina: Uma Abordagem Estatística*. 1ª ed. São Paulo: Blucher, 2021.  
  Este livro fornece uma introdução sólida ao modelo de Naive Bayes com base em fundamentos estatísticos, incluindo a interpretação bayesiana da probabilidade, o conceito de verossimilhança e a suposição de independência condicional entre atributos.

- NUNES, Raquel dos Santos; PARDO, Thiago Alexandre Salgueiro. *Uma Introdução aos Classificadores Bayesianos*.  
  Disponível em: [https://sites.icmc.usp.br/taspardo/NILCTR0225-PardoNunes.pdf](https://sites.icmc.usp.br/taspardo/NILCTR0225-PardoNunes.pdf)  
 






