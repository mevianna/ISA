# English Version

## Multinomial Model

Multinomial Naive Bayes is the ideal model for attributes that represent **counts** or **frequencies** of events, such as the number of times each word appears in a document [4]. Let's now formalize the intuition behind the formula we used in the introduction.

#### Intuitive Derivation of the Likelihood $P(X|C)$

To build the classifier, we need to calculate the likelihood $P(X|C)$: the probability of a document $X$ having been "generated" by a class $C$.

The simplest way to think about this is to imagine that the model generates the document **one word at a time**. The "naive" assumption is that the choice of each word is an event that is **independent** of the others.

If our document is the sequence of words $X = (X_1, X_2, ..., X_n)$, the probability of generating this exact sequence is simply the product of the probabilities of generating each word individually [2]:

$$P(X|C) = P(X_1|C) \times P(X_2|C) \times \dots \times P(X_n|C) = \prod_{i=1}^{n} P(X_i|C)$$

Where $X_i$ is the i-th word of the document. For example, for the email **"make money make"** ($n=3$), the calculation for the *Spam* class would be:

$$P(\text{"make money make"}|\text{Spam}) = P(\text{"make"}|\text{Spam}) \times P(\text{"money"}|\text{Spam}) \times P(\text{"make"}|\text{Spam})$$

This approach, based on the probability of a sequence of independent events, leads us directly to the formula we need for the classifier.


#### The Multinomial Classifier Formula

Now, we just need to insert our likelihood expression into the Bayes decision rule, from which we obtain the classifier's formula [1]:

$$\hat{C} = \arg\max_C \left[ P(C) \times \prod_{i=1}^{n} P(X_i|C) \right]$$

Finally, by applying the logarithm to avoid numerical problems (underflow) and to transform the multiplication into a sum, we arrive at the final formula we already know [4]:

$$\hat{C} = \arg\max_C \left[ \log P(C) + \sum_{i=1}^{n} \log P(X_i|C) \right]$$

#### Laplace Smoothing

The last step is to know how to calculate each $P(X_i|C)$. As we've seen, we do this by counting the frequencies in the training data and applying **Laplace Smoothing** to avoid the zero problem [3]:

$$P(X_i|C) = \frac{(\text{Count of word } X_i \text{ in class } C) + 1}{(\text{Total number of words in class } C) +  |\text{vocabulary}|}$$


### Complement Naive Bayes (CNB)

Complement Naive Bayes (CNB) is an adaptation of the Multinomial model, designed to better handle imbalanced datasets. In imbalanced datasets, the Multinomial model tends to favor majority classes, as they have a higher prior probability $P(C)$. CNB gets around this by focusing on how much a document **differs** from the **complement** of each class.

Instead of calculating the probability of a document belonging to class $C$, it calculates how incompatible the document is with the **complement** of $C$ (all other classes) and chooses the class that is most incompatible with its complement [5].

#### Complement Naive Bayes Classifier

The decision is based on finding the class that **minimizes** the following expression [5]:

$$\hat{C} = \arg\min_C \sum_{i=1}^{n} \log P(X_i|\bar{C})$$

The estimation of $P(X_i|\bar{C})$ follows the same logic as the multinomial likelihood, but the counts are done across all classes, *except* for class $C$:

$$P(X_i|\bar{C}) = \frac{(\text{Count of word } X_i \text{ in the complement of } C) + 1}{(\text{Total number of words in the complement of } C) + |\text{vocabulary}|}$$

More formally:

$$P(X_i|\bar{C}) = \frac{1 + \sum_{c \neq C} N_{ic}}{|\text{vocabulary}| + \sum_{c \neq C} \sum_{j=1}^{|\text{vocabulário}|} N_{jc}}$$

where $N_{ic}$ is the count of word $i$ in class $c$.

#### Practical Example: Movie Review Classification

Let's use a scenario with three classes: **Comedy**, **Action**, and **Drama**, with the **Comedy** class being the minority. We receive the new review: **"hilarious jokes, great fun"**. CNB will calculate the **log-probability of the complement** for each class; this value represents how likely it is that the review belongs to the set of complement classes.

Let's assume that after the calculations, the algorithm arrived at the following values:

* **Test for "Comedy"**: (Calculates the log-probability of the review belonging to {Action, Drama})
    * $\sum_{i=1}^{n} \log P(X_i|\overline{\text{Comedy}})$ = **-25.8**

* **Test for "Action"**: (Calculates the log-probability of the review belonging to {Comedy, Drama})
    * $\sum_{i=1}^{n} \log P(X_i|\overline{\text{Action}})$ = **-12.5**

* **Test for "Drama"**: (Calculates the log-probability of the review belonging to {Comedy, Action})
    * $\sum_{i=1}^{n} \log P(X_i|\overline{\text{Drama}})$ = **-14.2**

**Decision:**

- The classifier uses $\arg\min$, that is, it looks for the class that resulted in the **lowest log-probability of the complement**. Comparing the results, -25.8 is the lowest value.

- The value -25.8 for Comedy indicates that "hilarious and fun joke" is **very incompatible** with the set {Action, Drama}. The lower (more negative) this value, the greater the incompatibility with the complement, thus the higher the probability of belonging to the tested class.

- Therefore, the model classifies the review as **Comedy**. It concludes that the review is so incompatible with the set {Action, Drama} that it very likely belongs to the "Comedy" class.

#### Advantage in Imbalanced Datasets

This approach is especially effective in imbalanced scenarios because, by focusing on the complement, the algorithm gives similar weight to all classes, regardless of their frequency in the training set. This allows minority classes to have a fairer chance of being selected when they are, in fact, the correct class.

## References

[1] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

[2] Izbicki, R., & Santos, T. M. (2020). *Aprendizado de máquina: uma abordagem estatística*. (1st ed.). Rafael Izbicki.

[3] Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

[4] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.

[5] Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003). *Tackling the Poor Assumptions of Naive Bayes Text Classifiers*. Proceedings of the 20th International Conference on Machine Learning (ICML-03).

# Portuguese Version

## Modelo Multinomial

O Multinomial Naive Bayes é o modelo ideal para atributos que representam **contagens** ou **frequências** de eventos, como o número de vezes que cada palavra aparece em um documento [4]. Vamos agora formalizar a intuição por trás da fórmula que usamos na introdução.

#### Derivação Intuitiva da Verossimilhança $P(X|C)$

Para construir o classificador, precisamos calcular a verossimilhança $P(X|C)$: a probabilidade de um documento $X$ ter sido "gerado" por uma classe $C$.

A maneira mais simples de pensar nisso é imaginar que o modelo gera o documento **uma palavra de cada vez**. A suposição "ingênua" (Naive) é que a escolha de cada palavra é um evento **independente** das outras.

Se nosso documento for a sequência de palavras $X = (X_1, X_2, ..., X_n)$, a probabilidade de gerar essa sequência exata é simplesmente o produto das probabilidades de gerar cada palavra individualmente [2]:

$$P(X|C) = P(X_1|C) \times P(X_2|C) \times \dots \times P(X_n|C) = \prod_{i=1}^{n} P(X_i|C)$$

Onde $X_i$ é a i-ésima palavra do documento. Por exemplo, para o email **"ganhe dinheiro ganhe"** ($n=3$), o cálculo para a classe *Spam* seria:

$$P(\text{"ganhe dinheiro ganhe"}|\text{Spam}) = P(\text{"ganhe"}|\text{Spam}) \times P(\text{"dinheiro"}|\text{Spam}) \times P(\text{"ganhe"}|\text{Spam})$$

Essa abordagem, baseada na probabilidade de uma sequência de eventos independentes, nos leva diretamente à fórmula que precisamos para o classificador.


#### A Fórmula do Classificador Multinomial

Agora, basta inserir nossa expressão de verossimilhança na regra de decisão de Bayes, que obtemos a fórmula do classificador [1]:

$$\hat{C} = \arg\max_C \left[ P(C) \times \prod_{i=1}^{n} P(X_i|C) \right]$$

Finalmente, aplicando o logaritmo para evitar problemas numéricos (underflow) e transformar a multiplicação em uma soma, chegamos à fórmula final que já conhecemos [4]:

$$\hat{C} = \arg\max_C \left[ \log P(C) + \sum_{i=1}^{n} \log P(X_i|C) \right]$$

#### Suavização de Laplace

O último passo é saber como calcular cada $P(X_i|C)$. Como vimos, fazemos isso contando as frequências nos dados de treino e aplicando **Laplace Smoothing** para evitar o problema do zero [3]:

$$P(X_i|C) = \frac{(\text{Contagem da palavra } X_i \text{ na classe } C) + 1}{(\text{Nº total de palavras na classe } C) +  |\text{vocabulário}|}$$


### Complement Naive Bayes (CNB)

Complement Naive Bayes (CNB) é uma adaptação do modelo Multinomial, projetado para lidar melhor com datasets desbalanceados. Em datasets desbalanceados, o Multinomial tende a favorecer classes majoritárias, pois elas têm maior probabilidade a priori $P(C)$. O CNB contorna isso focando em quão bem cada documento se distingue do **complemento** de cada classe.

Em vez de calcular a probabilidade de um documento pertencer à classe $C$, ele calcula quão incompatível o documento é com o **complemento** de $C$ (todas as outras classes) e escolhe a classe que é mais incompatível com seu complemento [5].

#### Classificador Complement Naive Bayes

A decisão é baseada em encontrar a classe que **minimiza** a seguinte expressão [5]:

$$\hat{C} = \arg\min_C \sum_{i=1}^{n} \log P(X_i|\bar{C})$$

A estimativa de $P(X_i|\bar{C})$ segue a mesma lógica da verossimilhança multinomial, mas as contagens são feitas em todas as classes, *exceto* a classe $C$:

$$P(X_i|\bar{C}) = \frac{(\text{Contagem da palavra } X_i \text{ no complemento de } C) + 1}{(\text{Nº total de palavras no complemento de } C) + |\text{vocabulário}|}$$

Mais formalmente:

$$P(X_i|\bar{C}) = \frac{1 + \sum_{c \neq C} N_{ic}}{|\text{vocabulário}| + \sum_{c \neq C} \sum_{j=1}^{|\text{vocabulário}|} N_{jc}}$$

onde $N_{ic}$ é a contagem da palavra $i$ na classe $c$.

#### Exemplo Prático: Classificação de Críticas de Cinema

Vamos usar o cenário com três classes: **Comédia**, **Ação** e **Drama**, com a classe **Comédia** sendo minoritária. Recebemos a nova crítica: **"piada hilária e divertida"**. O CNB calculará a **log-probabilidade do complemento** para cada classe, este valor representa o quão provável é que a crítica pertença ao conjunto de classes do complemento.

Vamos supor que, após os cálculos, o algoritmo chegou aos seguintes valores:

* **Teste para "Comédia"**: (Calcula a log-probabilidade da crítica pertencer a {Ação, Drama})
    * $\sum_{i=1}^{n} \log P(X_i|\overline{\text{Comédia}})$ = **-25.8**

* **Teste para "Ação"**: (Calcula a log-probabilidade da crítica pertencer a {Comédia, Drama})
    * $\sum_{i=1}^{n} \log P(X_i|\overline{\text{Ação}})$ = **-12.5**

* **Teste para "Drama"**: (Calcula a log-probabilidade da crítica pertencer a {Comédia, Ação})
    * $\sum_{i=1}^{n} \log P(X_i|\overline{\text{Drama}})$ = **-14.2**

**Decisão:**

- O classificador usa $\arg\min$, ou seja, ele procura a classe que resultou na **menor log-probabilidade do complemento**. Comparando os resultados, $-25.8$ é o menor valor.

- O valor $-25.8$ para Comédia indica que "piada hilária e divertida" é **muito incompatível** com o conjunto {Ação, Drama}. Quanto menor (mais negativo) esse valor, maior a incompatibilidade com o complemento, logo maior a probabilidade de pertencer à classe testada.

- Portanto, o modelo classifica a crítica como **Comédia**. Ele conclui que a crítica é tão incompatível com o conjunto {Ação, Drama} que ela muito provavelmente pertence à classe "Comédia".

#### Vantagem em Datasets Desbalanceados

Esta abordagem é especialmente eficaz em cenários desbalanceados porque, ao focar no complemento, o algoritmo dá peso similar a todas as classes, independentemente de sua frequência no dataset de treino. Isso permite que classes minoritárias tenham uma chance mais justa de serem selecionadas quando realmente apropriado.

## Referências

[1] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

[2] Izbicki, R., & Santos, T. M. (2020). *Aprendizado de máquina: uma abordagem estatística*. (1st ed.). Rafael Izbicki.

[3] Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

[4] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.

[5] Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003). *Tackling the Poor Assumptions of Naive Bayes Text Classifiers*. Proceedings of the 20th International Conference on Machine Learning (ICML-03).
