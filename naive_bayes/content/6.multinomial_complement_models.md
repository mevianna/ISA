# English Version

# Portuguese Version

## Modelo Multinomial

O Multinomial Naive Bayes é o modelo ideal para atributos que representam **contagens** ou **frequências** de eventos, como o número de vezes que cada palavra aparece em um documento [4]. Vamos agora formalizar a intuição por trás da fórmula que usamos na introdução.

#### Derivação Intuitiva da Verossimilhança $P(X|C)$

Para construir o classificador, precisamos calcular a verossimilhança $P(X|C)$: a probabilidade de um documento $X$ ter sido "gerado" por uma classe $C$.

A maneira mais simples de pensar nisso é imaginar que o modelo gera o documento **uma palavra de cada vez**. A suposição "ingênua" (Naive) é que a escolha de cada palavra é um evento **independente** das outras.

Se nosso documento for a sequência de palavras $X = (X_1, X_2, ..., X_n)$, a probabilidade de gerar essa sequência exata é simplesmente o produto das probabilidades de gerar cada palavra individualmente [2]:

$$P(X|C) = P(X_1|C) \times P(X_2|C) \times \dots \times P(X_n|C) = \prod_{i=1}^{n} P(X_i|C)$$

Onde $X_i$ é a i-ésima palavra do documento. Por exemplo, para o email **"ganhe dinheiro ganhe"** ($n=3$), o cálculo para a classe *Spam* seria:

$$P(\text{"ganhe dinheiro ganhe"}|\text{Spam}) = P(\text{"ganhe"}|\text{Spam}) \times P(\text{"dinheiro"}|\text{Spam}) \times P(\text{"ganhe"}|\text{Spam})$$

Essa abordagem, baseada na probabilidade de uma sequência de eventos independentes, nos leva diretamente à fórmula que precisamos para o classificador.


#### A Fórmula do Classificador Multinomial

Agora, basta inserir nossa expressão de verossimilhança na regra de decisão de Bayes, que obtemos a fórmula do classificador [1]:

$$\hat{C} = \arg\max_C \left[ P(C) \times \prod_{i=1}^{n} P(X_i|C) \right]$$

Finalmente, aplicando o logaritmo para evitar problemas numéricos (underflow) e transformar a multiplicação em uma soma, chegamos à fórmula final que já conhecemos [4]:

$$\hat{C} = \arg\max_C \left[ \log P(C) + \sum_{i=1}^{n} \log P(X_i|C) \right]$$

#### Suavização de Laplace

O último passo é saber como calcular cada $P(X_i|C)$. Como vimos, fazemos isso contando as frequências nos dados de treino e aplicando **Laplace Smoothing** para evitar o problema do zero [3]:

$$P(X_i|C) = \frac{(\text{Contagem da palavra } X_i \text{ na classe } C) + 1}{(\text{Nº total de palavras na classe } C) +  |\text{vocabulário}|}$$


### Complement Naive Bayes (CNB)

Complement Naive Bayes (CNB) é uma adaptação do modelo Multinomial, projetado para lidar melhor com datasets desbalanceados. Em datasets desbalanceados, o Multinomial tende a favorecer classes majoritárias, pois elas têm maior probabilidade a priori $P(C)$. O CNB contorna isso focando em quão bem cada documento se distingue do **complemento** de cada classe.

Em vez de calcular a probabilidade de um documento pertencer à classe $C$, ele calcula quão incompatível o documento é com o **complemento** de $C$ (todas as outras classes) e escolhe a classe que é mais incompatível com seu complemento [5].

#### Classificador Complement Naive Bayes

A decisão é baseada em encontrar a classe que **minimiza** a seguinte expressão [5]:

$$\hat{C} = \arg\min_C \sum_{i=1}^{n} \log P(X_i|\bar{C})$$

A estimativa de $P(X_i|\bar{C})$ segue a mesma lógica da verossimilhança multinomial, mas as contagens são feitas em todas as classes, *exceto* a classe $C$:

$$P(X_i|\bar{C}) = \frac{(\text{Contagem da palavra } X_i \text{ no complemento de } C) + 1}{(\text{Nº total de palavras no complemento de } C) + |\text{vocabulário}|}$$

Mais formalmente:

$$P(X_i|\bar{C}) = \frac{1 + \sum_{c \neq C} N_{ic}}{|\text{vocabulário}| + \sum_{c \neq C} \sum_{j=1}^{|\text{vocabulário}|} N_{jc}}$$

onde $N_{ic}$ é a contagem da palavra $i$ na classe $c$.

#### Exemplo Prático: Classificação de Críticas de Cinema

Vamos usar o cenário com três classes: **Comédia**, **Ação** e **Drama**, com a classe **Comédia** sendo minoritária. Recebemos a nova crítica: **"piada hilária e divertida"**. O CNB calculará a **log-probabilidade do complemento** para cada classe, este valor representa o quão provável é que a crítica pertença ao conjunto de classes do complemento.

Vamos supor que, após os cálculos, o algoritmo chegou aos seguintes valores:

* **Teste para "Comédia"**: (Calcula a log-probabilidade da crítica pertencer a {Ação, Drama})
    * $\sum_{i=1}^{n} \log P(X_i|\overline{\text{Comédia}})$ = **-25.8**

* **Teste para "Ação"**: (Calcula a log-probabilidade da crítica pertencer a {Comédia, Drama})
    * $\sum_{i=1}^{n} \log P(X_i|\overline{\text{Ação}})$ = **-12.5**

* **Teste para "Drama"**: (Calcula a log-probabilidade da crítica pertencer a {Comédia, Ação})
    * $\sum_{i=1}^{n} \log P(X_i|\overline{\text{Drama}})$ = **-14.2**

**Decisão:**

- O classificador usa $\arg\min$, ou seja, ele procura a classe que resultou na **menor log-probabilidade do complemento**. Comparando os resultados, $-25.8$ é o menor valor.

- O valor $-25.8$ para Comédia indica que "piada hilária e divertida" é **muito incompatível** com o conjunto {Ação, Drama}. Quanto menor (mais negativo) esse valor, maior a incompatibilidade com o complemento, logo maior a probabilidade de pertencer à classe testada.

- Portanto, o modelo classifica a crítica como **Comédia**. Ele conclui que a crítica é tão incompatível com o conjunto {Ação, Drama} que ela muito provavelmente pertence à classe "Comédia".

#### Vantagem em Datasets Desbalanceados

Esta abordagem é especialmente eficaz em cenários desbalanceados porque, ao focar no complemento, o algoritmo dá peso similar a todas as classes, independentemente de sua frequência no dataset de treino. Isso permite que classes minoritárias tenham uma chance mais justa de serem selecionadas quando realmente apropriado.

## Referências

[1] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

[2] Izbicki, R., & Santos, T. M. (2020). *Aprendizado de máquina: uma abordagem estatística*. (1st ed.). Rafael Izbicki.

[3] Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

[4] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.

[5] Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003). *Tackling the Poor Assumptions of Naive Bayes Text Classifiers*. Proceedings of the 20th International Conference on Machine Learning (ICML-03).
