# English Version

# Portuguese version

## **Laplace Smoothing**

### O Problema do Zero

Durante o treinamento do Naive Bayes, pode acontecer de um atributo (como uma palavra) nunca aparecer em uma determinada classe. Por exemplo, imagine que no nosso dataset de e-mails, a palavra "promoção" aparece em 50 e-mails da classe spam, mas em **nenhum** e-mail da classe normal.

**Dados de Treinamento:**
* **Total de E-mails:** 3000 (sendo 900 spam e 2100 normais).
* **Probabilidades a Priori:** $P(\text{spam}) = \frac{900}{3000} = 0.3$ e $P(\text{normal}) = \frac{2100}{3000} = 0.7$.
* **Ocorrência da Palavra "promoção":**
    * A palavra "promoção" aparece 80 vezes nos e-mails spam (de um total de 50.000 palavras)
    * A palavra "promoção" aparece 0 vezes nos e-mails normais (de um total de 120.000 palavras)

Isso resulta no seguinte cálculo de probabilidade:

$P(\text{"promoção"}|\text{spam}) = \frac{80}{50000} \approx 0.0016$

$P(\text{"promoção"}|\text{normal}) = \frac{0}{120000} = 0$

O problema surge quando um novo e-mail chega contendo a palavra "promoção" junto com outras palavras que poderiam indicar que é um email normal. Ao usar log-probabilidades para classificar, teremos:

$$\log P(\text{normal}) + \log P(\text{"promoção"}|\text{normal}) + \log P(\text{outras palavras}|\text{normal})$$

Como $P(\text{"promoção"}|\text{normal}) = 0$, temos $\log(0) = -\infty$, e toda a pontuação para a classe "normal" resulta em $-\infty$. O algoritmo **nunca** classificará um e-mail como normal se ele contiver a palavra "promoção", não importa o que as outras palavras do e-mail sugiram. Esse problema é bastante comum que afeta qualquer aplicação de Naive Bayes [4]. 


### A Solução: Adicionar Um (Add-One Smoothing)

A solução mais comum e elegante é a **Suavização de Laplace**, também conhecida como **Add-One Smoothing**. A ideia é simples: antes de calcular as probabilidades, fingimos que já vimos cada palavra do nosso vocabulário **uma vez a mais** em cada classe [2].

Em vez de calcular a probabilidade como uma simples frequência:

$$P(X_i|C) = \frac{\text{Contagem da palavra } X_i \text{ na classe } C}{\text{Nº total de palavras da classe } C}$$

Nós adicionamos `1` ao numerador e o tamanho do vocabulário ao denominador [3]:

$$P(X_i|C) = \frac{(\text{Contagem da palavra } X_i \text{ na classe } C) + 1}{(\text{Nº total de palavras da classe } C) + |\text{vocabulário}|}$$

> [!NOTE]
> **Por que `+ |vocabulário|` no denominador?**
> 
>  * Se adicionamos `1` "ocorrência fantasma" para a palavra $X_i$, para sermos justos e manter a soma das probabilidades igual a 1, precisamos fazer isso para **todas** as palavras do vocabulário. Como temos `|vocabulário|` palavras no total, o denominador aumenta em `|vocabulário|` (uma contagem extra para cada palavra) [3].

**Exemplo prático com smoothing:**

$P(\text{"promoção"}|\text{spam}) = \frac{80 + 1}{50.000 + 10.000} = \frac{81}{60.000} = 0.00135$

$P(\text{"promoção"}|\text{normal}) = \frac{0 + 1}{120.000 + 10.000} = \frac{1}{130.000} \approx 0.0000077$

Agora podemos calcular com log-probabilidades sem problemas:

* **SPAM:** $\log(0.3) + \log(0.00135) = -1.204 + (-6.608) = -7.812$
* **NORMAL:** $\log(0.7) + \log(0.0000077) = -0.357 + (-11.772) = -12.129$

Agora outras palavras no e-mail podem influenciar a classificação final, o modelo não fica mais "travado" em uma decisão absoluta.

Mesmo que a palavra "promoção" nunca tenha aparecido nos e-mails normais, não é razoável assumir que sua presença elimina completamente a possibilidade de um e-mail ser normal. A suavização reflete essa incerteza: ela assume que mesmo eventos não observados podem acontecer, apenas com baixa probabilidade. Isso evita que o modelo "zere" alternativas e se torne excessivamente rígido.

### Base Teórica: MLE vs MAP

A intuição por trás do smoothing tem uma fundamentação teórica sólida. Para entender por que ele funciona, podemos contrastar duas abordagens de estimativa:

- **Estimação por Máxima Verossimilhança (Maximum Likelihood Estimation - MLE): "Só acredito no que vejo nos dados."**
    * Esta abordagem calcula as probabilidades contando simplesmente as ocorrências nos dados de treinamento. O Naive Bayes sem smoothing usa essa filosofia. Sua fórmula é exatamente a que usamos para o cálculo inicial, sem correção [4]:
      
      $$P_{MLE}(X_i \mid C) = \frac{\text{Contagem da palavra } X_i \text{ na classe}}{\text{Número total de palavras da classe } C}$$
  
   * O ponto fraco é que se um evento nunca ocorreu na sua amostra, sua probabilidade é definida como zero, o que torna o modelo frágil.

- **Máximo A Posteriori (MAP): "Eu combino o que eu já acredito com o que os dados me mostram."**
    * A ideia fundamental do MAP é, de fato, **incorporar um "conhecimento prévio" (a priori) aos dados observados** para chegar a uma estimativa final (a posteriori) [1]. 

    * No nosso contexto, a **Suavização de Laplace** é um exemplo perfeito de uma estimativa MAP. O **+1** que adicionamos é uma pseudocontagem. Nosso "conhecimento prévio" é a suposição simples e útil de que toda palavra do vocabulário tem uma pequena chance de aparecer, mesmo que não a tenhamos visto no treino. Ao adicionar uma contagem "fantasma" para cada palavra, estamos embutindo essa crença diretamente no nosso modelo matemático.


## Referências

[1] Bishop, C. M. (2006). *Pattern recognition and machine learning*. Springer.

[2] Izbicki, R., & Santos, T. M. (2020). *Aprendizado de máquina: uma abordagem estatística*. (1st ed.). Rafael Izbicki.

[3] Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer.

[4] Murphy, K. P. (2012). *Machine Learning: A Probabilistic Perspective*. MIT Press.


