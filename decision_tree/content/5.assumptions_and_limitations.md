# English Version

## Overfitting

As decision trees grow and become more complex, they tend to overfit — a process in which a machine learning model fits excessively to the data, capturing not only actual patterns but also noise — resulting in a less accurate model with low generalization capability.

To solve this issue, **pre-pruning** and **post-pruning** techniques are used. In pre-pruning, the growth of the tree is limited through constraints, preventing it from exceeding a certain depth. In post-pruning, after the full growth of the tree structure, some subtrees with irrelevant information are removed from the model. [1]

## High Variance

Decision tree-based models are highly sensitive to data, meaning that small changes in the data can significantly alter the tree's structure.

To address this, ensemble methods are implemented, specifically the **bagging** technique. This method consists of combining multiple base machine learning models to build a stronger model. In the case of bagging, this combination is homogeneous (same algorithm and similar training data). The **Random Forest** model is an ensemble example that uses bagging with decision trees. [2]

To learn more about the Random Forest model, visit: [random_forest](https://github.com/mevianna/ISA/tree/random_forest)

## Bias

Decision trees tend to favor majority classes during splits, resulting in biased predictions.

Like high variance, bias can be addressed through ensemble methods. In this case, the technique used is called **boosting**. In boosting, models use different algorithms and the same training dataset.

Another way to mitigate this bias is to balance the classes using techniques like Oversampling and Undersampling.

**Oversampling** increases the number of samples in the minority class. One of the most common methods is SMOTE (Synthetic Minority Oversampling Technique). **Undersampling**, on the other hand, balances the dataset by reducing the number of majority class samples. [3]

## Computational Cost

Decision trees tend to have a low computational cost during the prediction phase, with a runtime of O(log₂(m)), since the model compares only one feature at each node.

However, during model training, the algorithm compares all features of all training nodes to find the best splits. Thus, the computational cost is O(n × m log(m)), where *n* is the number of features and *m* is the number of samples. One way to reduce this cost is to pre-select the most relevant features, minimizing the number of comparisons. [4]

The methods used to implement the techniques mentioned above can be found at [1.decision_tree_classification](https://github.com/mevianna/ISA/blob/decision_tree/decision_tree/code/1.decision_tree_classification.md).

## References

[1] Witten, I. H., Frank, E., & Hall, M. A. (2011). *Data Mining: Practical Machine Learning Tools and Techniques* (3rd ed.). Morgan Kaufmann.

[2] “1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking,” scikit-learn. [Online]. Available: https://scikit-learn.org/stable/modules/ensemble.html

[3] “User guide: contents — Version 0.13.0,” Imbalanced-learn.org. [Online]. Available: https://imbalanced-learn.org/stable/user_guide.html

[4] A. Geron, *Hands-on Machine Learning with Scikit-Learn and TensorFlow*. Sebastopol, CA, USA: O’Reilly Media, 2017.

# Portuguese Version

## Overfitting

As árvores de decisão, conforme vão crescendo e se tornando mais complexas, tendem ao overfitting - processo em que um modelo de machine learning  se ajusta excessivamente aos dados, não captando apenas os padrões reais, mas também ruídos - resultando em um modelo pouco preciso, com baixa capacidade de generalização.

Para solucionar esse problema, utiliza-se as técnicas de **pré-poda** e **pós-poda**. Na pré-poda, limita-se o crescimento da árvore através de restrições, impedindo que ela ultrapasse uma determinada profundidade. Já na pós-poda, após o crescimento completo da estrutura, algumas subárvores com informações irrelevantes são removidas do modelo.[1]

## Alta variância

O modelo baseado em árvores de decisão possuem uma alta sensibilidade aos dados, ou seja, com pequenas mudanças nos dados, a estrutura da árvore pode mudar significativamente.

Para resolver tal situação, implementasse os métodos de ensemble, mais especificamente a técnica de **bagging**. Esse método consiste na combinação de diferentes modelos base de aprendizado de máquina, afim de construir um modelo mais "forte". No caso do bagging essa combinação é homogênea (mesmo algoritmo e dados de treinamento semelhantes). O modelo de **Random Forest** é um exemplo de ensemble que utiliza a técnica de bagging através das árvores de decisão[2]

Para saber mais sobre o modelo Random Forest, visite: [random_forest](https://github.com/mevianna/ISA/tree/random_forest)

## Viés

As árvores de decisão tendem a favorecer as classes majoritárias durante as divisões, resultando em previsões tendenciosas.

Assim como a alta variânica, o viés pode ser solucionado atravès dos métodos de ensemble. Nessa caso, a técnica utilizada é chamada de **boosting**. Nessa técnica, os modelos possume algoritmos diferentes e um mesmo conjunto de dados de treinamento.

Outro método para mitigar essa tendência é balancear as classes por meio de tècnicas como o Oversampling e o Undersampling.

O **Oversampling** é uma técnica de balanceamento que aumenta a quantidade de dados da classe minoritária, um dos métodos mais comuns é o SMOTE (Synthetic Minority Oversampling Technique). Já o **Undersampling**, realiza o balanceamento diminuindo a quantidade de dados da classe majoritária.[3]

## Custo Computacional

  As árvores de decisão tendem a ter um baixo custo computacional na fase de predição, tendo como tempo de execução O(log₂(m)), uma vez que o modelo realiza a comparação com apenas uma característica em cada nó. 

  No entanto, durante o treinamento do modelo, para realizar a divisão, o algoritmo compara todas as características de todos os nós de treinamento. Desse modo, o custo computacional é de O(n × m log(m)), em que *n* é o número de características e *m* o número de amostras. Uma forma de resolver esse problema é realizar a pré-seleção das características mais relevantes, reduzindo o número de comparações realizadas.[4] 
  
  Os métodos utilizados para realizar as técnicas citadas acima podem ser visualizadas em [1.decision_tree_classification](https://github.com/mevianna/ISA/blob/decision_tree/decision_tree/code/1.decision_tree_classification.md).

## Referências
[1] Witten, I. H., Frank, E., & Hall, M. A. (2011). Data Mining: Practical Machine Learning Tools and Techniques (3rd ed.). Morgan Kaufmann.

[2]	“1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking,” scikit-learn. [Online]. Available: https://scikit-learn.org/stable/modules/ensemble.html. 

[3] “User guide: contents — Version 0.13.0,” Imbalanced-learn.org. [Online]. Available: https://imbalanced-learn.org/stable/user_guide.html. 

[4] A. Geron, Hands-on machine learning with scikit-learn and TensorFlow. Sebastopol, CA, USA: O’Reilly Media, 2017.
