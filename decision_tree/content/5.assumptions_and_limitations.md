# English Version

## Overfitting

As decision trees grow and become more complex, they tend to overfit ‚Äî a process in which a machine learning model fits excessively to the data, capturing not only actual patterns but also noise ‚Äî resulting in a less accurate model with low generalization capability.

To solve this issue, **pre-pruning** and **post-pruning** techniques are used. In pre-pruning, the growth of the tree is limited through constraints, preventing it from exceeding a certain depth. In post-pruning, after the full growth of the tree structure, some subtrees with irrelevant information are removed from the model. [1]

## High Variance

Decision tree-based models are highly sensitive to data, meaning that small changes in the data can significantly alter the tree's structure.

To address this, ensemble methods are implemented, specifically the **bagging** technique. This method consists of combining multiple base machine learning models to build a stronger model. In the case of bagging, this combination is homogeneous (same algorithm and similar training data). The **Random Forest** model is an ensemble example that uses bagging with decision trees. [2]

To learn more about the Random Forest model, visit: [random_forest](https://github.com/mevianna/ISA/tree/random_forest)

## Bias

Decision trees tend to favor majority classes during splits, resulting in biased predictions.

Like high variance, bias can be addressed through ensemble methods. In this case, the technique used is called **boosting**. In boosting, models use different algorithms and the same training dataset.

Another way to mitigate this bias is to balance the classes using techniques like Oversampling and Undersampling.

**Oversampling** increases the number of samples in the minority class. One of the most common methods is SMOTE (Synthetic Minority Oversampling Technique). **Undersampling**, on the other hand, balances the dataset by reducing the number of majority class samples. [3]

## Computational Cost

Decision trees tend to have a low computational cost during the prediction phase, with a runtime of O(log‚ÇÇ(m)), since the model compares only one feature at each node.

However, during model training, the algorithm compares all features of all training nodes to find the best splits. Thus, the computational cost is O(n √ó m log(m)), where *n* is the number of features and *m* is the number of samples. One way to reduce this cost is to pre-select the most relevant features, minimizing the number of comparisons. [4]

The methods used to implement the techniques mentioned above can be found at [1.decision_tree_classification](https://github.com/mevianna/ISA/blob/decision_tree/decision_tree/code/1.decision_tree_classification.md).

## References

[1] Witten, I. H., Frank, E., & Hall, M. A. (2011). *Data Mining: Practical Machine Learning Tools and Techniques* (3rd ed.). Morgan Kaufmann.

[2] ‚Äú1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking,‚Äù scikit-learn. [Online]. Available: https://scikit-learn.org/stable/modules/ensemble.html

[3] ‚ÄúUser guide: contents ‚Äî Version 0.13.0,‚Äù Imbalanced-learn.org. [Online]. Available: https://imbalanced-learn.org/stable/user_guide.html

[4] A. Geron, *Hands-on Machine Learning with Scikit-Learn and TensorFlow*. Sebastopol, CA, USA: O‚ÄôReilly Media, 2017.


## üëæ **Contributors**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/197432407?v=4" width=115><br><sub>Beatriz Schuelter Tartare</sub>](https://github.com/beastartare) |
| :---: |

## **License**  
[![Licen√ßa MIT](https://img.shields.io/badge/Licen√ßa-MIT-blue.svg)](https://pt.wikipedia.org/wiki/Licen%C3%A7a_MIT)  
**Traslation:** Use, modify, and share at will! ‚úåÔ∏è

***

# Portuguese Version

## Overfitting

As √°rvores de decis√£o, conforme v√£o crescendo e se tornando mais complexas, tendem ao overfitting - processo em que um modelo de machine learning  se ajusta excessivamente aos dados, n√£o captando apenas os padr√µes reais, mas tamb√©m ru√≠dos - resultando em um modelo pouco preciso, com baixa capacidade de generaliza√ß√£o.

Para solucionar esse problema, utiliza-se as t√©cnicas de **pr√©-poda** e **p√≥s-poda**. Na pr√©-poda, limita-se o crescimento da √°rvore atrav√©s de restri√ß√µes, impedindo que ela ultrapasse uma determinada profundidade. J√° na p√≥s-poda, ap√≥s o crescimento completo da estrutura, algumas sub√°rvores com informa√ß√µes irrelevantes s√£o removidas do modelo.[1]

## Alta vari√¢ncia

O modelo baseado em √°rvores de decis√£o possuem uma alta sensibilidade aos dados, ou seja, com pequenas mudan√ßas nos dados, a estrutura da √°rvore pode mudar significativamente.

Para resolver tal situa√ß√£o, implementasse os m√©todos de ensemble, mais especificamente a t√©cnica de **bagging**. Esse m√©todo consiste na combina√ß√£o de diferentes modelos base de aprendizado de m√°quina, afim de construir um modelo mais "forte". No caso do bagging essa combina√ß√£o √© homog√™nea (mesmo algoritmo e dados de treinamento semelhantes). O modelo de **Random Forest** √© um exemplo de ensemble que utiliza a t√©cnica de bagging atrav√©s das √°rvores de decis√£o[2]

Para saber mais sobre o modelo Random Forest, visite: [random_forest](https://github.com/mevianna/ISA/tree/random_forest)

## Vi√©s

As √°rvores de decis√£o tendem a favorecer as classes majorit√°rias durante as divis√µes, resultando em previs√µes tendenciosas.

Assim como a alta vari√¢nica, o vi√©s pode ser solucionado atrav√®s dos m√©todos de ensemble. Nessa caso, a t√©cnica utilizada √© chamada de **boosting**. Nessa t√©cnica, os modelos possume algoritmos diferentes e um mesmo conjunto de dados de treinamento.

Outro m√©todo para mitigar essa tend√™ncia √© balancear as classes por meio de t√®cnicas como o Oversampling e o Undersampling.

O **Oversampling** √© uma t√©cnica de balanceamento que aumenta a quantidade de dados da classe minorit√°ria, um dos m√©todos mais comuns √© o SMOTE (Synthetic Minority Oversampling Technique). J√° o **Undersampling**, realiza o balanceamento diminuindo a quantidade de dados da classe majorit√°ria.[3]

## Custo Computacional

  As √°rvores de decis√£o tendem a ter um baixo custo computacional na fase de predi√ß√£o, tendo como tempo de execu√ß√£o O(log‚ÇÇ(m)), uma vez que o modelo realiza a compara√ß√£o com apenas uma caracter√≠stica em cada n√≥. 

  No entanto, durante o treinamento do modelo, para realizar a divis√£o, o algoritmo compara todas as caracter√≠sticas de todos os n√≥s de treinamento. Desse modo, o custo computacional √© de O(n √ó m log(m)), em que *n* √© o n√∫mero de caracter√≠sticas e *m* o n√∫mero de amostras. Uma forma de resolver esse problema √© realizar a pr√©-sele√ß√£o das caracter√≠sticas mais relevantes, reduzindo o n√∫mero de compara√ß√µes realizadas.[4] 
  
  Os m√©todos utilizados para realizar as t√©cnicas citadas acima podem ser visualizadas em [1.decision_tree_classification](https://github.com/mevianna/ISA/blob/decision_tree/decision_tree/code/1.decision_tree_classification.md).

## Refer√™ncias
[1] Witten, I. H., Frank, E., & Hall, M. A. (2011). Data Mining: Practical Machine Learning Tools and Techniques (3rd ed.). Morgan Kaufmann.

[2]	‚Äú1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking,‚Äù scikit-learn. [Online]. Available: https://scikit-learn.org/stable/modules/ensemble.html. 

[3] ‚ÄúUser guide: contents ‚Äî Version 0.13.0,‚Äù Imbalanced-learn.org. [Online]. Available: https://imbalanced-learn.org/stable/user_guide.html. 

[4] A. Geron, Hands-on machine learning with scikit-learn and TensorFlow. Sebastopol, CA, USA: O‚ÄôReilly Media, 2017.

## üëæ **Contribuidores**  
| [<img loading="lazy" src="https://avatars.githubusercontent.com/u/197432407?v=4" width=115><br><sub>Beatriz Schuelter Tartare</sub>](https://github.com/beastartare) |
| :---: |

## **Licen√ßa**  
[![Licen√ßa MIT](https://img.shields.io/badge/Licen√ßa-MIT-blue.svg)](https://pt.wikipedia.org/wiki/Licen%C3%A7a_MIT)  
**Traslation:** Use, modifique e compartilhe √† vontade! ‚úåÔ∏è
