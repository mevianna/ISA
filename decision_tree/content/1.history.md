# English Version
The history of decision trees can be traced back to two main lines of development. The first is more closely related to classification, originating with the CLS (Concept Learning System) algorithm, which later gave rise to algorithms such as ID3 and C4.5. The second line is more connected to regression, with the AID (Automatic Interaction Detector) system standing out, whose approach influenced algorithms such as CART. We will discuss all of this in more detail below!

## Classification

### CLS (Concept Learning System)
Decision trees trace their origins to the development of the **CLS (Concept Learning System)**, created by **Hunt, Marin, and Stone** in the 1960s. This system is considered the **"patriarch"** of the so-called **TDIDT (Top-Down Induction of Decision Trees)**[1] family, which has been widely explored in the field of Machine Learning.

> [!NOTE]
> The CLS is considered one of the first methods to employ the **top-down induction of decision trees** approach.[1]  
>
> "Induction" means creating a general model from specific examples.  
>  
> "Top-down" means that the tree is built from the top (root) down to the leaves.

Hunt’s system builds decision trees by trying to **minimize the cost of classification**, which involves two factors: the cost of measuring a feature of the object and the cost of misclassification.[1]  

The cost of measuring a feature refers, for example, to when the measurement is expensive or difficult to obtain.  
The cost of misclassification is related to deciding that an object belongs to one class when, in fact, it belongs to another.

To minimize the cost, Hunt’s system uses a **lookahead** strategy.[1]  

That is, it simulates a few steps ahead before deciding how to split the tree.

From what was developed by Hunt et al., the ID3 algorithm was later created.

### ID3
ID3 was developed by J.R. Quinlan during the late 1970s and early 1980s.  
A fundamental change was the **replacement of cost-based evaluation with an information-oriented evaluation function**.[1]  

This new evaluation works as follows:  
1. Start with a small sample of the data (window)  
2. Build a tree that correctly classifies this sample  
3. Test the tree on the remaining data  
4. If it classifies everything correctly, the process ends. If not, the misclassified examples are added to the sample and the process repeats  

### C4.5
C4.5 was also introduced by Quinlan. Developed in the 1990s, C4.5 is an evolution of ID3 and brought several improvements, such as the ability to handle continuous data, deal with missing values, apply pruning after tree construction, and use the gain ratio for attribute selection, avoiding the bias present in ID3.[2]

## Regression

### AID
**AID (Automatic Interaction Detection)**, developed by **Morgan and Sonquist**, was the first regression tree algorithm published in the literature, in 1963.[3]  

AID **splits the data** into two child nodes in order to **minimize the variance within subgroups**, until it meets **predefined stopping criteria**. Then, the tree stops growing. However, these criteria still allowed the tree to overfit the training data, leading to the overfitting phenomenon.[3]  

>[!NOTE]  
> Overfitting occurs when the model fits the training data too closely and loses its ability to generalize.  
> We will explore this concept in more detail in the context of decision trees on (link to page). 

### CART
**CART** stands for **Classification And Regression Trees** and was created by **Breiman, Friedman, Olshen, and Stone** in 1984.[3]  

One of the main differences between CART and AID is that, instead of using predefined stopping criteria, the algorithm allows the tree to grow fully and then **applies "pruning"**.[3]  

> Although applicable to both regression and classification problems, CART is included in this section because it was inspired by AID — not by the CLS or ID3 family of algorithms.  

>[!NOTE]  
> We will cover this pruning concept in more detail on (link to page).

## References
[1]  Quinlan, J.R. Induction of decision trees. Mach Learn 1, 81–106 (1986). https://doi.org/10.1007/BF00116251

[2] Badr HSSINA, Abdelkarim MERBOUHA, Hanane EZZIKOURI and Mohammed ERRITALI, “A comparative study of decision tree ID3 and C4.5” International Journal of Advanced Computer Science and Applications(IJACSA), Special Issue on Advances in Vehicular Ad Hoc Networking and Applications 2014, 2014. http://dx.doi.org/10.14569/SpecialIssue.2014.040203

[3] Loh, W.-Y. (2014). Fifty Years of Classification and Regression Trees. International Statistical Review, 82(3), 329–348. https://doi.org/10.1111/insr.12016

--------------------------------------------------
# Portuguese Version

A história das árvores de decisão pode ser traçada a partir de duas linhas principais de desenvolvimento. A primeira está mais relacionada à classificação, tendo origem no algoritmo CLS (Concept Learning System), que mais tarde deu origem a algoritmos como o ID3 e o C4.5. A segunda linha está mais conectada à regressão, com destaque para o sistema AID (Automatic Interaction Detector), cuja abordagem influenciou algoritmos como o CART. Abordaremos tudo isso com mais detalhes a seguir!

## Classificação

### CLS (Concept Learning System)

As árvores de decisão têm suas origens atreladas ao desenvolvimento do **CLS (Concept Learning System)**, criado por **Hunt, Marin e Stone** na década de 1960. Esse sistema é considerado o **"patriarca"** da chamada família **TDIDT (Top-Down Induction of Decision Trees)**[1], amplamente explorada no campo de Machine Learning. 

>[!NOTE]
> O CLS é considerado um dos primeiros métodos a empregar a abordagem de **indução descendente para construção de árvores de decisão**.[1]
>
> "Indução" significa criar um modelo geral a partir de exemplos específicos
> 
> "Descendente" significa que a árvore é construída do topo (raiz) para baixo (folhas)

O sistema de Hunt constrói árvores de decisão tentando **minimizar o custo da classificação**, que envolve dois fatores: o custo de medir uma característica do objeto e o custo de errar a classificação. [1] 

O custo de medir uma característica do objeto se refere, por exemplo, a quando a medição é cara ou difícil de obter. Já o custo de errar a classificação está relacionado a decidir que o objeto pertence a uma classe quando, na verdade, ele pertence a outra.

Para minimizar o custo, ele usa uma estratégia de **“lookahead”**. [1] 

Isto é, ele simula alguns passos à frente antes de tomar a decisão de como dividir a árvore.

A partir do que foi desenvolvido por Hunt et al, temos o ID3.

### ID3
O ID3 foi desenvolvido por J.R. Quinlan durante o final da década de 70 e início da década de 80. Uma mudança fundamental foi a **substituição da avaliação baseada em custo por uma função de avaliação orientada a informações**. [1]

Essa nova avaliação funciona da seguinte forma:
1. Começa com uma amostra pequena dos dados (janela)
2. Cria-se uma árvore que acerta essa amostra.
3. Testa-se a árvore no restante dos dados.
4. Se acertar tudo, o processo termina. Se errar, os exemplos incorretos são adicionados à amostra e o processo se repete.

### C4.5
Foi um algoritmo apresentado também por Quinlan. O C4.5, desenvolvido na década de 90, é uma evolução do ID3 e trouxe diversas melhorias, como a capacidade de lidar com dados contínuos, tratar valores ausentes, aplicar a poda da árvore após sua construção e utilizar a razão de ganho para a escolha dos atributos, evitando o viés presente no ID3. [2]


## Regressão

### AID
O **AID (Automatic Interaction Detection)**, de **Morgan e Sonquist**, foi o primeiro algoritmo de árvore de regressão publicado na literatura, em 1963. [3]

AID **divide os dados** em dois nós filhos de forma a **minimizar a variância dentro dos subgrupos**, até que atingisse **critérios de parada pré-definidos**. Então, a árvore parava de crescer. No entanto, esses critérios ainda permitiam que a árvore se ajustasse demais aos dados de treinamento, levando ao fenômeno de overfitting. [3]

>[!NOTE]
> Overfitting ocorre quando o modelo se ajusta excessivamente aos dados de treinamento e perde sua capacidade de generalização.
> Veremos com mais detalhes esse conceito no contexto de arvores de decisão em (link para pagina). 

### CART
**CART** significa **Classification And Regression Trees** e foi criado por **Breiman, Friedman, Olshen e Stone** em 1984. [3]

Uma das principais diferenças do CART para o AID é que, em vez de utilizar critérios de parada pré-definidos, o algoritmo deixa a árvore crescer e, depois, **aplica a "poda"**. [3]

> Apesar de ser aplicável tanto a problemas de regressão quanto de classificação, o CART foi incluído nesta seção por ter sido inspirado no AID — e não nos algoritmos da linha CLS ou ID3.

>[!NOTE]
> Veremos com mais detalhes esse conceito de poda em (link para pagina). 



## Referências
[1]  Quinlan, J.R. Induction of decision trees. Mach Learn 1, 81–106 (1986). https://doi.org/10.1007/BF00116251

[2] Badr HSSINA, Abdelkarim MERBOUHA, Hanane EZZIKOURI and Mohammed ERRITALI, “A comparative study of decision tree ID3 and C4.5” International Journal of Advanced Computer Science and Applications(IJACSA), Special Issue on Advances in Vehicular Ad Hoc Networking and Applications 2014, 2014. http://dx.doi.org/10.14569/SpecialIssue.2014.040203

[3] Loh, W.-Y. (2014). Fifty Years of Classification and Regression Trees. International Statistical Review, 82(3), 329–348. https://doi.org/10.1111/insr.12016

