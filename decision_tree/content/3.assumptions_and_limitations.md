# English Version


# Portuguese Version

## Overfitting

As árvores de decisão, conforme vão crescendo e se tornando mais complexas, tendem ao overfitting - processo em que um modelo de machine learning  se ajusta excessivamente aos dados, não captando apenas os padrões reais, mas também ruídos - resultando em um modelo pouco preciso, com baixa capacidade de generalização.

Para solucionar esse problema, utiliza-se as técnicas de pré-poda e pós-poda. Na pré-poda, limita-se o crescimento da árvore através de restrições, impedindo que ela ultrapasse uma determinada profundidade. Já na pós-poda, após o crescimento completo da estrutura, algumas subárvores com informações irrelevantes são removidas do modelo.[1]

## Alta variância

O modelo baseado em árvores de decisão possuem uma alta sensibilidade aos dados, ou seja, com pequenas mudanças nos dados, a estrutura da árvore pode mudar significativamente.

Para resolver tal situação, implementasse os métodos de ensemble. Esse método consiste na combinação de diferentes modelos de aprendizado de máquina, afim de construir um modelo mais "forte". Essa combinação pode tanto ser homogênea (mesmo algoritmo e dados de treinamento semelhantes) como heterogênea (diferentes algoritmos). O método mais comum são os ensembles homogêneos, como exemplo a Random Forest.[2]

Para saber mais sobre o modelo Random Forest, visite: [link]

## Viés

As árvores de decisão tendem a favorecer as classes majoritárias durante as divisões, resultando em previsões tendenciosas.

Além da Random Forest, outro método para mitigar essa tendência é balancear as classes por meio de tècnicas como o Oversampling e o Undersampling.

O Oversampling é uma técnica de balanceamento que aumenta a quantidade de dados da classe minoritária, um dos métodos mais comuns é o SMOTE (Synthetic Minority Oversampling Technique). Já o Undersampling, realiza o balanceamento diminuindo a quantidade de dados da classe majoritária.[3]

## Custo Computacional

Para árvores muito complexas, com grande número de dados, o custo computacional - como memória e tempo de CPU, tanto para construir a árvore como para percorrer - tende a aumentar, tornando esse modelo menos eficiênte. Novamente, a Random Forest surge como uma alternativa para solucionar esse problema.

## Referências
