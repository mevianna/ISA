# English Version


# Portuguese Version

## Overfitting

As árvores de decisão, conforme vão crescendo e se tornando mais complexas, tendem ao overfitting - processo em que um modelo de machine learning  se ajusta excessivamente aos dados, não captando apenas os padrões reais, mas também ruídos - resultando em um modelo pouco preciso, com baixa capacidade de generalização.

Para solucionar esse problema, utiliza-se as técnicas de **pré-poda** e **pós-poda**. Na pré-poda, limita-se o crescimento da árvore através de restrições, impedindo que ela ultrapasse uma determinada profundidade. Já na pós-poda, após o crescimento completo da estrutura, algumas subárvores com informações irrelevantes são removidas do modelo.[1]

## Alta variância

O modelo baseado em árvores de decisão possuem uma alta sensibilidade aos dados, ou seja, com pequenas mudanças nos dados, a estrutura da árvore pode mudar significativamente.

Para resolver tal situação, implementasse os métodos de ensemble, mais especificamente a técnica de **bagging**. Esse método consiste na combinação de diferentes modelos base de aprendizado de máquina, afim de construir um modelo mais "forte". No caso do bagging essa combinação é homogênea (mesmo algoritmo e dados de treinamento semelhantes). O modelo de **Random Forest** é um exemplo de ensemble que utiliza a técnica de bagging através das árvores de decisão[2]

Para saber mais sobre o modelo Random Forest, visite: [link]

## Viés

As árvores de decisão tendem a favorecer as classes majoritárias durante as divisões, resultando em previsões tendenciosas.

Assim como a alta variânica, o viés pode ser solucionado atravès dos métodos de ensemble. Nessa caso, a técnica utilizada é chamada de **boosting**. Nessa técnica, os modelos possume algoritmos diferentes e um mesmo conjunto de dados de treinamento.

Outro método para mitigar essa tendência é balancear as classes por meio de tècnicas como o Oversampling e o Undersampling.

O **Oversampling** é uma técnica de balanceamento que aumenta a quantidade de dados da classe minoritária, um dos métodos mais comuns é o SMOTE (Synthetic Minority Oversampling Technique). Já o **Undersampling**, realiza o balanceamento diminuindo a quantidade de dados da classe majoritária.[3]

## Custo Computacional

  As árvores de decisão tendem a ter um baixo custo computacional na fase de predição, tendo como tempo de execução O(log₂(m)), uma vez que o modelo realiza a comparação com apenas uma característica em cada nó. 

  No entanto, durante o treinamento do modelo, para realizar a divisão, o algoritmo compara todas as características de todos os nós de treinamento. Desse modo, o custo computacional é de O(n × m log(m)), em que n é o número de características e m o número de amostras. Uma forma de resolver esse problema é realizar a pré-seleção das características mais relevantes, reduzindo o número de comparações realizadas. Os métodos utilizados para realizar essa técnica pode ser visualizada em [link].

## Referências
