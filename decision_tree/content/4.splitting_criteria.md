# English Version

# Portuguese Version

## Qual a melhor ordem de testes?

Devemos ter o quanto antes o teste que:
- Para regressão, faz ter menor variabilidade entre os valores dos dados separados.
  
- Para classificação, mais separa as classes, fazendo os nós serem mais "puros".

De forma mais objetiva, temos a minimizacão de algumas funcoes específicas relacionadas ao erro (para regressão) ou a impurezas (para classificação). Vamos considerá-las a partir de agora:

### Regressão
(residual sum-of-squares)

Devemos ter o **menor RSS (Soma dos Quadrados dos Resíduos)** possível.[1] 

Sua fórmula, retirada de [3], é:

$$ 
RSS = e_1^2 + e_2^2 + \cdots + e_n^2 
$$ 

Em que:
- $RSS$ é a Soma dos Quadrados dos Resíduos.
- $e_i$ é o erro cometido pelo modelo na i-ésima previsão.

#### Exemplo




### Classificacao
Temos **duas formas** de avaliar se a divisão está promovendo um bom desempenho: **entropia (cross-entropy) e o índice de Gini**.

#### Entropia

A Entropia tem a seguinte fórmula, retirada de [1]:

$$
Q_\tau(T) = \sum_{k=1}^{K} p_{\tau k} \ln p_{\tau k}
$$

Em que:
- $Q_\tau(T)$: impureza (entropia) do nó ou região $R_\tau$. Esse valor diz quão misturado está o nó em relação às classes.
Quanto menor o valor, mais puro (ou homogêneo) é o nó.
- $\sum_{k=1}^{K}$: soma sobre todas as $K$ classes possíveis.
- $p_{\tau k}$: proporção de exemplos da classe $k$ no nó $\tau$.
- $\ln p_{\tau k}$: logaritmo natural da proporção. 

#### Índice de Gini

O Índice de Gini tem a seguinte fórmula, retirada de [1]:

$$
Q_\tau(T) = \sum_{k=1}^{K} p_{\tau k} (1 - p_{\tau k})
$$

Em que:
- $Q_\tau(T)$: medida de impureza de Gini da região (ou nó) $R_\tau$. Ela indica o quão misturado está o nó em relação às classes.
- $\sum_{k=1}^{K}$: soma sobre todas as $K$ classes possíveis.
- $p_{\tau k}$: proporção de exemplos da classe $k$ no nó $R_\tau$.
- $(1 - p_{\tau k})$: complemento da proporção da classe $k$.
- produto $p_{\tau k}(1 - p_{\tau k})$: representa a impureza de se escolher um exemplo da classe $k$.

## Referências
[1] Bishop, C. M. (2006). Pattern recognition and machine learning. Springer.

[2] Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction (2nd ed.). Springer.

[3] James, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, A. (2023). An Introduction to Statistical Learning with Applications in Python. Springer.
