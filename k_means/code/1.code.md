# English Version
In this section, the execution of the K-Means Clustering algorithm using the *Python* programming language will be presented and demonstrated. The implementation that will be explored follows the same structure described in: [6.How the algorithm is executed?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/6.how_the_algorithm_is_executed.md).

***

# Portuguese Version
Nessa seção, será apresentada e demonstrada a execução do algoritmo K-Means Clustering utilizando a linguagem de programação *Python*. A implementação que será explorada segue a mesma estrutura descrita em: [6.Como o algoritmo é executado?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/6.how_the_algorithm_is_executed.md). Na execução abordada nesse arquivo, foi utilizado um Dataset já existente.

***

## Detalhes do Dataset
Para executar o K-Means, foi utilizado o dataset "Iris", disponível em: [Iris Dataset - UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/53/iris). Esse Dataset é um dos mais utilizados para estudos de Machine Learning, possuindo 150 linhas de dados, divididos em 3 classes, as quais cada uma representa um tipo de flor Iris, e 5 colunas, sendo elas: sepal_length, sepal_width, petal_length, petal_width e species. As quatro primeiras colunas representam caracteristícas morfológicas das flores, enquanto a última coluna ("species") corrresponde à classe da flor, isto é, o rótulo verdadeiro (target). Cabe ressaltar que esse dataset não possui valores nulos.

>[!NOTE]
> Como o "target" é o rótulo verdadeiro, ele não é utilizado no K-Means, uma vez que o algoritmo é não-supervisionado, ou seja, ele agrupa os dados sem conhecer suas reais classificações.

***

## Código:

### Importação de bibliotecas e funções:

```python 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
```

No trecho de código acima, são importadas bibliotecas e funções que serão utilizadas ao longo do código, sendo elas:

* ```import pandas as pd```: Importa a biblioteca "pandas", utilizada para manipulação e análise de dados, nomeada como "pd";
* ```import numpy as np```: Importa a biblioteca "numpy", utilizada para operações matemáticas e matriciais, nomeada como "np";
* ```import matplotlib.pyplot as plt```: Importa o módulo "pyplot" da biblioteca "matplotlib", utilizada para visualização gráfica, nomeada como "plt";
* ```from sklearn.cluster import KMeans```: Importa a classe "KMeans" do módulo de agrupamento do "scikit-learn", que implementa o algoritmo de clusterização K-Means.
* ```from sklearn.preprocessing import StandardScaler```: Importa a classe "StandardScaler" do módulo de agrupamento do "scikit-learn", que realiza a normalização dos dados o algoritmo de clusterização K-Means.
* ```from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score```: Importa três métricas de validação do módulo de agrupamento do "scikit-learn".

### Importação do Dataset
```python
url = "https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv"
df = pd.read_csv(url)
```

No trecho de código acima, o Dataset é lido a partir de um link disponível na internet. Ele é definido, então, como "df" para o resto do código.

> Sugestão: para obter caracteristícas do Dataset, como a quantidade de linhas, se há espaços em branco, nome das colunas, execute: 
> ```python 
> print(df.head())
> print(df.shape)
> df.isnull().values.any()
> ```

### Padronização dos dados
A normalização (padronização) é um processo de transformação dos dados para que fiquem em uma mesma escala, sem distorcer as relações entre eles. Isso é necessário pois os atributos podem possuir escalas diferente; por exemplo, enquanto a idade pode variar entre 0 e 100, o salário pode estar na casa dos milhares.
Se não normalizados, atributos com maiores valores podem dominar algoritmos baseados em distância, como o K-Means. Para resolver isso, utiliza-se uma função que padroniza os dados, transformando-os de forma que tenham média 0 e desvio padrão 1, tornando todos os atributos igualmente relevantes para os cálculos. No scikit-learn, isso é feito com a classe StandardScaler, conforme visto abaixo:

```python 
X = df[['sepal_length',  'sepal_width',  'petal_length',  'petal_width']]
scaler = StandardScaler()
X_padronizado = scaler.fit_transform(X)
```

Na primeira linha, são separadas apenas as linhas "feature" do dataset, ou seja, os atributos utilizados para o agrupamento. Por ventura, nesse dataset, todas as colunas são numéricas, então não é preciso nenhuma manipulação em relação a isso. Assim, "X" é uma tabela com as 4 primeiras colunas do dataset e todas as linhas do conjunto de dados.

A segunda linha cria um objeto do tipo StandardScaler, que é utilizado para padronizar os dados. Assim, essa padronização é aplicada na nova tabela na terceira linha, em que "X_scaled" é a nova tabela, com as 4 colunas padronizadas, isto é, com valores com desvio padrão 1 e média 0.

### Plotagem dos dados sem agrupamentos
Para a plotagem do gráfico dos dados sem agrupamentos, foram utilizadas apenas duas dimensões, ou seja, dois atributos do conjunto de dados. Nesse caso, optou-se pelos atributos "sepal_length" e "sepal_width", o que permite uma visualização bidimensional mais simples e direta da distribuição dos dados. O gráfico correspondente pode ser visualizado abaixo, logo após o trecho de código responsável pela sua geração.

```python
plt.figure(figsize=(6, 5))
plt.scatter(X_padronizado[:, 0], X_padronizado[:, 1], c='hotpink')
plt.xlabel("Sepal length")
plt.ylabel("Sepal width")
plt.title("Dados Iris sem agrupamento")
plt.show()
``` 

<div align="center">
<img src= "..\img\dataset1_without_clusters.png" alt="Plotagem dos dados sem agrupamento" width="400">
</div>

### Como definir o número de clusters

### Método do cotovelo
### Método da silhueta

### Definição do número de clusters

### Plotagem do gráfico com os agrupamentos

### Métricas de validação
