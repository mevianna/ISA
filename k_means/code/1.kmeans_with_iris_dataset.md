# English Version
In this section, the execution of the K-Means Clustering algorithm using the *Python* programming language will be presented and demonstrated. The implementation that will be explored follows the same structure described in: [6.How the algorithm is executed?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/6.how_the_algorithm_is_executed.md).

***

# Portuguese Version
Nessa seção, será apresentada e demonstrada a execução do algoritmo K-Means Clustering utilizando a linguagem de programação *Python*. A implementação que será explorada segue a mesma estrutura descrita em: [6.Como o algoritmo é executado?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/6.how_the_algorithm_is_executed.md). Na execução abordada nesse arquivo, foi utilizado um Dataset já existente.

***

## Detalhes do Dataset
Para a execução do algoritmo K-Means, foi utilizado o dataset "Iris", disponível em: [Iris Dataset - UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/53/iris). Esse conjunto de dados é um dos mais utilizados para estudos de Machine Learning, dada a sua simplicidade e estrutura. Ele contém 150 linhas de dados, isto é, 150 amostras de flores da espécie Iris, divididas em 3 classes, que representam tipos de flores Iris diferentes.
O dataset possui cinco colunas: sepal_length, sepal_width, petal_length, petal_width e species. As quatro primeiras representam caracteristícas morfológicas das flores, enquanto a última indica a classe da flor, ou seja, o rótulo verdadeiro (target). Cabe ressaltar que esse dataset não possui valores ausentes.

>[!NOTE]
> Como o campo "species" representa o rótulo verdadeiro das amostras, ele não é utilizado diretamente no K-Means, pois se trata de um algoritmo não-supervisionado. Isso significa que o K-Means agrupa os dados sem conhecimento prévios das reais classificações.

***

## Código:

### Importação de bibliotecas e funções:

```python 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
```

No trecho de código acima, são importadas bibliotecas e funções que serão utilizadas ao longo do código, sendo elas:

* ```import pandas as pd```: Importa a biblioteca "pandas", utilizada para manipulação e análise de dados, nomeada como "pd";
* ```import numpy as np```: Importa a biblioteca "numpy", utilizada para operações matemáticas e matriciais, nomeada como "np";
* ```import matplotlib.pyplot as plt```: Importa o módulo "pyplot" da biblioteca "matplotlib", utilizada para visualização gráfica, nomeada como "plt";
* ```from sklearn.cluster import KMeans```: Importa a classe "KMeans" do módulo de agrupamento do "scikit-learn", que implementa o algoritmo de clusterização K-Means.
* ```from sklearn.preprocessing import StandardScaler```: Importa a classe "StandardScaler" do módulo de agrupamento do "scikit-learn", que realiza a normalização dos dados o algoritmo de clusterização K-Means.
* ```from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score```: Importa três métricas de validação do módulo de agrupamento do "scikit-learn".

### Importação do Dataset
```python
url = "https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv"
df = pd.read_csv(url)
```

No trecho de código acima, o Dataset é carregado a partir de um link da internet e armazenado na variável **df**, que será utilizado ao longo de todo código.

> [!TIP]
> Para visualizar caracteristícas básicas do Dataset, como os nomes das colunas, a quantidade de linhas e verificar se há valores nulos, pode-se executar: 
> ```python 
> print(df.head())
> print(df.shape)
> df.isnull().values.any()
> ```

### Padronização dos dados
A padronização (ou normalização) é um processo de transformação dos dados para que todos os atributos estejam na mesma escala, sem distorcer as relações entre eles. Isso é necessário pois os atributos podem possuir escalas diferente; por exemplo, enquanto a idade pode variar entre 0 e 100, o salário pode estar na casa dos milhares. Nesse caso, sem a normalização, os altos valores de salário em comparação com a idade podem distorcer os resultados reais - este é um problema tipíco dos algoritmos que dependem de distância.

Para resolver isso, utiliza-se uma função que padroniza os dados, transformando-os de forma que tenham média 0 e desvio padrão 1, tornando todos os atributos igualmente relevantes para os cálculos. No scikit-learn, essa tarefa é feita com a classe StandardScaler, conforme visto abaixo:

```python 
X = df[['sepal_length',  'sepal_width',  'petal_length',  'petal_width']]
scaler = StandardScaler()
X_padronizado = scaler.fit_transform(X)
```

Na primeira linha, são separadas apenas as linhas de caracteristicas (feature) do dataset, ou seja, os atributos que serão utilizados no agrupamento. No caso do dataset Iris, todas essas colunas são numéricas, não precisando, então, de nenhum tipo de conversão. Dessa forma, a variável "X" é uma matriz contendo as 4 primeiras colunas do dataset e todas as suas linhas.

Na segunda linha, é criado um objeto do tipo ```StandardScaler```, da biblioteca *scikit-learn*. Ele é utilizado para padronizar os dados, de forma que cada atributo tenha média 0 e desvio padrão 1. 

Por fim, na terceira linha, a padronização é aplicada sobre X, resultando a nova matriz "X_padronizado". Essa matriz contém os mesmos dados, porém, normalizados.

### Plotagem dos dados sem agrupamentos
No conjunto de dados, há 4 colunas de atributos, isto é, 4 dimensões. Como não é possível visualizar dados diretamente em mais de 3 dimensões, optou-se por simplificar e representar dados em apenas duas dimensões.

Nesse exemplo, foram escolhidos os atributos "sepal_length" e "sepal_width", mas qualquer outro par de atributos poderia ser utilizado. A seguir, há o trecho de código que realiza a plotagem do gráfico.

```python
plt.figure(figsize=(6, 5))
plt.scatter(X_padronizado[:, 0], X_padronizado[:, 1], c='hotpink')
plt.xlabel("Sepal length")
plt.ylabel("Sepal width")
plt.title("Dados Iris sem agrupamento")
plt.show()
``` 

A primeira linha, ```python plt.figure(figsize=(6, 5))```, cria a figura que será o gráfico, definindo o seu tamanho como 6x5 polegadas. Em seguida, ```python plt.scatter(X_padronizado[:, 0], X_padronizado[:, 1], c='hotpink')```, gera um gráfico de dispersão, o qual plota pontos numa figura 2D. Nessa função,
* O primeiro argumento, ```python X_padronizado[:, 0]```, fornece os dados do eixo X. Ele seleciona todas as linhas da coluna 0 (primeira coluna) da matriz X_padronizada.
* O segundo argumento, ```python X_padronizado[:, 1]```, faz o mesmo para o eixo Y, escolhendo todas as linhas da segunda coluna (coluna 1) da matriz X_padronizada. 
* O último argumento, ```python c='hotpink'```, determina a cor dos pontos como "hotpink".

As 2 linhas seguintes determinam rõtulos do gráfico, sendo, em ordem, o rótulo do eixo X e o rótulo do eixo Y. A linha posterior determina o título do grãfico. 

Por fim, a última linha, ```python plt.show()``` exibe o gráfico formado, o qual pode ser visto abaixo.

<div align="center">
<img src= "..\img\dataset1_without_clusters.png" alt="Plotagem dos dados sem agrupamento" width="400">
</div>

### Como definir o número de clusters
Como discutido anteriormente em [4.Como escolher o número de clusters?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/3.how_to_choose_the_number_of_clusters.md), existem diversos métodos para determinar o número de clusters. Neste exemplo, serão utilizados os métodos do cotovelo e da silhueta.

### Método do cotovelo
Uma das formas mais utilizadas para determinar o número de clusters, é através do método do cotovelo. Para implementar esse método, é executado o seguinte código:

```python
inercias = []
valores_k = range(1, 11)

for k in valores_k:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_padronizado)
    inercias.append(kmeans.inertia_)

plt.plot(valores_k, inercias, 'mo-')
plt.xlabel("Número de clusters (K)")
plt.ylabel("Inércia")
plt.title("Método do Cotovelo")
plt.show()
```

Na primeira linha, ```python inercias = []```, é criada uma lista vazia que irá armazenar os valores da inércia. A linha seguinte, ```python valores_k = range(1, 11)```, gera uma sequência de 1 a 10, que são os valores de k a serem testados.

A terceira linha é um loop *for*, o qual percorre os valores da sequência gerada com a variável k. A linha posterior, ```python kmeans = KMeans(n_clusters=k, random_state=42)```, cria um objeto do algoritmo KMeans, com *k* clusters e semente aleatória.

A linha seguinte, ```python kmeans.fit(X_padronizado)```, treina o objeto KMeans criado com os dados da matriz X_padronizada. A linha seguinte, ```python inercias.append(kmeans.inertia_)```, adiciona na lista a inércia da execução atual do KMeans, a qual é um atributo do objeto KMeans.

Após todos os testes, a linha ```python plt.plot(valores_k, inercias, 'mo-')``` cria o gráfico de linha para o método do cotovelo. O primeiro argumento representa o valor do eixo X; o segundo, o valor do eixo Y; e, o terceiro, uma formatação, em que:
* "m": define a cor da linha como magenta;
* "o": adiciona marcadores em formato de circulo a cada ponto de dados no gráfico;
* "-": traça uma linha entre os pontos do gráfico.

As linhas posteriores, apenas rotulam o gráfico e o exibem. Ele pode ser visualizado abaixo:

<div align="center">
<img src= "..\img\elbow_method_dataset1.png" alt="Plotagem do método do cotovelo" width="400">
</div>

Analisando o gráfico, é possível ver que a "dobra" do cotovelo está mais nitída em **k = 3**. Assim, o melhor valor de k, segundo este método, é 3.

### Método da silhueta
Outra maneira de determinar o melhor valor de k, é através do método da silhueta. Para tal, executamos o seguinte código:

```python
silhueta = []
valores_k = range(2, 11)

for k in valores_k:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_padronizado)
    score = silhouette_score(X_padronizado, labels)
    silhueta.append(score)

plt.plot(valores_k, silhueta, 'mo-')
plt.xlabel("Número de clusters (k)")
plt.ylabel("Silhueta média")
plt.title("Método da Silhueta para escolha de k")
plt.grid(True)
plt.show()
```

Como o código é bastante semelhante ao do método do cotovelo, serão comentadas apenas as principais diferenças.

Na linha ```python valores_k = range(2, 11)```, é criada uma sequência de 2 a 10, uma vez que o método da silhueta não é determinado para o valor igual a 1.

Em ```python labels = kmeans.fit_predict(X_padronizado)```, além do treino de dados, essa linha atribui cada ponto ao seu centróide mais próximo, retornando uma lista (labels). Nessa lista, a posição é um ponto de dado do dataset, e o valor desta posição é o número do cluster que o ponto de dado foi atribuído.

Em seguida, ```python score = silhouette_score(X_padronizado, labels)```, calcula a pontuação média da silhueta para cada valor de k. Na linha seguinte, esse valor é adicionado na lista de silhuetas.

Por fim, o gráfico é plotado da mesma forma que os anteriores. A única diferença é a linha ```python plt.grid(True)```, que adiciona grades ao fundo para melhor visualização. O gráfico formado pode ser visualizado abaixo.

<div align="center">
<img src= "..\img\sillhoutte_method_dataset1.png" alt="Plotagem do método da silhueta" width="400">
</div>

Nele, é possível visualizar que o maior valor de silhueta é 2. Dessa forma, o número de clusters, segundo esse método, é 2.

> [!WARNING]
> É notável que os dois métodos obtiveram resultados diferentes. Isso acontece pois cada um utiliza formas diferentes de cálculos, assim, o resultado nem sempre é o mesmo. Assim, é interessante executar o algoritmo com os dois valores de clusters para visualizar qual obtém o melhor desempenho.

### Definição do número de clusters
Inicialmente, será utilizado **k = 3**. Assim, essa atribuição é feita abaixo.

```python
k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
labels = kmeans.fit_predict(X_padronizado)
```

Na primeira linha, é definido que o número de clusters é 3. Em seguida, ```python kmeans = KMeans(n_clusters=k, random_state=42)```, cria uma instância KMeans com o número de clusters definido e semente aleatória. Na última linha, o modelo é treinado e atribuido cada ponto de dado ao seu respectivo centróide, resultando na lista já comentada.

### Plotagem do gráfico com os agrupamentos
Para plotar o gráfico, é executado o código abaixo:

```python
plt.scatter(X_padronizado[:, 0], X_padronizado[:, 1], c=labels, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=200, c='red', marker='X', label='Centroides')

plt.xlabel("Sepal length")
plt.ylabel("Sepal width")
plt.title("Clusters com K-Means")
plt.legend()
plt.show()
```
A primeira linha define outro gráfico de dispersão, da mesma forma que os anteriores. A diferença está nos dois últimos argumentos, que significam, em ordem, separar os pontos por cores de acordo com a lista **labels**, definida anteriormente, e o espectro de cores.
Já a linha ```python plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X', label='Centroides')``` pega as coordenadas X e Y de todos os centróides e marca com X, de tamanho 200px, na cor vermelha, além de adicionar uma legenda.
As linhas posteriores formatam e plotam o gráfico, conforme já visto. O gráfico gerado pode ser visto abaixo.

<div align="center">
<img src= "..\img\dataset1_with_clusters.png" alt="Plotagem do KMeans com k = 2" width="400">
</div>

### Métricas de validação
Por fim, é possível verificar o quão bem o algoritmo foi executado. Para isso, podemos utilizar as métricas de validação internas e externas, apresentadas em [2.Conceitos fundamentais](https://github.com/mevianna/ISA/tree/k_means/k_means/content/2.fundamental_concepts.md). Assim, é possível utilizar: Indice da Silhueta, Indice Davies-Bounce, Indice Clh e Indice de Rand Ajustado, as quais já são próprias da biblioteca do sklearn, diferentemente do Indice de Dunn (que por motivos de praicidade, não é executado aqui). Portanto, o código se reduz a:

```python
slht_score = silhouette_score(X_padronizado, labels)
clh_score = calinski_harabasz_score(X_padronizado, labels)
db_score = davies_bouldin_score(X_padronizado, labels)
ari_score = adjusted_rand_score(df['species'], labels)
```

>[!NOTE]
> No caso desse dataset, é possível utilizar métrica de validação externa, já que possuem classificações reais.

Em que, para as 3 primeiras linhas, é executada cada métrica de validação com os dados da matriz X_padronizado e os rótulos de labels. Na última linha, é comparada as classificações reais (coluna 'species' do dataset) e os resultados obtidos. O resultado é visualizado pela tabela executada com:

```python
df_resultados = pd.DataFrame({
    'Métrica': ['Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin', 'Ari'],
    'Valor': [slht_score, clh_score, db_score, ari_score]
})

print(df_resultados)
```

A qual gera a tabela:

| Métrica           | Valor      |
| :---------------- | :--------- |
| Silhouette        | 0.478724   |
| Calinski-Harabasz | 156.143038 |
| Davies-Bouldin    | 0.786801   |
| Ari               | 0.428951   |

Assim, é possível perceber que o índice silhueta está abaixo de 0.5, o que indica que o agrupamento não está tão adequado. O resultado é semelhante ao obtido com os indices DB e ARI, os quais, em ordem, demonstram alto valor (o que não é indicado, pois quão mais próximo de 0, melhor), e valor baixo, que mostra que o agrupamento não está tão semelhante ao real. Apesar do indice CH obter um valor maior, ele ainda é relativamente baixo. Assim, este agrupamento não é ideal.

> Dentre os motivos que esse algoritmo não esteja tão bem agrupado é o uso de apenas duas dimensões, má escolha das dimensões e outras possibilidades.

### Execução com k = 2
Caso utilizamos k = 2, o agrupamento se torna:

<div align="center">
<img src= "..\img\dataset1_with_2_clusters.png" alt="Plotagem do KMeans com k = 2" width="400">
</div>

E as métricas de validação:
| Métrica           | Valor      |
| :---------------- | :--------- |
| Silhouette        | 0.580184   |
| Calinski-Harabasz | 248.903428 |
| Davies-Bouldin    | 0.597555   |
| Ari               | 0.568116   |

O que indica que para essa escolha de dimensões, é melhor o uso de dois clusters.

> [!IMPORTANT]
> Para outras escolhas de pares de atributos para as duas dimensões, as métricas de validação obtém os mesmos resultados. Ou seja, independentemente do par de atributos, com k = 2, os indices obtém os mesmos resultados - o mesmo é observado para k = 3. Teste você mesmo!


### Pior valor de k
Apenas por curiosidade, o pior valor de k, que pode ser visto como o mais baixo no gráfico da silhueta, é k = 7. Assim, os indices se tornam:


| Métrica           | Valor      |
| :---------------- | :--------- |
| Silhouette        | 0.268562   |
| Calinski-Harabasz | 152.748286 |
| Davies-Bouldin    | 1.121924   |
| Ari               | 0.388810   |

Os quais são valores péssimos, independetemente da métrica.