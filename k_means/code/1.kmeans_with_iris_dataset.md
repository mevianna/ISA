# English Version

In this section, the execution of the K-Means Clustering algorithm using the *Python* programming language will be presented and demonstrated. The implementation that will be explored follows the same structure described in: [6. How the algorithm is executed?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/6.how_the_algorithm_is_executed.md). In the execution covered in this file, an existing Dataset was used.

---

## Dataset Details

To run the K-Means algorithm, the "Iris" dataset was used, available at: [Iris Dataset - UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/53/iris). This dataset is widely used in Machine Learning studies due to its simplicity and structure. It contains 150 rows of data, i.e., 150 flower samples from the Iris species, divided into 3 classes, each representing a different Iris flower type.

The dataset has five columns: sepal_length, sepal_width, petal_length, petal_width, and species. The first four represent morphological characteristics of the flowers, while the last indicates the flower class, i.e., the true label (target). It is worth noting that this dataset has no missing values.

> \[!NOTE]
> Since the "species" column represents the true labels, it is not directly used in K-Means, as this is an unsupervised algorithm. This means that K-Means clusters the data without prior knowledge of the actual classifications.

---

## Code:

### Importing Libraries and Functions:

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score
```

In the code snippet above, libraries and functions are imported that will be used throughout the code, namely:

* ```import pandas as pd```: Imports the "pandas" library, used for data manipulation and analysis, named "pd";
* ```import numpy as np```: Imports the "numpy" library, used for mathematical and matrix operations, named "np";
* ```import matplotlib.pyplot as plt```: Imports the "pyplot" module from the "matplotlib" library, used for graphical visualization, named "plt";
* ```from sklearn.cluster import KMeans```: Imports the "KMeans" class from the "scikit-learn" clustering module, which implements the K-Means clustering algorithm.
* ```from sklearn.preprocessing import StandardScaler```: Imports the "StandardScaler" class from the "scikit-learn" clustering module, which performs data normalization using the K-Means clustering algorithm.
* ```from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score```: Imports four validation metrics from the "scikit-learn" clustering module.

### Loading the Dataset:

```python
url = "https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv"
df = pd.read_csv(url)
```

In the code snippet above, the Dataset is loaded from an internet link and stored in the variable **df**, which will be used throughout the code.

> \[!TIP]
> To view basic characteristics of the Dataset, such as column names, number of rows and check for null values, you can run:
> ```python
> print(df.head())
> print(df.shape)
> df.isnull().values.any()
> ```

### Data Standardization:
Standardization (or normalization) is a process of transforming data so that all attributes are on the same scale, without distorting the relationships between them. This is necessary because attributes can have different scales; for example, while age can range from 0 to 100, salary can be in the thousands. In this case, without normalization, high salary values ​​compared to age can distort the actual results - this is a typical problem of algorithms that depend on distance.

To solve this, a function is used that standardizes the data, transforming it so that it has a mean of 0 and a standard deviation of 1, making all attributes equally relevant to the calculations. In scikit-learn, this task is done with the StandardScaler class, as seen below:

```python
X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

In the first line, only the feature rows of the dataset are separated, that is, the attributes that will be used in the grouping. In the case of the Iris dataset, all of these columns are numeric, so they do not require any type of conversion. Thus, the variable "X" is a matrix containing the first 4 columns of the dataset and all of its rows.

In the second line, an object of the type ```StandardScaler```, from the *scikit-learn* library, is created. It is used to standardize the data, so that each attribute has a mean of 0 and a standard deviation of 1.

Finally, in the third line, the standardization is applied to X, resulting in the new matrix "X_standardized". This matrix contains the same data, but normalized.

### Plotting Data without Clustering:
In the data set, there are 4 attribute columns, i.e. 4 dimensions. Since it is not possible to visualize data directly in more than 3 dimensions, we decided to simplify and represent data in only two dimensions.

In this example, the attributes "sepal_length" and "sepal_width" were chosen, but any other pair of attributes could be used for this visualization. Below is the code snippet that plots the graph.

```python
plt.figure(figsize=(6, 5))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c='hotpink')
plt.xlabel("Sepal length")
plt.ylabel("Sepal width")
plt.title("Iris Data without Clustering")
plt.show()
```
The first line, ```python plt.figure(figsize=(6, 5))```, creates the figure that will be the graph, setting its size to 6x5 inches. Next, ```python plt.scatter(X_standardized[:, 0], X_standardized[:, 1], c='hotpink')```, generates a scatterplot, which plots points on a 2D figure. In this function,
* The first argument, ```python X_standardized[:, 0]```, provides the data for the X-axis. It selects all the rows in column 0 (the first column) of the X_standardized matrix.
* The second argument, ```python X_standardized[:, 1]```, does the same for the Y-axis, selecting all the rows in the second column (the second column) of the X_standardized matrix.
* The last argument, ```python c='hotpink'```, sets the color of the points to "hotpink".

The next 2 lines set the labels for the graph, in order, the X-axis label and the Y-axis label. The next line sets the title of the graph.

Finally, the last line, ```python plt.show()``` displays the graph that has been formed, which can be seen below.

<div align="center">
<img src= "..\img\dataset1_without_clusters.png" alt="Plotting data without clustering" width="400">
</div>

### Choosing the Number of Clusters
As previously discussed in [4. How to choose the number of clusters?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/3.how_to_choose_the_number_of_clusters.md), there are several methods for determining the number of clusters. In this example, the elbow and silhouette methods will be used.

#### Elbow method
One of the most commonly used methods for determining the number of clusters is through the elbow method. To implement this method, the following code is executed:

```python
inertias = []
k_values = range(1, 11)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

plt.plot(k_values, inertias, 'mo-')
plt.xlabel("Number of clusters (K)")
plt.ylabel("Inertia")
plt.title("Elbow Method")
plt.show()
```

In the first line, ```python inertias = []```, an empty list is created that will store the inertia values. The next line, ```python k_values ​​= range(1, 11)```, generates a sequence from 1 to 10, which are the k values ​​to be tested.

The third line is a *for* loop, which iterates through the values ​​of the generated sequence with the variable k. The next line, ```python kmeans = KMeans(n_clusters=k, random_state=42)```, creates a KMeans algorithm object, with *k* clusters and a random seed.

The next line, ```python kmeans.fit(X_standardized)```, trains the created KMeans object with the data from the X_standardized matrix. The next line, ```python inertias.append(kmeans.inertia_)```, adds the inertia of the current KMeans run to the list, which is an attribute of the KMeans object.

After all the tests, the line ```python plt.plot(k_values, inertias, 'mo-')``` creates the line graph for the elbow method. The first argument represents the value of the X axis; the second, the value of the Y axis; and the third, a formatting, where:
* "m": sets the color of the line to magenta;
* "o": adds circle-shaped markers to each data point in the graph;
* "-": draws a line between the points in the graph.

The following lines simply label the graph and display it. It can be seen below:

<div align="center">
<img src= "..\img\elbow_method_dataset1.png" alt="Elbow Method" width="400">
</div>

Analyzing the graph, it is possible to see that the "bend" of the elbow is clearer at **k = 3**. Thus, the best value of k, according to this method, is 3.

#### Silhouette Method:
Another way to determine the best value of k is through the silhouette method. To do this, we execute the following code:

```python
silhouette = []
k_values = range(2, 11)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    silhouette.append(score)

plt.plot(k_values, silhouette, 'mo-')
plt.xlabel("Number of clusters (k)")
plt.ylabel("Average Silhouette")
plt.title("Silhouette Method")
plt.grid(True)
plt.show()
```

Since the code is quite similar to that of the elbow method, only the main differences will be commented on.

In the line ```python values_k = range(2, 11)```, a sequence from 2 to 10 is created, since the silhouette method is not determined for the value equal to 1.

In ```python labels = kmeans.fit_predict(X_standardized)```, in addition to the data training, this line assigns each point to its closest centroid, returning a list (labels). In this list, the position is a data point from the dataset, and the value of this position is the number of the cluster to which the data point was assigned.

Next, ```python score = silhouette_score(X_standardized, labels)```, calculates the average score of the silhouette for each value of k. In the next line, this value is added to the list of silhouettes.

Finally, the graph is plotted in the same way as the previous ones. The only difference is the line ```python plt.grid(True)```, which adds grids to the background for better visualization. The resulting graph can be seen below.

<div align="center">
<img src= "..\img\sillhoutte_method_dataset1.png" alt="Silhouette Method" width="400">
</div>

Based on the graph, it can be seen that the highest silhouette value occurs when **k = 2**. Therefore, the best number of clusters, according to the silhouette method, is 2.

> [!WARNING]
> It is important to note that the two methods (elbow and silhouette) obtained different results for the ideal value of **k**. This is expected, since each method uses different criteria to determine the best value of k. Therefore, it is interesting to run the algorithm with both values ​​and analyze visually or with the validation metrics which k obtained the best performance.

### Defining the Number of Clusters (k = 3):
Initially, **k = 3** will be used, as suggested by the elbow method. The definition and application of this value in K-Means is done below:

```python
k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
labels = kmeans.fit_predict(X_scaled)
```

In the first line, the number of clusters is set to three. Then, ```python kmeans = KMeans(n_clusters=k, random_state=42)```, creates a KMeans instance with the defined number of clusters and random seed. In the last line, the model is trained and assigns each data point to its respective centroid, resulting in the list of labels already discussed.

> [!IMPORTANT]
> In the sk-learn library, the default for choosing centroids is K-Means++. Therefore, the use of ```random_state=42``` is important to ensure the reproducibility of the code, that is, there will always be the same final result for the same random seed value.

### Plotting Clusters:
Next, a scatter plot is generated to visualize the clusters formed by K-Means, with k = 3.

```python
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='spring')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=200, c='red', marker='X', label='Centroids')
plt.xlabel("Sepal length")
plt.ylabel("Sepal width")
plt.title("Clusters with K-Means")
plt.legend()
plt.show()
```
The first line creates the scatter plot, in the same way as the previous ones. However, the last two arguments are different:
* ```c = labels```: colors the points according to the list of labels, that is, each cluster receives a different color;
* ```cmp = 'spring'```: defines the color spectrum used.

The line ```python plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X', label='Centroids')``` plots the centroids found. The first two arguments obtain the X and Y coordinates of the centroids; the next three arguments mark these centroids with an X, 200px in size, in red. In addition, a legend is added to the graph to identify the centroids.

The subsequent lines format and plot the graph, as discussed previously. The generated graph can be seen below.

<div align="center">
<img src= "..\img\dataset1_with_3_clusters.png" alt="KMeans with k=3" width="400">
</div>

### Validation Metrics:
Finally, it is possible to evaluate the quality of the clustering generated by K-Means, through internal and external validation metrics, presented in [2. Fundamental Concepts](https://github.com/mevianna/ISA/tree/k_means/k_means/content/2.fundamental_concepts.md). In this example, the following metrics were used:
* Silhouette Index;
* Calinski-Harabasz Index;
* Davies-Bouldin Index (DB);
* Adjusted Rand Index (ARI).

>[!NOTE]
> In this dataset, it is possible to use external validation metrics, since there are true labels available in the 'species' column.

These metrics are specific to the sklearn library, unlike the Dunn Index, which requires manual implementation. For this reason, it is not used in this example.

The code for calculating the metrics is:

```python
slht_score = silhouette_score(X_scaled, labels)
clh_score = calinski_harabasz_score(X_scaled, labels)
db_score = davies_bouldin_score(X_scaled, labels)
ari_score = adjusted_rand_score(df['species'], labels)
```

The first 3 lines calculate internal metrics, based only on the data and the generated labels. The last line calculates the ARI, which is an external validation metric, thus comparing the real classifications ('species' column of the dataset) and the obtained labels.

The result of these metrics can be viewed in table form with the following code:

```python
df_results = pd.DataFrame({
    'Metric': ['Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin', 'Ari'],
    'Value': [slht_score, clh_score, db_score, ari_score]
})

print(df_results.round(3))
```

Which generates the following table with 3 decimal places approximation:

| Metric            | Value   |
| ----------------- | ------- |
| Silhouette        | 0.479   |
| Calinski-Harabasz | 156.143 |
| Davies-Bouldin    | 0.787   |
| Ari               | 0.429   |

Based on the results of the metrics, it is possible to observe that the silhouette index is below 0.5, which indicates that the data is not well grouped. Similar results are observed in the other metrics:
* The Davies-Bouldin (DB) index presents a high value, which also suggests a poor separation between the clusters, since values ​​closer to 0 indicate better performance;
* The Adjusted Rand Index (ARI) demonstrates a low value, which means that there is little similarity between the obtained clusters and the real labels of the dataset.
* The Calinski-Harabasz (CH) index, although presenting a relatively higher value, is still considered low, since it can present values ​​from 0 to infinity, which reinforces the idea that the clustering was not ideal.

Therefore, the results indicate that the clustering generated with k = 3 is not satisfactory.

> [!WARNING]
> Several factors may justify this result, such as:
> - Data that is difficult to group due to the number of dimensions, which may overlap in space;
> - Limitations of K-Means, such as sensitivity to outliers and the assumption of convex and symmetric clusters;

### Execution with k = 2:
If k = 2 is used, the grouping becomes:

<div align="center">
<img src= "..\img\dataset1_with_2_clusters.png" alt="KMeans with k=2" width="400">
</div>

With the following validation metrics:

| Metric            | Value   |
| ----------------- | ------- |
| Silhouette        | 0.580   |
| Calinski-Harabasz | 248.903 |
| Davies-Bouldin    | 0.597   |
| Ari               | 0.568   |

These values ​​indicate that the clustering was better performed. So, for this choice of dimensions, k = 2 obtains the best clustering quality.

### Worst k (e.g., k = 7):
As a curiosity, the worst value of k, which can be seen as the lowest point in the silhouette graph, is k = 7. The validation metrics obtained in this case are:

| Metric            | Value   |
| ----------------- | ------- |
| Silhouette        | 0.269   |
| Calinski-Harabasz | 152.748 |
| Davies-Bouldin    | 1.122   |
| Ari               | 0.389   |

These are very bad values, regardless of the metric, indicating that it is a bad cluster. This reinforces the importance of choosing a good k, since inadequate values ​​can harm the quality of the clusters.

---

## References

**SCIKIT-LEARN**. sklearn.cluster.KMeans. Available at: [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)

**SCIKIT-LEARN**. Demonstration of k-means assumptions. Available at: [https://scikit-learn.org/stable/auto\_examples/cluster/plot\_kmeans\_assumptions.html](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html)

***

# Portuguese Version
Nessa seção, será apresentada e demonstrada a execução do algoritmo K-Means Clustering utilizando a linguagem de programação *Python*. A implementação que será explorada segue a mesma estrutura descrita em: [6.Como o algoritmo é executado?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/6.how_the_algorithm_is_executed.md). Na execução abordada nesse arquivo, foi utilizado um Dataset já existente.

***

## Detalhes do Dataset
Para a execução do algoritmo K-Means, foi utilizado o dataset "Iris", disponível em: [Iris Dataset - UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/53/iris). Esse conjunto de dados é um dos mais utilizados para estudos de Machine Learning, dada a sua simplicidade e estrutura. Ele contém 150 linhas de dados, isto é, 150 amostras de flores da espécie Iris, divididas em 3 classes, que representam tipos de flores Iris diferentes.
O dataset possui cinco colunas: sepal_length, sepal_width, petal_length, petal_width e species. As quatro primeiras representam caracteristícas morfológicas das flores, enquanto a última indica a classe da flor, ou seja, o rótulo verdadeiro (target). Cabe ressaltar que esse dataset não possui valores ausentes.

>[!NOTE]
> Como o campo "species" representa o rótulo verdadeiro das amostras, ele não é utilizado diretamente no K-Means, pois se trata de um algoritmo não-supervisionado. Isso significa que o K-Means agrupa os dados sem conhecimento prévios das reais classificações.

***

## Código:

### Importação de bibliotecas e funções:

```python 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score
```

No trecho de código acima, são importadas bibliotecas e funções que serão utilizadas ao longo do código, sendo elas:

* ```import pandas as pd```: Importa a biblioteca "pandas", utilizada para manipulação e análise de dados, nomeada como "pd";
* ```import numpy as np```: Importa a biblioteca "numpy", utilizada para operações matemáticas e matriciais, nomeada como "np";
* ```import matplotlib.pyplot as plt```: Importa o módulo "pyplot" da biblioteca "matplotlib", utilizada para visualização gráfica, nomeada como "plt";
* ```from sklearn.cluster import KMeans```: Importa a classe "KMeans" do módulo de agrupamento do "scikit-learn", que implementa o algoritmo de clusterização K-Means.
* ```from sklearn.preprocessing import StandardScaler```: Importa a classe "StandardScaler" do módulo de agrupamento do "scikit-learn", que realiza a normalização dos dados o algoritmo de clusterização K-Means.
* ```from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score```: Importa quatro métricas de validação do módulo de agrupamento do "scikit-learn".

### Importação do Dataset
```python
url = "https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv"
df = pd.read_csv(url)
```

No trecho de código acima, o Dataset é carregado a partir de um link da internet e armazenado na variável **df**, que será utilizado ao longo de todo código.

> [!TIP]
> Para visualizar caracteristícas básicas do Dataset, como os nomes das colunas, a quantidade de linhas e verificar se há valores nulos, pode-se executar: 
> ```python 
> print(df.head())
> print(df.shape)
> df.isnull().values.any()
> ```

### Padronização dos dados
A padronização (ou normalização) é um processo de transformação dos dados para que todos os atributos estejam na mesma escala, sem distorcer as relações entre eles. Isso é necessário pois os atributos podem possuir escalas diferente; por exemplo, enquanto a idade pode variar entre 0 e 100, o salário pode estar na casa dos milhares. Nesse caso, sem a normalização, os altos valores de salário em comparação com a idade podem distorcer os resultados reais - este é um problema tipíco dos algoritmos que dependem de distância.

Para resolver isso, utiliza-se uma função que padroniza os dados, transformando-os de forma que tenham média 0 e desvio padrão 1, tornando todos os atributos igualmente relevantes para os cálculos. No scikit-learn, essa tarefa é feita com a classe StandardScaler, conforme visto abaixo:

```python 
X = df[['sepal_length',  'sepal_width',  'petal_length',  'petal_width']]
scaler = StandardScaler()
X_padronizado = scaler.fit_transform(X)
```

Na primeira linha, são separadas apenas as linhas de caracteristicas (feature) do dataset, ou seja, os atributos que serão utilizados no agrupamento. No caso do dataset Iris, todas essas colunas são numéricas, não precisando, então, de nenhum tipo de conversão. Dessa forma, a variável "X" é uma matriz contendo as 4 primeiras colunas do dataset e todas as suas linhas.

Na segunda linha, é criado um objeto do tipo ```StandardScaler```, da biblioteca *scikit-learn*. Ele é utilizado para padronizar os dados, de forma que cada atributo tenha média 0 e desvio padrão 1. 

Por fim, na terceira linha, a padronização é aplicada sobre X, resultando a nova matriz "X_padronizado". Essa matriz contém os mesmos dados, porém, normalizados.

### Plotagem dos dados sem agrupamentos
No conjunto de dados, há 4 colunas de atributos, isto é, 4 dimensões. Como não é possível visualizar dados diretamente em mais de 3 dimensões, optou-se por simplificar e representar dados em apenas duas dimensões.

Nesse exemplo, foram escolhidos os atributos "sepal_length" e "sepal_width", mas qualquer outro par de atributos poderia ser utilizado para essa visualização. A seguir, há o trecho de código que realiza a plotagem do gráfico.

```python
plt.figure(figsize=(6, 5))
plt.scatter(X_padronizado[:, 0], X_padronizado[:, 1], c='hotpink')
plt.xlabel("Sepal length")
plt.ylabel("Sepal width")
plt.title("Dados Iris sem agrupamento")
plt.show()
``` 

A primeira linha, ```python plt.figure(figsize=(6, 5))```, cria a figura que será o gráfico, definindo o seu tamanho como 6x5 polegadas. Em seguida, ```python plt.scatter(X_padronizado[:, 0], X_padronizado[:, 1], c='hotpink')```, gera um gráfico de dispersão, o qual plota pontos numa figura 2D. Nessa função,
* O primeiro argumento, ```python X_padronizado[:, 0]```, fornece os dados do eixo X. Ele seleciona todas as linhas da coluna 0 (primeira coluna) da matriz X_padronizada.
* O segundo argumento, ```python X_padronizado[:, 1]```, faz o mesmo para o eixo Y, escolhendo todas as linhas da segunda coluna (coluna 1) da matriz X_padronizada. 
* O último argumento, ```python c='hotpink'```, determina a cor dos pontos como "hotpink".

As 2 linhas seguintes determinam rótulos do gráfico, sendo, em ordem, o rótulo do eixo X e o rótulo do eixo Y. A linha posterior determina o título do grãfico. 

Por fim, a última linha, ```python plt.show()``` exibe o gráfico formado, o qual pode ser visto abaixo.

<div align="center">
<img src= "..\img\dataset1_without_clusters.png" alt="Plotagem dos dados sem agrupamento" width="400">
</div>

### Como definir o número de clusters
Como discutido anteriormente em [4.Como escolher o número de clusters?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/3.how_to_choose_the_number_of_clusters.md), existem diversos métodos para determinar o número de clusters. Neste exemplo, serão utilizados os métodos do cotovelo e da silhueta.

#### Método do cotovelo
Uma das formas mais utilizadas para determinar o número de clusters, é através do método do cotovelo. Para implementar esse método, é executado o seguinte código:

```python
inercias = []
valores_k = range(1, 11)

for k in valores_k:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_padronizado)
    inercias.append(kmeans.inertia_)

plt.plot(valores_k, inercias, 'mo-')
plt.xlabel("Número de clusters (K)")
plt.ylabel("Inércia")
plt.title("Método do Cotovelo")
plt.show()
```

Na primeira linha, ```python inercias = []```, é criada uma lista vazia que irá armazenar os valores da inércia. A linha seguinte, ```python valores_k = range(1, 11)```, gera uma sequência de 1 a 10, que são os valores de k a serem testados.

A terceira linha é um loop *for*, o qual percorre os valores da sequência gerada com a variável k. A linha posterior, ```python kmeans = KMeans(n_clusters=k, random_state=42)```, cria um objeto do algoritmo KMeans, com *k* clusters e semente aleatória.

A linha seguinte, ```python kmeans.fit(X_padronizado)```, treina o objeto KMeans criado com os dados da matriz X_padronizada. A linha seguinte, ```python inercias.append(kmeans.inertia_)```, adiciona na lista a inércia da execução atual do KMeans, a qual é um atributo do objeto KMeans.

Após todos os testes, a linha ```python plt.plot(valores_k, inercias, 'mo-')``` cria o gráfico de linha para o método do cotovelo. O primeiro argumento representa o valor do eixo X; o segundo, o valor do eixo Y; e, o terceiro, uma formatação, em que:
* "m": define a cor da linha como magenta;
* "o": adiciona marcadores em formato de circulo a cada ponto de dados no gráfico;
* "-": traça uma linha entre os pontos do gráfico.

As linhas posteriores, apenas rotulam o gráfico e o exibem. Ele pode ser visualizado abaixo:

<div align="center">
<img src= "..\img\elbow_method_dataset1.png" alt="Plotagem do método do cotovelo" width="400">
</div>

Analisando o gráfico, é possível ver que a "dobra" do cotovelo está mais nitída em **k = 3**. Assim, o melhor valor de k, segundo este método, é 3.

#### Método da silhueta
Outra maneira de determinar o melhor valor de k, é através do método da silhueta. Para tal, executamos o seguinte código:

```python
silhueta = []
valores_k = range(2, 11)

for k in valores_k:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_padronizado)
    score = silhouette_score(X_padronizado, labels)
    silhueta.append(score)

plt.plot(valores_k, silhueta, 'mo-')
plt.xlabel("Número de clusters (k)")
plt.ylabel("Silhueta média")
plt.title("Método da Silhueta para escolha de k")
plt.grid(True)
plt.show()
```

Como o código é bastante semelhante ao do método do cotovelo, serão comentadas apenas as principais diferenças.

Na linha ```python valores_k = range(2, 11)```, é criada uma sequência de 2 a 10, uma vez que o método da silhueta não é determinado para o valor igual a 1.

Em ```python labels = kmeans.fit_predict(X_padronizado)```, além do treino de dados, essa linha atribui cada ponto ao seu centróide mais próximo, retornando uma lista (labels). Nessa lista, a posição é um ponto de dado do dataset, e o valor desta posição é o número do cluster que o ponto de dado foi atribuído.

Em seguida, ```python score = silhouette_score(X_padronizado, labels)```, calcula a pontuação média da silhueta para cada valor de k. Na linha seguinte, esse valor é adicionado na lista de silhuetas.

Por fim, o gráfico é plotado da mesma forma que os anteriores. A única diferença é a linha ```python plt.grid(True)```, que adiciona grades ao fundo para melhor visualização. O gráfico formado pode ser visualizado abaixo.

<div align="center">
<img src= "..\img\sillhoutte_method_dataset1.png" alt="Plotagem do método da silhueta" width="400">
</div>

Com base no gráfico, observa-se que o maior valor de silhueta ocorre quando **k = 2**. Dessa forma, o melhor número de clusters, segundo o método da silhueta, é 2.

> [!WARNING]
> É importante notar que os dois métodos (cotovelo e silhueta) obtiveram resultados diferentes para o valor ideal de **k**. Isso é esperado, uma vez que cada método um utiliza critérios distintos para determinar o melhor valor de k. Por isso, é interessante executar o algoritmo com ambos os valores e analisar visualmente ou com as métricas de validação qual k obteve o melhor desempenho.

### Definição do número de clusters
Inicialmente, será utilizado **k = 3**, conforme sugerido pelo método do cotovelo. A definição e aplicação desse valor no K-Means é feita abaixo:

```python
k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
labels = kmeans.fit_predict(X_padronizado)
```

Na primeira linha, é definido o número de clusters igual a três. Em seguida, ```python kmeans = KMeans(n_clusters=k, random_state=42)```, cria uma instância KMeans com o número de clusters definido e semente aleatória. Na última linha, o modelo é treinado e atribui cada ponto de dado ao seu respectivo centróide, resultando na lista de rótulos já abordada.

> [!IMPORTANT]
> Na biblioteca sk-learn, o padrão para escolha dos centróides é o K-Means++. Assim, o uso de ```random_state=42``` é importante para garantir a reprodutibilidade do código, ou seja, sempre haverá o mesmo resultado final para o mesmo valor de semente aleatória.

### Plotagem do gráfico com os agrupamentos
A seguir, é gerado um gráfico de dispersão para visualização dos agrupamentos formados pelo K-Means, com k = 3.

```python
plt.scatter(X_padronizado[:, 0], X_padronizado[:, 1], c = labels, cmap = 'spring')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=200, c='red', marker='X', label='Centroides')

plt.xlabel("Sepal length")
plt.ylabel("Sepal width")
plt.title("Clusters com K-Means")
plt.legend()
plt.show()
```
A primeira linha cria o gráfico de dispersão, da mesma forma que os anteriores. No entanto, os últimos dois argumentos são diferentes:
* ```c = labels```: colore os pontos de acordo com a lista de rótulos, ou seja, cada agrupamento recebe uma cor diferente;
* ```cmp = 'spring'```: define o espectro de cores utilizado.

Já a linha ```python plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X', label='Centroides')``` plota os centróides encontrados. Os dois primeiros argumentos obtém as coordenadas X e Y dos centróides; os três argumentos seguintes marcam com um X, de tamanho 200px, na cor vermelha, tais centróides. Além disso, é adicionado uma legenda no gráfico pra identificar os centróides.

As linhas posteriores formatam e plotam o gráfico, conforme abordado anteriormente. O gráfico gerado pode ser visto abaixo.

<div align="center">
<img src= "..\img\dataset1_with_3_clusters.png" alt="Plotagem do KMeans com k = 3" width="400">
</div>

### Métricas de validação
Por fim, é possível avaliar a qualidade do agrupamento gerado pelo K-Means, através das métricas de validação internas e externas, apresentadas em [2.Conceitos fundamentais](https://github.com/mevianna/ISA/tree/k_means/k_means/content/2.fundamental_concepts.md). Nesse exemplo, utilizou-se as métricas:
* Índice da Silhueta;
* Índice de Calinski-Harabasz;
* Índice de Davies-Bouldin (DB); 
* Índice de Rand Ajustado (ARI).

>[!NOTE]
> Neste dataset, é possível utilizar métricas de validação externa, pois há rótulos verdadeiros disponíveis na coluna 'species'.

Essas métricas são próprias da biblioteca do sklearn, diferentemente do Indice de Dunn que exige implementação manual. Por esse motivo, ele não é utilizado nesse exemplo.
O código para cálculo das métricas é:

```python
slht_score = silhouette_score(X_padronizado, labels)
clh_score = calinski_harabasz_score(X_padronizado, labels)
db_score = davies_bouldin_score(X_padronizado, labels)
ari_score = adjusted_rand_score(df['species'], labels)
```

As 3 primeiras linhas calculam métricas internas, com base apenas nos dados e nos rótulos gerados. Já a última linha, calcula o ARI, que é uma métrica de validação externa, assim, compara-se as classificações reais (coluna 'species' do dataset) e os rótulos obtidos.

O resultado dessas métricas pode ser visualizado em forma de tabela com o seguinte código:

```python
df_resultados = pd.DataFrame({
    'Métrica': ['Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin', 'Ari'],
    'Valor': [slht_score, clh_score, db_score, ari_score]
})

print(df_resultados.round(3))
```

A qual gera a seguinte tabela com aproximação de 3 casas decimais:

| Métrica           | Valor   |
| :---------------- | :------ |
| Silhouette        | 0.479   |
| Calinski-Harabasz | 156.143 |
| Davies-Bouldin    | 0.787   |
| Ari               | 0.429   |

Com base nos resultados das métricas, é possível observar que o índice de silhueta está abaixo de 0.5, o que indica que os dados não estão bem agrupados. Resultados semelhantes são observados nas demais métricas:
* O índice de Davies-Bouldin (DB) apresenta um alto valor, o que também sugere uma má separação entre os clusters, uma vez que valores mais próximos de 0 indicam melhor desempenho;
* O Adjusted Rand Index (ARI) demonstra um valor baixo, o que significa que há pouca semelhança entre os agrupamentos obtidos e os rótulos reais do dataset.
* O índice de Calinski-Harabasz (CH) embora apresente um valor relativamente maior, ele ainda é considerado baixo, pois ele pode apresentar valores de 0 ao infinito, o que reforça a ideia de que o agrupamento não foi o ideal.

Portanto, os resultados indicam que o agrupamento gerado com k = 3 não é satisfatório.

> [!WARNING]
> Vários fatores podem justificar esse resultado, como:
> - Dados difíceis de agrupar devido ao número de dimensões, que podem se sobrepor no espaço;
> - Limitações do K-Means, como sensibilidade a outliers e a presunção de clusters convexos e simétricos;

### Execução com k = 2
Caso seja utilizado k = 2, o agrupamento se torna:

<div align="center">
<img src= "..\img\dataset1_with_2_clusters.png" alt="Plotagem do KMeans com k = 2" width="400">
</div>

Com as seguintes métricas de validação:

| Métrica           | Valor   |
| :---------------- | :------ |
| Silhouette        | 0.580   |
| Calinski-Harabasz | 248.903 |
| Davies-Bouldin    | 0.597   |
| Ari               | 0.568   |

Esses valores indicam que o agrupamento foi melhor realizado. Então, para essa escolha de dimensões, k = 2 obtém a melhor qualidade de agrupamento.

### Pior valor de k
A título de curiosidade, o pior valor de k, que pode ser visto como o ponto mais baixo no gráfico da silhueta, é k = 7. As métricas de validação obtidas nesse caso são:

| Métrica           | Valor      |
| :---------------- | :--------- |
| Silhouette        | 0.268562   |
| Calinski-Harabasz | 152.748286 |
| Davies-Bouldin    | 1.121924   |
| Ari               | 0.388810   |

Os quais são valores péssimos, independetemente da métrica, indicam que é um agrupamento ruim. Isso reforça a importância de uma boa escolha de k, já que valores inadequados podem prejudicar a qualidade dos agrupamentos.

***

### Referências
**SCIKIT-LEARN**. sklearn.cluster. KMeans. Disponível em: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html.

**SCIKIT-LEARN**. Demonstration of k-means assumptions. Disponível em: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py. 
