# English Version
In this section, the execution of the K-Means Clustering algorithm using the *Python* programming language will be presented and demonstrated. The implementation that will be explored follows the same structure described in: [6.How the algorithm is executed?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/6.how_the_algorithm_is_executed.md).

***

# Portuguese Version
Nessa seção, será apresentada e demonstrada a execução do algoritmo K-Means Clustering utilizando a linguagem de programação *Python*. A implementação que será explorada segue a mesma estrutura descrita em: [6.Como o algoritmo é executado?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/6.how_the_algorithm_is_executed.md). Na execução abordada nesse arquivo, foi utilizado um Dataset já existente.

***

## Detalhes do Dataset
Para a execução do algoritmo K-Means, foi utilizado o dataset "Iris", disponível em: [Iris Dataset - UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/53/iris). Esse conjunto de dados é um dos mais utilizados para estudos de Machine Learning, dada a sua simplicidade e estrutura. Ele contém 150 linhas de dados, isto é, 150 amostras de flores da espécie Iris, divididas em 3 classes, que representam tipos de flores Iris diferentes.
O dataset possui cinco colunas: sepal_length, sepal_width, petal_length, petal_width e species. As quatro primeiras representam caracteristícas morfológicas das flores, enquanto a última indica a classe da flor, ou seja, o rótulo verdadeiro (target). Cabe ressaltar que esse dataset não possui valores ausentes.

>[!NOTE]
> Como o campo "species" representa o rótulo verdadeiro das amostras, ele não é utilizado diretamente no K-Means, pois se trata de um algoritmo não-supervisionado. Isso significa que o K-Means agrupa os dados sem conhecimento prévios das reais classificações.

***

## Código:

### Importação de bibliotecas e funções:

```python 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
```

No trecho de código acima, são importadas bibliotecas e funções que serão utilizadas ao longo do código, sendo elas:

* ```import pandas as pd```: Importa a biblioteca "pandas", utilizada para manipulação e análise de dados, nomeada como "pd";
* ```import numpy as np```: Importa a biblioteca "numpy", utilizada para operações matemáticas e matriciais, nomeada como "np";
* ```import matplotlib.pyplot as plt```: Importa o módulo "pyplot" da biblioteca "matplotlib", utilizada para visualização gráfica, nomeada como "plt";
* ```from sklearn.cluster import KMeans```: Importa a classe "KMeans" do módulo de agrupamento do "scikit-learn", que implementa o algoritmo de clusterização K-Means.
* ```from sklearn.preprocessing import StandardScaler```: Importa a classe "StandardScaler" do módulo de agrupamento do "scikit-learn", que realiza a normalização dos dados o algoritmo de clusterização K-Means.
* ```from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score```: Importa três métricas de validação do módulo de agrupamento do "scikit-learn".

### Importação do Dataset
```python
url = "https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv"
df = pd.read_csv(url)
```

No trecho de código acima, o Dataset é carregado a partir de um link da internet e armazenado na variável **df**, que será utilizado ao longo de todo código.

> [!TIP]
> Para visualizar caracteristícas básicas do Dataset, como os nomes das colunas, a quantidade de linhas e verificar se há valores nulos, pode-se executar: 
> ```python 
> print(df.head())
> print(df.shape)
> df.isnull().values.any()
> ```

### Padronização dos dados
A padronização (ou normalização) é um processo de transformação dos dados para que todos os atributos estejam na mesma escala, sem distorcer as relações entre eles. Isso é necessário pois os atributos podem possuir escalas diferente; por exemplo, enquanto a idade pode variar entre 0 e 100, o salário pode estar na casa dos milhares. Nesse caso, sem a normalização, os altos valores de salário em comparação com a idade podem distorcer os resultados reais - este é um problema tipíco dos algoritmos que dependem de distância.

Para resolver isso, utiliza-se uma função que padroniza os dados, transformando-os de forma que tenham média 0 e desvio padrão 1, tornando todos os atributos igualmente relevantes para os cálculos. No scikit-learn, essa tarefa é feita com a classe StandardScaler, conforme visto abaixo:

```python 
X = df[['sepal_length',  'sepal_width',  'petal_length',  'petal_width']]
scaler = StandardScaler()
X_padronizado = scaler.fit_transform(X)
```

Na primeira linha, são separadas apenas as linhas de caracteristicas (feature) do dataset, ou seja, os atributos que serão utilizados no agrupamento. No caso do dataset Iris, todas essas colunas são numéricas, não precisando, então, de nenhum tipo de conversão. Dessa forma, a variável "X" é uma matriz contendo as 4 primeiras colunas do dataset e todas as suas linhas.

Na segunda linha, é criado um objeto do tipo ```StandardScaler```, da biblioteca *scikit-learn*. Ele é utilizado para padronizar os dados, de forma que cada atributo tenha média 0 e desvio padrão 1. 

Por fim, na terceira linha, a padronização é aplicada sobre X, resultando a nova matriz "X_padronizado". Essa matriz contém os mesmos dados, porém, normalizados.

### Plotagem dos dados sem agrupamentos
No conjunto de dados, há 4 colunas de atributos, isto é, 4 dimensões. Como não é possível visualizar dados diretamente em mais de 3 dimensões, optou-se por simplificar e representar dados em apenas duas dimensões.

Nesse exemplo, foram escolhidos os atributos "sepal_length" e "sepal_width", mas qualquer outro par de atributos poderia ser utilizado. A seguir, há o trecho de código que realiza a plotagem do gráfico.

```python
plt.figure(figsize=(6, 5))
plt.scatter(X_padronizado[:, 0], X_padronizado[:, 1], c='hotpink')
plt.xlabel("Sepal length")
plt.ylabel("Sepal width")
plt.title("Dados Iris sem agrupamento")
plt.show()
``` 

A primeira linha, ```python plt.figure(figsize=(6, 5))```, cria a figura que será o gráfico, definindo o seu tamanho como 6x5 polegadas. Em seguida, ```python plt.scatter(X_padronizado[:, 0], X_padronizado[:, 1], c='hotpink')```, gera um gráfico de dispersão, o qual plota pontos numa figura 2D. Nessa função,
* O primeiro argumento, ```python X_padronizado[:, 0]```, fornece os dados do eixo X. Ele seleciona todas as linhas da coluna 0 (primeira coluna) da matriz X_padronizada.
* O segundo argumento, ```python X_padronizado[:, 1]```, faz o mesmo para o eixo Y, escolhendo todas as linhas da segunda coluna (coluna 1) da matriz X_padronizada. 
* O último argumento, ```python c='hotpink'```, determina a cor dos pontos como "hotpink".

As 2 linhas seguintes determinam rõtulos do gráfico, sendo, em ordem, o rótulo do eixo X e o rótulo do eixo Y. A linha posterior determina o título do grãfico. 

Por fim, a última linha, ```python plt.show()``` exibe o gráfico formado, o qual pode ser visto abaixo.

<div align="center">
<img src= "..\img\dataset1_without_clusters.png" alt="Plotagem dos dados sem agrupamento" width="400">
</div>

### Como definir o número de clusters
Como discutido anteriormente em [4.Como escolher o número de clusters?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/3.how_to_choose_the_number_of_clusters.md), existem diversos métodos para determinar o número de clusters. Neste exemplo, serão utilizados os métodos do cotovelo e da silhueta.

### Método do cotovelo
Uma das formas mais utilizadas para determinar o número de clusters, é através do método do cotovelo. Para implementar esse método, é executado o seguinte código:

```python
inercias = []
valores_k = range(1, 11)

for k in valores_k:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_padronizado)
    inercias.append(kmeans.inertia_)

plt.plot(valores_k, inercias, 'mo-')
plt.xlabel("Número de clusters (K)")
plt.ylabel("Inércia")
plt.title("Método do Cotovelo")
plt.show()
```

Na primeira linha, ```python inercias = []```, é criada uma lista vazia que irá armazenar os valores da inércia. A linha seguinte, ```python valores_k = range(1, 11)```, gera uma sequência de 1 a 10, que são os valores de k a serem testados.

A terceira linha é um loop *for*, o qual percorre os valores da sequência gerada com a variável k. A linha posterior, ```python kmeans = KMeans(n_clusters=k, random_state=42)```, cria um objeto do algoritmo KMeans, com *k* clusters e semente aleatória.

A linha seguinte, ```python kmeans.fit(X_padronizado)```, treina o objeto KMeans criado com os dados da matriz X_padronizada. A linha seguinte, ```python inercias.append(kmeans.inertia_)```, adiciona na lista a inércia da execução atual do KMeans, a qual é um atributo do objeto KMeans.

Após todos os testes, a linha ```python plt.plot(valores_k, inercias, 'mo-')``` cria o gráfico de linha para o método do cotovelo. O primeiro argumento representa o valor do eixo X; o segundo, o valor do eixo Y; e, o terceiro, uma formatação, em que:
* "m": define a cor da linha como magenta;
* "o": adiciona marcadores em formato de circulo a cada ponto de dados no gráfico;
* "-": traça uma linha entre os pontos do gráfico.

As linhas posteriores, apenas rotulam o gráfico e o exibem. Ele pode ser visualizado abaixo:

<div align="center">
<img src= "..\img\elbow_method_dataset1.png" alt="Plotagem do método do cotovelo" width="400">
</div>

Analisando o gráfico, é possível ver que a "dobra" do cotovelo está mais nitída em **k = 3**. Assim, o melhor valor de k, segundo este método, é 3.

### Método da silhueta
Outra maneira de determinar o melhor valor de k, é através do método da silhueta. Para tal, executamos o seguinte código:

```python
silhueta = []
valores_k = range(2, 11)

for k in valores_k:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_padronizado)
    score = silhouette_score(X_padronizado, labels)
    silhueta.append(score)

plt.plot(valores_k, silhueta, 'mo-')
plt.xlabel("Número de clusters (k)")
plt.ylabel("Silhueta média")
plt.title("Método da Silhueta para escolha de k")
plt.grid(True)
plt.show()
```

Como o código é bastante semelhante ao do método do cotovelo, serão comentadas apenas as principais diferenças.

Na linha ```python valores_k = range(2, 11)```, é criada uma sequência de 2 a 10, uma vez que o método da silhueta não é determinado para o valor igual a 1.

Em ```python labels = kmeans.fit_predict(X_padronizado)```, além do treino de dados, essa linha atribui cada ponto ao seu centróide mais próximo, retornando uma lista (labels). Nessa lista, a posição é um ponto de dado do dataset, e o valor desta posição é o número do cluster que o ponto de dado foi atribuído.

Em seguida, ```python score = silhouette_score(X_padronizado, labels)```, calcula a pontuação média da silhueta para cada valor de k. Na linha seguinte, esse valor é adicionado na lista de silhuetas.

Por fim, o gráfico é plotado da mesma forma que os anteriores. A única diferença é a linha ```python plt.grid(True)```, que adiciona grades ao fundo para melhor visualização. O gráfico formado pode ser visualizado abaixo.

<div align="center">
<img src= "..\img\sillhoutte_method_dataset1.png" alt="Plotagem do método da silhueta" width="400">
</div>

Com base no gráfico, observa-se que o maior valor de silhueta ocorre quando **k = 2**. Dessa forma, o melhor número de clusters, segundo o método da silhueta, é 2.

> [!WARNING]
> É importante notar que os dois métodos (cotovelo e silhueta) obtiveram resultados diferentes para o valor ideal de **k**. Isso é esperado, uma vez que cada método um utiliza critérios distintos para determinar o melhor valor de k. Por isso, é interessante executar o algoritmo com ambos os valores e analisar visualmente ou com as métricas de validação qual k obteve o melhor desempenho.

### Definição do número de clusters
Inicialmente, será utilizado **k = 3**, conforme sugerido pelo método do cotovelo. A definição e aplicação desse valor no K-Means é feita abaixo:

```python
k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
labels = kmeans.fit_predict(X_padronizado)
```

Na primeira linha, é definido o número de clusters igual a três. Em seguida, ```python kmeans = KMeans(n_clusters=k, random_state=42)```, cria uma instância KMeans com o número de clusters definido e semente aleatória. Na última linha, o modelo é treinado e atribui cada ponto de dado ao seu respectivo centróide, resultando na lista de rótulos já abordada.

> [!IMPORTANT]
> Na biblioteca sk-learn, o padrão para escolha dos centróides é o K-Means++. Assim, o uso de ```random_state=42``` é importante para garantir a reprodutibilidade do código, ou seja, sempre haverá o mesmo resultado final para o mesmo valor de semente aleatória.

### Plotagem do gráfico com os agrupamentos
A seguir, é gerado um gráfico de dispersão para visualização dos agrupamentos formados pelo K-Means, com k = 3.

```python
plt.scatter(X_padronizado[:, 0], X_padronizado[:, 1], c = labels, cmap = 'spring')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=200, c='red', marker='X', label='Centroides')

plt.xlabel("Sepal length")
plt.ylabel("Sepal width")
plt.title("Clusters com K-Means")
plt.legend()
plt.show()
```
A primeira linha cria o gráfico de dispersão, da mesma forma que os anteriores. No entanto, os últimos dois argumentos são diferentes:
* ```c = labels```: colore os pontos de acordo com a lista de rótulos, ou seja, cada agrupamento recebe uma cor diferente;
* ```cmp = 'spring'```: define o espectro de cores utilizado.

Já a linha ```python plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X', label='Centroides')``` plota os centróides encontrados. Os dois primeiros argumentos obtém as coordenadas X e Y dos centróides; os três argumentos seguintes marcam com um X, de tamanho 200px, na cor vermelha, tais centróides. Além disso, é adicionado uma legenda no gráfico pra identificar os centróides.

As linhas posteriores formatam e plotam o gráfico, conforme abordado anteriormente. O gráfico gerado pode ser visto abaixo.

<div align="center">
<img src= "..\img\dataset1_with_clusters.png" alt="Plotagem do KMeans com k = 2" width="400">
</div>

### Métricas de validação
Por fim, é possível avaliar a qualidade do agrupamento gerado pelo K-Means, através das métricas de validação internas e externas, apresentadas em [2.Conceitos fundamentais](https://github.com/mevianna/ISA/tree/k_means/k_means/content/2.fundamental_concepts.md). Nesse exemplo, utilizou-se as métricas:
* Índice da Silhueta;
* Índice de Calinski-Harabasz;
* Índice de Davies-Bouldin (DB); 
* Índice de Rand Ajustado (ARI).

>[!NOTE]
> Neste dataset, é possível utilizar métricas de validação externa, pois há rótulos verdadeiros disponíveis na coluna 'species'.

Essas métricas são próprias da biblioteca do sklearn, diferentemente do Indice de Dunn que exige implementação manual. Por esse motivo, ele não é utilizado nesse exemplo.
O código para cálculo das métricas é:

```python
slht_score = silhouette_score(X_padronizado, labels)
clh_score = calinski_harabasz_score(X_padronizado, labels)
db_score = davies_bouldin_score(X_padronizado, labels)
ari_score = adjusted_rand_score(df['species'], labels)
```

As 3 primeiras linhas calculam métricas internas, com base apenas nos dados e nos rótulos gerados. Já a última linha, calcula o ARI, que é uma métrica de validação externa, assim, compara-se as classificações reais (coluna 'species' do dataset) e os rótulos obtidos.

O resultado dessas métricas pode ser visualizado em forma de tabela com o seguinte código:

```python
df_resultados = pd.DataFrame({
    'Métrica': ['Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin', 'Ari'],
    'Valor': [slht_score, clh_score, db_score, ari_score]
})

print(df_resultados.round(3))
```

A qual gera a seguinte tabela com aproximação de 3 casas decimais:

| Métrica           | Valor   |
| :---------------- | :------ |
| Silhouette        | 0.479   |
| Calinski-Harabasz | 156.143 |
| Davies-Bouldin    | 0.787   |
| Ari               | 0.429   |

Com base nos resultados das métricas, é possível observar que o índice de silhueta está abaixo de 0.5, o que indica que os dados não estão bem agrupados. Resultados semelhantes são observados nas demais métricas:
* O índice de Davies-Bouldin (DB) apresenta um alto valor, o que também sugere uma má separação entre os clusters, uma vez que valores mais próximos de 0 indicam melhor desempenho;
* O Adjusted Rand Index (ARI) demonstra um valor baixo, o que significa que há pouca semelhança entre os agrupamentos obtidos e os rótulos reais do dataset.
* O índice de Calinski-Harabasz (CH) embora apresente um valor relativamente maior, ele ainda é considerado baixo, pois ele pode apresentar valores de 0 ao infinito, o que reforça a ideia de que o agrupamento não foi o ideal.

Portanto, os resultados indicam que o agrupamento gerado com k = 3 não é satisfatório.

> [!WARNING]
> Vários fatores podem justificar esse resultado, como:
> * Redução para apenas duas dimensões;
> * Escolha inadequada das dimensões utilizadas;
> * A própria estrutura dos dados, que pode não se ajustar bem ao modelo K-Means.

### Execução com k = 2
Caso seja utilizado k = 2, o agrupamento se torna:

<div align="center">
<img src= "..\img\dataset1_with_2_clusters.png" alt="Plotagem do KMeans com k = 2" width="400">
</div>

Com as seguintes métricas de validação:

| Métrica           | Valor   |
| :---------------- | :------ |
| Silhouette        | 0.580   |
| Calinski-Harabasz | 248.903 |
| Davies-Bouldin    | 0.597   |
| Ari               | 0.568   |

Esses valores indicam que o agrupamento foi melhor realizado. Então, para essa escolha de dimensões, k = 2 obtém a melhor qualidade de agrupamento.

> [!IMPORTANT]
> Testes adicionais mostram que, independentemente do par de atributos escolhido (entre os quatro disponíveis), os resultados das métricas de validação permanecem iguais para um mesmo valor de k. Ou seja, trocar o par de atributos  não altera os resultados das métricas, para qualquer valor de k. Teste você mesmo e confirme!

### Pior valor de k
A título de curiosidade, o pior valor de k, que pode ser visto como o ponto mais baixo no gráfico da silhueta, é k = 7. As métricas de validação obtidas nesse caso são:

| Métrica           | Valor      |
| :---------------- | :--------- |
| Silhouette        | 0.268562   |
| Calinski-Harabasz | 152.748286 |
| Davies-Bouldin    | 1.121924   |
| Ari               | 0.388810   |

Os quais são valores péssimos, independetemente da métrica, indicam que é um agrupamento ruim. Isso reforça a importância de uma boa escolha de k, já que valores inadequados podem prejudicar a qualidade dos agrupamentos.

***

### Referências
**SCIKIT-LEARN**. sklearn.cluster. KMeans. Disponível em: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html.

**SCIKIT-LEARN**. Demonstration of k-means assumptions. Disponível em: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py. 
