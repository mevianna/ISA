# English Version
To better understand how the K-Means algorithm works, it is interesting to visualize it operating with simulated data. This approach allows you to more clearly observe how the algorithm identifies and forms good clusters.
It is recommended that you first read the file [1.kmeans_with_iris_dataset.md](https://github.com/mevianna/ISA/tree/k_means/k_means/code/1.kmeans_with_iris_dataset.md.md), as this document presents practically the same code as the first one, explaining only the main differences.

## Code

### Importing libraries

To run K-Means with simulated data, you need the following libraries:
```python
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
```

The main difference in relation to the example with the Iris dataset is in the line ```from sklearn.datasets import make_blobs```. This line imports the make_blobs function from the scikit-learn library, which is used to generate clustered artificial data. This function makes it easier to create datasets with a defined distribution, ideal for testing and visualizing clustering algorithms.

### Creating simulated data:

To create the simulated dataset, execute the following instruction:

```python
X, real_labels = make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=42)
```

In it, the ```make_blobs``` function is used to create the simulated data. The arguments of this function are:
* ```n_samples=300```: number of points generated (in this case, 300);
* ```centers=3```: number of centers (groups), which is 3;
* ```cluster_std=1.0```: standard deviation of the points around the centroids that are associated (the larger, the more spread out);
* ```random_state=42```: defines the seed for random generation, ensuring that the data is always the same when the code is run (reproducibility).

While the function returns two values:
* ```X```: the coordinates of the generated points, usually a NumPy array, by default, in two dimensions;
* ```real_labels```: the true labels of each point.

### Plotting the graph with simulated data:

To visualize the generated data, we can execute the following code:

```python
plt.figure(figsize=(8, 5))
plt.scatter(X[:, 0], X[:, 1], c='hotpink', s=50)
plt.title("Clusters com K-Means (dados simulados)")
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.grid(True)
plt.show()
```

This snippet creates a scatter plot, similar to those used previously, showing the simulated points in two-dimensional space, which can be seen below.

<div align="center">
<img src= "..\img\dataset2_without_clusters.png" alt="Plotting the simulated data." width="400">
</div>

> [!NOTE]
> The data generated by the ```make_blobs``` function already has the same scale by default, since all attributes are created with similar variances. Therefore, it is not necessary to normalize the data.

### Executing K-Means:
After creating the simulated data, the next step is to apply the K-Means algorithm to perform the clustering. This can be done with the following code snippet:

```python
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)
```

This code follows the same logic used in previous examples:
* ```KMeans(n_clusters=3)```: instantiates the K-Means algorithm, defining that we want to form 3 clusters;
* ```random_state=42```: ensures reproducibility of the results;
* ```fit_predict(X)```: fits the model to the data and returns the labels corresponding to the cluster of each point. These labels will be used to color the data according to the grouping throughout the visualizations.

### Plotting the graph with K-Means:

After running K-Means, you can view the clusters formed with the following code:

```python
plt.figure(figsize=(8, 5))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='spring', s=50)
plt.title("Clusters with K-Means (simulated data)")
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.grid(True)
plt.show()
```

This code colors the points in the data set according to the clusters. The graph can be seen below:

<div align="center">
<img src="..\img\dataset2_with_clusters.png" alt="Cluster plot for simulated data" width="400">
</div>

### Validation metrics:

As in the previous example, it is possible to evaluate the quality of the clusters using internal and external validation metrics. Next, the following metrics are applied, the same as in the previous example:
* Silhouette Index
* Calinski-Harabasz Index
* Davies-Bouldin Index
* Adjusted Rand Index (ARI)

The calculation can be done with the code below:

```python
slht_score = silhouette_score(X, labels)
clh_score = calinski_harabasz_score(X, labels)
db_score = davies_bouldin_score(X, labels)
ari_score = adjusted_rand_score(real_labels, labels)
```

The first three metrics are internal, as they evaluate the clusters based only on the structure of the data. The last one, the ARI, is an external metric, which compares the assigned labels with the real labels.

To display the results in an organized way, in a table, you can use:

```python
df_resultados = pd.DataFrame({
'Metric': ['Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin', 'ARI'],
'Value': [slht_score, clh_score, db_score, ari_score]
})

print(df_resultados.round(3))
```

Which generates the following table:

|Metric |Value |
|-------------------|-----------|
| Silhouette | 0.924 |
| Calinski-Harabasz | 20582.156 |
| Davies-Bouldin | 0.107 |
| ARI | 1.000 |

The interpretation of the results can be as follows:
* Silhouette ≈ 0.92: a value close to 1 indicates that the points are well grouped and distant from the other clusters;
* Calinski-Harabasz high: suggests that the clusters are compact and well separated;
* Davies-Bouldin close to 0: indicates that the clusters have good separation and low overlap;
* ARI = 1.0: means that the K-Means grouping was identical to the real labels.

These results are excellent, which was expected, since the data were artificially generated with well-defined separations. In real data, such as those from the Iris set, the groupings may not present such good metrics, reflecting limitations of the algorithm or complexity in the data.

***
### References
**SCIKIT-LEARN**. sklearn.cluster. KMeans. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html.

**SCIKIT-LEARN**. Demonstration of k-means assumptions. Available at: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py.

***

# Portuguese Version

Para um melhor entendimento do funcionamento do algoritmo K-Means, é interessante visualizá-lo operando com dados simulados. Essa abordagem permite observar de forma mais clara como o algoritmo identifica e forma bons agrupamentos (clusters).
Recomenda-se a leitura prévia do arquivo [1.kmeans_with_iris_dataset.md](https://github.com/mevianna/ISA/tree/k_means/k_means/code/1.kmeans_with_iris_dataset.md.md), pois este documento se apresenta praticamente o mesmo código do primeiro, explicando apenas as principais diferenças.

## Código

### Importação de bibliotecas

Para executar o K-Means com dados simulados, é necessário as seguintes bibliotecas:

```python
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
```
A principal diferença em relação ao exemplo com o dataset Iris está na linha ```from sklearn.datasets import make_blobs```. Nessa linha, é importada a função make_blobs da biblioteca scikit-learn, utilizada para gerar dados artificiais agrupados. Essa função facilita a criação de conjuntos de dados com distribuição definida, ideais para testes e visualização de algoritmos de agrupamento.

### Criação dos dados simulados:

Para criar o conjunto de dados simulados, executa-se a seguinte instrução:

```python
X, real_labels = make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=42)
```

Nela, utiliza-se a função ```make_blobs``` para criar os dados simulados. Os argumentos dessa função são:
* ```n_samples=300```: número de pontos gerados (nesse caso, 300);
* ```centers=3```: número de centros (grupos), que é 3;
* ```cluster_std=1.0```: desvio padrão de dos pontos em torno dos centróides que estão associados (quanto maior, mais espalhados);
* ```random_state=42```: define a semente para geração aleatória, garantindo que os dados sejam sempre os mesmos quando rodar o código (reprodutibilidade).

Enquanto o retorno da função são dois valores:
* ```X```: as coordenadas dos pontos gerados, geralmente um array NumPy, por padrão, em duas dimensões;
* ```real_labels```: os rótulos verdadeiros de cada ponto.

### Plotagem do gráfico com dados simulados:

Para visualizar os dados gerados, podemos executar o seguinte código:

```python
plt.figure(figsize=(8, 5))
plt.scatter(X[:, 0], X[:, 1], c='hotpink', s=50)
plt.title("Clusters com K-Means (dados simulados)")
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.grid(True)
plt.show()
```

Esse trecho cria um gráfico de dispersão, semelhante aos já utilizados anteriormente, mostrando os pontos simulados no espaço bidimensional, o qual pode ser visto abaixo.

<div align="center">
<img src= "..\img\dataset2_without_clusters.png" alt="Plotagem dos dados simulados" width="400">
</div>

> [!NOTE]
> Os dados gerados pela função ```make_blobs``` já possuem a mesma escala por padrão, pois todos os atributos são criados com variâncias semelhantes. Por isso, não é necessário normalizar os dados.

### Execução do K-Means:

Após a criação dos dados simulados, o próximo passo é aplicar o algoritmo K-Means para realizar o agrupamento. Isso pode ser feito com o seguinte trecho de código:

```python
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)
```

Esse código segue a mesma lógica utilizada em exemplos anteriores:
* ```KMeans(n_clusters=3)```: instancia o algoritmo K-Means, definindo que queremos formar 3 clusters;
* ```random_state=42```: garante reprodutibilidade dos resultados;
* ```fit_predict(X)```: ajusta o modelo aos dados e retorna os rótulos (labels) correspondentes ao cluster de cada ponto.Esses rótulos serão utilizados para colorir os dados de acordo com o agrupamento ao longo das visualizações.

### Plotagem do gráfico com K-Means:

Após a execução do K-Means, é possível visualizar os agrupamentos formados com o seguinte código:

```python
plt.figure(figsize=(8, 5))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='spring', s=50)
plt.title("Clusters com K-Means (dados simulados)")
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.grid(True)
plt.show()
```

Esse código colore os pontos do conjunto de dados de acordo com os agrupamentos. O gráfico pode ser visto abaixo:

<div align="center">
<img src= "..\img\dataset2_with_clusters.png" alt="Plotagem dos agrupamentos para dados simulados" width="400">
</div>

### Métricas de validação:

Assim como no exemplo anterior, é possível avaliar a qualidade dos agrupamentos utilizando métricas de validação interna e externa. A seguir, são aplicadas as seguintes métricas, iguais ao exemplo anterior:
* Índice de Silhueta
* Índice de Calinski-Harabasz
* Índice de Davies-Bouldin
* Índice Rand Ajustado (ARI)

O cálculo pode ser feito com o código abaixo:

```python
slht_score = silhouette_score(X, labels)
clh_score = calinski_harabasz_score(X, labels)
db_score = davies_bouldin_score(X, labels)
ari_score = adjusted_rand_score(real_labels, labels)
```

As três primeiras métricas são internas, pois avaliam os agrupamentos com base apenas na estrutura dos dados. Já a última, o ARI, é uma métrica externa, que compara os rótulos atribuídos com os rótulos reais.

Para exibir os resultados de forma organizada, em uma tabela, pode-se utilizar:

```python
df_resultados = pd.DataFrame({
    'Métrica': ['Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin', 'ARI'],
    'Valor': [slht_score, clh_score, db_score, ari_score]
})

print(df_resultados.round(3))
```

O que gera a seguinte tabela:

|Métrica            |Valor      |
|-------------------|-----------|
| Silhouette        | 0.924     |
| Calinski-Harabasz | 20582.156 |
| Davies-Bouldin    | 0.107     |
| ARI               | 1.000     |

A interpretação dos resultados pode ser:
* Silhouette ≈ 0.92: valor próximo de 1 indica que os pontos estão bem agrupados e distantes dos demais clusters;
* Calinski-Harabasz alto: sugere que os clusters são compactos e bem separados;
* Davies-Bouldin próximo de 0: indica que os clusters têm boa separação e baixa sobreposição;
* ARI = 1.0: significa que o agrupamento do K-Means foi idêntico aos rótulos reais.

Esses resultados são excelentes, o que era esperado, já que os dados foram gerados artificialmente com separações bem definidas. Em dados reais, como os do conjunto Iris, os agrupamentos podem não apresentar métricas tão boas, refletindo limitações do algoritmo ou complexidade nos dados.

***

### Referências
**SCIKIT-LEARN**. sklearn.cluster. KMeans. Disponível em: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html.

**SCIKIT-LEARN**. Demonstration of k-means assumptions. Disponível em: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py. 
