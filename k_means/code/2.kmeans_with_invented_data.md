# English Version

# Portuguese Version

Para um melhor entendimento do funcionamento do algoritmo K-Means, é interessante visualizá-lo operando com dados simulados. Essa abordagem permite observar de forma mais clara como o algoritmo identifica e forma bons agrupamentos (clusters).
Recomenda-se a leitura prévia do arquivo [1.kmeans_with_iris_dataset.md](https://github.com/mevianna/ISA/tree/k_means/k_means/code/1.kmeans_with_iris_dataset.md.md), pois este documento se apresenta praticamente o mesmo código do primeiro, explicando apenas as principais diferenças.

## Código

### Importação de bibliotecas

Para executar o K-Means com dados simulados, é necessário as seguintes bibliotecas:

```python
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
```
A principal diferença em relação ao exemplo com o dataset Iris está na linha ```from sklearn.datasets import make_blobs```. Nessa linha, é importada a função make_blobs da biblioteca scikit-learn, utilizada para gerar dados artificiais agrupados. Essa função facilita a criação de conjuntos de dados com distribuição definida, ideais para testes e visualização de algoritmos de agrupamento.

### Criação dos dados simulados:

Para criar o conjunto de dados simulados, executa-se a seguinte instrução:

```python
X, real_labels = make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=42)
```

Nela, utiliza-se a função ```make_blobs``` para criar os dados simulados. Os argumentos dessa função são:
* ```n_samples=300```: número de pontos gerados (nesse caso, 300);
* ```centers=3```: número de centros (grupos), que é 3;
* ```cluster_std=1.0```: desvio padrão de dos pontos em torno dos centróides que estão associados (quanto maior, mais espalhados);
* ```random_state=42```: define a semente para geração aleatória, garantindo que os dados sejam sempre os mesmos quando rodar o código (reprodutibilidade).

Enquanto o retorno da função são dois valores:
* ```X```: as coordenadas dos pontos gerados, geralmente um array NumPy, por padrão, em duas dimensões;
* ```real_labels```: os rótulos verdadeiros de cada ponto.

### Plotagem do gráfico com dados simulados:

Para visualizar os dados gerados, podemos executar o seguinte código:

```python
plt.figure(figsize=(8, 5))
plt.scatter(X[:, 0], X[:, 1], c='hotpink', s=50)
plt.title("Clusters com K-Means (dados simulados)")
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.grid(True)
plt.show()
```

Esse trecho cria um gráfico de dispersão, semelhante aos já utilizados anteriormente, mostrando os pontos simulados no espaço bidimensional, o qual pode ser visto abaixo.

<div align="center">
<img src= "..\img\dataset2_without_clusters.png" alt="Plotagem dos dados simulados" width="400">
</div>

> [!NOTE]
> Os dados gerados pela função ```make_blobs``` já possuem a mesma escala por padrão, pois todos os atributos são criados com variâncias semelhantes. Por isso, não é necessário normalizar os dados.

### Execução do K-Means:

Após a criação dos dados simulados, o próximo passo é aplicar o algoritmo K-Means para realizar o agrupamento. Isso pode ser feito com o seguinte trecho de código:

```python
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)
```

Esse código segue a mesma lógica utilizada em exemplos anteriores:
* ```KMeans(n_clusters=3)```: instancia o algoritmo K-Means, definindo que queremos formar 3 clusters;
* ```random_state=42```: garante reprodutibilidade dos resultados;
* ```fit_predict(X)```: ajusta o modelo aos dados e retorna os rótulos (labels) correspondentes ao cluster de cada ponto.Esses rótulos serão utilizados para colorir os dados de acordo com o agrupamento ao longo das visualizações.

### Plotagem do gráfico com K-Means:

Após a execução do K-Means, é possível visualizar os agrupamentos formados com o seguinte código:

```python
plt.figure(figsize=(8, 5))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='spring', s=50)
plt.title("Clusters com K-Means (dados simulados)")
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.grid(True)
plt.show()
```

Esse código colore os pontos do conjunto de dados de acordo com os agrupamentos. O gráfico pode ser visto abaixo:

<div align="center">
<img src= "..\img\dataset2_without_clusters.png" alt="Plotagem dos agrupamentos para dados simulados" width="400">
</div>

### Métricas de validação:

Assim como no exemplo anterior, é possível avaliar a qualidade dos agrupamentos utilizando métricas de validação interna e externa. A seguir, são aplicadas as seguintes métricas, iguais ao exemplo anterior:
* Índice de Silhueta
* Índice de Calinski-Harabasz
* Índice de Davies-Bouldin
* Índice Rand Ajustado (ARI)

O cálculo pode ser feito com o código abaixo:

```python
slht_score = silhouette_score(X, labels)
clh_score = calinski_harabasz_score(X, labels)
db_score = davies_bouldin_score(X, labels)
ari_score = adjusted_rand_score(real_labels, labels)
```

As três primeiras métricas são internas, pois avaliam os agrupamentos com base apenas na estrutura dos dados. Já a última, o ARI, é uma métrica externa, que compara os rótulos atribuídos com os rótulos reais.

Para exibir os resultados de forma organizada, em uma tabela, pode-se utilizar:

```python
df_resultados = pd.DataFrame({
    'Métrica': ['Silhouette', 'Calinski-Harabasz', 'Davies-Bouldin', 'ARI'],
    'Valor': [slht_score, clh_score, db_score, ari_score]
})

print(df_resultados.round(3))
```

O que gera a seguinte tabela:

|Métrica            |Valor      |
|-------------------|-----------|
| Silhouette        | 0.924     |
| Calinski-Harabasz | 20582.156 |
| Davies-Bouldin    | 0.107     |
| ARI               | 1.000     |

A interpretação dos resultados pode ser:
* Silhouette ≈ 0.92: valor próximo de 1 indica que os pontos estão bem agrupados e distantes dos demais clusters;
* Calinski-Harabasz alto: sugere que os clusters são compactos e bem separados;
* Davies-Bouldin próximo de 0: indica que os clusters têm boa separação e baixa sobreposição;
* ARI = 1.0: significa que o agrupamento do K-Means foi idêntico aos rótulos reais.

Esses resultados são excelentes, o que era esperado, já que os dados foram gerados artificialmente com separações bem definidas. Em dados reais, como os do conjunto Iris, os agrupamentos podem não apresentar métricas tão boas, refletindo limitações do algoritmo ou complexidade nos dados.

***

### Referências
**SCIKIT-LEARN**. sklearn.cluster. KMeans. Disponível em: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html.

**SCIKIT-LEARN**. Demonstration of k-means assumptions. Disponível em: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py. 