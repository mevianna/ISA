# English Version

Having understood the fundamental requirements for the correct functioning of the K-means algorithm, the step-by-step execution will now be presented. In other words, we will explain, stage by stage, how K-Means performs data clustering until convergence is achieved.

> [!NOTE]
> The algorithm's implementation, presented in code, is available at: [1.Code](https://github.com/mevianna/ISA/tree/k_means/k_means/code/1.code.md)

### Step 0: Initialization of stopping criteria
Before starting the model and its essential points, it is necessary to consider *when will my algorithm stop?* Thus, in addition to aiming for convergence, it is crucial to consider the maximum number of iterations so that the model does not risk running for excessive time.

> Furthermore, it is at this point that other stopping criteria are defined, as discussed in [5.When the algorithm stops?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/5.when_the_algorithm_stops?.md).

### First step: Choosing the number of clusters
The first step of the entire process involves choosing the number of clusters, i.e., the value of k. Suppose that, for a given dataset, `k = 3` has been chosen, based on the best possible result presented by the methods seen in [4.How to choose the number of clusters?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/3.how_to_choose_the_number_of_clusters.md).

### Second step: Choosing the initial centroids
With the number of clusters defined, it's time to choose the initial centroids. In this case, the centroids are assumed to have been chosen using the **Random Data Points** method, as seen in [4.How to choose the initial centroids?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/4.how_to_choose_the_initial_centroids.md), which also presents other selection methods.

> [!IMPORTANT]
> Considering that 3 clusters were chosen, as previously mentioned, how many centroids are then needed?
<details>
  <summary>Click to see the answer</summary>
  For each cluster, only 1 centroid is needed. Therefore, 3 centroids are required, one for each cluster.
</details>

### Third step: Assigning points to clusters
With the number of clusters and centroids defined, the other points in the dataset are ready to be assigned to the closest clusters, according to their distances to the centroids. In this step, the Euclidean distance is calculated between each point in the dataset and the centroids. The point is associated with the cluster whose centroid is closest, i.e., the one with the smallest distance among those calculated.

### Fourth step: Analysis of stopping criteria
With the points assigned to a cluster, the algorithm checks if any stopping criterion has been met. For example, if one of the criteria is to execute only one iteration, the algorithm would stop quickly. However, it is important to emphasize that the algorithm can only analyze convergence from the second iteration onwards, as it needs to compare the current results with those from the previous iteration to verify whether or not there have been significant changes in the positions of the centroids or in the assignments of the points.

### Fifth step: Recalculating the centroids
With all points assigned to a cluster, according to their distances to the centroids, the centroids are then recalculated. In the first iteration, the centroids are initially chosen following a previously defined method, as discussed. However, from the second iteration onwards, for each formed cluster, the arithmetic mean of the positions of all points belonging to that cluster is calculated. Thus, this mean is the new centroid, which represents the "center of mass" of the cluster – that is, the most densely occupied region of the cluster.

In the example discussed, for each of the 3 clusters, the average of the positions of all points assigned to it would be calculated. This would yield 3 averages, which would correspond to the new centroids to be used.

### Sixth step: Repetition
Subsequently, after the centroids are redefined, the points are again assigned to the closest centroids. Therefore, steps 3, 4, and 5 are repeated iteratively until, as mentioned, at least one of the stopping criteria is met. This characteristic ensures that the algorithm runs until it achieves the best possible configuration, both in terms of centroid positions and point assignments.

### Seventh step: Model evaluation
After the algorithm terminates, if desired, the efficiency of the algorithm can be calculated. For this purpose, metrics such as inertia calculation or the Dunn Index can be used, as seen in [2.Fundamental Concepts](https://github.com/mevianna/ISA/tree/k_means/k_means/content/2.fundamental_concepts.md).

***

> [!WARNING]
> If you encountered any difficulty at any point in this file, it is recommended that you read the previous files about K-Means Clustering, starting at: [1.Introduction](https://github.com/mevianna/ISA/tree/k_means/k_means/content/1.introduction.md).

***

## References:
**BISHOP, C. M.**. Pattern Recognition and Machine Learning. New York: Springer, 2006. Available at: <https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>. Accessed: May 30, 2025.
**IBM.** K-means clustering. Available at: <https://www.ibm.com/think/topics/k-means-clustering>. Accessed: May 14, 2025.
**SCIKIT-LEARN**. sklearn.cluster.KMeans. Available at: <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html>. Accessed: May 14, 2025.

***

# Portuguese Version

Tendo visto e compreendido os requisitos fundamentais para o funcionamento correto do algoritmo K-means, a seguir será apresentado o passo-a-passo de sua execução. Ou seja, vamos explicar, etapa por etapa, como o K-Means realiza o agrupamento dos dados em clusters até atingir a convergência.

> [!NOTE]
> A implementação do algoritmo, apresentada em código, está disponível em: [1.Code](https://github.com/mevianna/ISA/tree/k_means/k_means/code/1.code.md)

### Etapa 0: inicialização de critérios de parada
Antes de iniciar o modelo e seus pontos essenciais, é necessário pensar *quando o meu algoritmo irá parar?* Com isso, além da convergência que visa ser atingido, é preciso considerar o número de iterações máximo para o modelo não correr o risco de executar por tempo excedente.

> Ademais, é neste momento que se define se haverá outros critérios de parada, conforme abordado em [5.Quando o algoritmo para?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/5.when_the_algorithm_stops?.md).

### Primeira etapa: escolha do número de clusters
A primeira etapa de todo o processo consiste em escolher o número de clusters, isto é, o valor de k. Suponha-se que, para determinado conjunto de dados, tenha sido escolhido ```k = 3```, com base no melhor resultado possível apresentado pelos métodos vistos em [4.Como escolher o número de clusters?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/3.how_to_choose_the_number_of_clusters.md).

### Segunda etapa: escolha dos centróides iniciais
Com o número de clusters definidos, é a vez da escolha dos centróides iniciais. Neste caso, os centróides, supõe-se, terem sido escolhidos com o método **Pontos de dados aleatórios**, conforme visto em [4.Como escolher os centróides iniciais?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/4.how_to_choose_the_initial_centroids.md), que também apresenta outros métodos de escolha.

 > [!IMPORTANT]
 > Considerando que foram escolhidos 3 clusters, como dito anteriormente, quantos centróides são, então, necessários?
 <details>
   <summary>Aperte para ver a resposta</summary>
   Para cada cluster, é necessário apenas 1 centróide. Dessa forma, são precisos 3 centróides, um para cada cluster.
 </details>

### Terceira etapa: atribuição dos pontos aos clusters
Com o número de clusters e os centróides definidos, os demais pontos do conjunto estão prontos para serem atribuídos aos clusters mais próximos, de acordo com as distâncias até os centróides. Nesta etapa, a distância euclidiana é calculada entre cada ponto do conjunto e os centróides. O ponto é associado ao cluster cujo centróide estiver mais próximo, ou seja, aquele que apresentar a menor distância dentre as calculadas. 

### Quarta etapa: análise dos critérios de parada
Com os pontos atribuídos a um cluster, o algoritmo analisa se algum critério de parada foi atingido. Por exemplo, caso um dos critérios seja a execução de apenas uma iteração, o algoritmo encerraria sua execução rapidamente. No entanto, é importante ressaltar que o algoritmo apenas consegue analisar a convergência a partir da segunda iteração, pois é preciso comparar os resultados atuais com os da iteração anterior, com o objetivo de verificar se houve ou não mudanças significativas nas posições dos centróides ou nas atribuições dos pontos.

### Quinta etapa: recálculo dos centróides
Com todos os pontos atribuídos a um cluster, de acordo com as distâncias até os centróides, é, então, recalculado os centróides. Na primeira iteração, os centróides são inicialmente escolhidos seguindo algum método previamente definido, como abordado. Porém, a partir da segunda iteração, para cada cluster formado, calcula-se a média aritmética das posições de todos os pontos que pertencem a esse cluster. Assim, tal média é o novo centróide, o qual representa o "centro de massa" do cluster - ou seja, a região mais densamente ocupada do cluster.

No exemplo abordado, para cada um dos 3 clusters, seria realizada a média das posições de todos os pontos atribuídos a ele. Dessa forma, seriam obtidas 3 médias, que corresponderiam aos novos centróides a serem utilizados.

### Sexta etapa: repetição
Em seguida, após a redefinição dos centróides, os pontos são novamente atribuídos aos centróides mais próximos. Portanto, as etapas 3, 4 e 5 são repetidas de forma iterativa até que, conforme mencionado, pelo menos um dos critérios de parada seja atingido. Essa caracteristíca garante que o algoritmo seja executado até que alcance a melhor configuração possível, tanto da posição dos centróides quanto na atribuição dos pontos.

### Sétima etapa: avaliação do modelo
Após o encerramento do algoritmo, caso deseje, é possível calcular a eficiência do algoritmo. Para isso, é possível utilizar métricas como o cálculo da inércia ou o Índice de Dunn, conforme visto em [2.Conceitos fundamentais](https://github.com/mevianna/ISA/tree/k_means/k_means/content/2.fundamental_concepts.md).

***

> [!WARNING]
> Caso você tenha sentido dificuldade em algum momento deste arquivo, recomenda-se que você leia os arquivos anteriores acerca do K-Means Clusterning, começando em: [1.Introdução](https://github.com/mevianna/ISA/tree/k_means/k_means/content/1.introduction.md).

***

### Referências:
**BISHOP, C. M.**. Pattern Recognition and Machine Learning. New York: Springer, 2006. Disponível em: <https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>. Acesso em: 30 mai. 2025.
**IBM**. K-means clustering. Disponível em: <https://www.ibm.com/think/topics/k-means-clustering>. Acesso em: 14 maio 2025.
**SCIKIT-LEARN**. sklearn.cluster.KMeans. Disponível em: <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html>. Acesso em: 14 maio 2025.