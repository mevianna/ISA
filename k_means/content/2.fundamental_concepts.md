## English version

To understand how K-Means works, it's necessary to grasp some fundamental concepts, which will be covered throughout this research. The following are essential concepts to facilitate understanding of the algorithm and how it operates.

### Unlabeled Data
K-means operates on datasets where there is no pre-existing information about which "class" or "group" each point belongs to. The goal of the algorithm is to discover this inherent structure within the data.

### Data Points
Each individual observation in your dataset. It can be a single number (in one dimension), a pair of numbers (in two dimensions, like on an x-y graph), or a sequence of numbers (in multiple dimensions, representing different features).

### Clusters
The groups that the K-means algorithm tries to identify in the data. The goal is for the points within the same cluster to be similar to each other and different from the points in other clusters.

### Centroids
These are the central points of each cluster. Initially, they can be chosen randomly or by some heuristic method. During the K-means process, centroids are iteratively recalculated as the mean of all points assigned to that cluster.

### Assigning Points to Clusters
Each data point is assigned to the cluster whose centroid is closest to it, based on some distance measure.

***

## Calculating Distance
The calculation of the distance between data points and centroids is a crucial step for K-means, as it determines which cluster each point will be assigned to. Therefore, the choice of the distance metric directly influences the shape and structure of the resulting clusters.

### Euclidean Distance Metric
Euclidean distance is the most common way to measure the distance between two points. To calculate the Euclidean distance between two points, p and q, in an n-dimensional space, where p is defined as (p_1,p_2,...,p_n) and q as (q_1,q_2,...,q_n), we use the following formula:


$$d(p, q) = \sqrt{\sum_{i=1}^{n} (q_i - p_i)^2}$$
​
In this formula, the symbol 
sum indicates summation, n is the number of dimensions of the space, and p_i and q_i are the coordinates of points p and q, respectively. The calculation involves the following steps: first, calculate the difference between the coordinates of the two points in each dimension; then, square each of these differences; next, sum all the resulting values; and, finally, calculate the square root of this sum, obtaining the Euclidean distance between the two points.

**Example:**

$$\sqrt{(4 - 1)^2 + (6 - 2)^2} = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = \sqrt{25} = 5$$

<div align="center">
<img src="https://media.datacamp.com/cms/google/ad_4nxfw85hlpdhvogfbjrxi9jiktdv2memq-leatixmm_u0_jevvnifvpnhfza7b7xxbd3uw_ilm-rqvtf-xe9hxnks01or84mh4hfuy7ejn2dc6j1jzu691cl0f8dfqgdgjogzkp45p0eekkex8rrrxzzbxtol.png" alt="2D Euclidean distance graph" width="800">
<p align="center"><b>The image above is a 2D representation of Euclidean Distance.</b></p>
</div>

## Validation Metrics
Since the K-Means algorithm does not use true labels during training, it is necessary to evaluate the quality of the formed clusters using validation metrics. These metrics check if the generated clusters are compact, well-separated from each other, and coherent with the data.

There are two types of metrics: internal and external. Internal metrics assess how well the data is clustered based only on the data itself and the results obtained. External metrics, on the other hand, compare the resulting clusters with the true ones, when they exist, allowing for a check of the similarity of the results to the expected classification.

### Inertia (Within-Cluster Sum of Squares - WCSS)
Inertia is a fundamental internal metric that quantifies the sum of squared distances of each point within a cluster to its respective centroid. In more detail, inertia is calculated by measuring the distance between a data point and its centroid, squaring this distance, and summing these squares for each point in the cluster, as per the formula below:

$$
Inércia = Σ_{x ∈ C} ||x - μ||² = ||x_1 - μ||² + ||x_2 - μ||² + ⋯ + ||x_n - μ||²
$$

where:

- `C = { x₁, x₂, ..., xₙ }` is the set of `n` points in the cluster,

- `x_i`  represents each point within cluster `C`,

- `μ` is the centroid of the cluster,
  
- `|| · ||` represents the Euclidean distance.

The summation ends after summing the squared distance of all (n) points in the cluster to the centroid. This sum is the representation of the intra-cluster distance; that is, the smaller the sum, the better, as it means that the points within the clusters are more compact or similar. Therefore, the goal of the algorithm is to minimize this inertia.

For the algorithm as a whole, the total inertia is obtained by summing the inertia of each cluster, as per the formula below.

$$
Inércia = Σ_{i = 1}^{k} Σ_{x ∈ C_i} |x - μ_i|²
$$


- `k` is the number of clusters,

- `C_i` is the set of points belonging to cluster i,

- `x` is a point within cluster C_i,

- `μ_i` is the centroid of cluster i,

- `|| · ||` represents the Euclidean distance.

It is important to note that inertia always decreases with an increase in the number of clusters and assumes that the clusters are convex and isotropic—which may not reflect the real structure of the data. Thus, inertia is typically used to determine the ideal number of clusters, as presented in 4. How to choose the number of clusters?. To determine the quality of the clusters, it is recommended to use inertia along with other metrics.

### Dunn Index
The Dunn Index, also an internal metric, represents the ratio between the smallest distance between clusters and the largest distance within a cluster. Clusters with a high distance between them indicate better quality, as it means that the clusters are as different from each other as possible.

### Davies-Bouldin Index
The Davies-Bouldin validation index (DB index), proposed in 1979, is an internal validation metric that evaluates how similar clusters are internally and how different they are from each other. This index ranges from 0 to infinity, but unlike the Dunn index, the closer the value is to 0, the better the clustering.

Thus, this index yields better results for compact and well-separated clusters; in other words, low DB index values represent well-adjusted and externally distant groups.

### Calinski-Harabasz Index
The Calinski-Harabasz index, also known as the Variance Ratio Criterion, is another internal method for calculating the quality of clusters. It is the ratio between the sum of the dispersion between the clusters and the sum of the internal dispersion of the clusters, considering all the formed clusters.

Dispersion is calculated through the sum of squared distances, that is, by the method of least squares, both between the clusters and within each cluster.
Therefore, the higher the value of the index, the better the clusters, because it means that the points in each cluster are close to each other (high density) and each cluster is far from the others.

### Rand Index
The Rand Index, proposed in 1971 by William M. Rand, is an external metric used to evaluate the similarity between two distinct clusterings. This index measures how similar the clusters obtained by the algorithm are to the real result, when it is known.

This index is calculated based on the ratio between the sum of the number of pairs of elements that are in the same cluster in both cases and the number of pairs of elements that are in different clusters in both cases, and the possible number of clusters.
The results range from 0 to 1, where 1 indicates a perfect clustering and 0, totally different.

>[!IMPORTANT]
>The Rand Index does not take into account randomly generated clusters, which can create a false impression that the clusters are coherent. This problem occurs mainly in large datasets, as most data pairs end up in different clusters, which increases the index value. Thus, there is the Adjusted Rand Index, which takes into account the possibility of similar clusters occurring by chance. In this version, negative results are allowed, which means that the clustering is worse than random.

***

References
DATACAMP. Euclidean Distance: A Complete Guide. Available at: https://www.datacamp.com/pt/tutorial/euclidean-distance.

DATACAMP. K-Means Clustering in Python: A Practical Guide. Available at: https://www.datacamp.com/tutorial/k-means-clustering-python.

IBM. K-means Clustering. Available at: https://www.ibm.com/think/topics/k-means-clustering.

OLIVEIRA, Anderson F.. Agrupamento de dados utilizando algoritmo K-Modes. 2019. Master's Thesis (Computer Science) – Faculdade de Ciências, Universidade Estadual Paulista, São José do Rio Preto, 2019. Available at: https://www.cc.faccamp.br/Dissertacoes/AndersonFranciscoOliveira.pdf.

SV, Misha. Calinski‑Harabasz Index for K‑Means Clustering Evaluation using Python. Towards Data Science, Medium, 2021. Available at: https://towardsdatascience.com/calinski-harabasz-index-for-k-means-clustering-evaluation-using-python-4fefeeb2988e/.

***

Contributors
| <img loading="lazy" src="https://avatars.githubusercontent.com/u/207051125?s=400&u=985341a59692bda296ac3e384872f8d5d92fb51a&v=4" width=115><br><sub>Arthur Bogoni (https://github.com/ArthurBogoni) |
| :---: | 

***

## Portuguese version

Para entender como funciona o K-Means, é necessário compreender alguns conceitos fundamentais, que serão abordados ao longo desta pesquisa. A seguir, são apresentados concepções essenciais para facilitar a compreensão do algoritmo e de seu funcionamento.

#### Dados Não Rotulados
O K-means opera em conjuntos de dados onde não há informações pré-existentes sobre a qual "classe" ou "grupo" cada ponto pertence. O objetivo do algoritmo é descobrir essa estrutura inerente nos dados.

#### Pontos de Dados (Data Points)
Cada observação individual no seu conjunto de dados. Pode ser um único número (em uma dimensão), um par de números (em duas dimensões, como em um gráfico x-y), ou uma sequência de números (em múltiplas dimensões, representando diferentes características).

#### Clusters
Os grupos que o algoritmo K-means tenta identificar nos dados. O objetivo é que os pontos dentro do mesmo cluster sejam semelhantes entre si e diferentes dos pontos em outros clusters.

#### Centróides
São os pontos centrais de cada cluster. Inicialmente, eles podem ser escolhidos aleatoriamente ou por algum método heurístico. Durante o processo do K-means, os centróides são recalculados iterativamente como a média de todos os pontos atribuídos aquele cluster.

#### Atribuição de Pontos a Clusters
Cada ponto de dados é atribuído ao cluster cujo centróide está mais próximo dele, com base em alguma medida de distância.

***

## Cálculo da distância
O cálculo da distância entre os pontos de dados e os centróides é um passo crucial para o K-means, pois determina a qual cluster cada ponto será atribuído. Logo, a escolha da métrica de distância influencia diretamente a forma e a estrutura dos clusters resultantes.

### Métrica de Distância Euclidiana
A distância euclidiana é a forma mais comum de medir a distância entre dois pontos. Para calcular a distância euclidiana entre dois pontos, $p$ e $q$, num espaço $n$-dimensional, onde $p$ é definido como $(p_1, p_2, ..., p_n)$ e $q$ como $(q_1, q_2, ..., q_n)$, usamos a seguinte fórmula:

$$d(p, q) = \sqrt{\sum_{i=1}^{n} (q_i - p_i)^2}$$

Nesta fórmula, o símbolo $\sum$ indica o somatório, $n$ é o número de dimensões do espaço e $p_i$ e $q_i$ são as coordenadas dos pontos $p$ e $q$, respectivamente. O cálculo envolve os seguintes passos: primeiro, calcula-se a diferença entre as coordenadas dos dois pontos em cada dimensão; depois, eleva-se ao quadrado cada uma dessas diferenças; em seguida, somam-se todos os valores resultantes; e, finalmente, calcula-se a raiz quadrada desta soma, obtendo-se a distância euclidiana entre os dois pontos.

**Exemplo:**

$$\sqrt{(4 - 1)^2 + (6 - 2)^2} = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = \sqrt{25} = 5$$

<div align="center">
  <img src="https://media.datacamp.com/cms/google/ad_4nxfw85hlpdhvogfbjrxi9jiktdv2memq-leatixmm_u0_jevvnifvpnhfza7b7xxbd3uw_ilm-rqvtf-xe9hxnks01or84mh4hfuy7ejn2dc6j1jzu691cl0f8dfqgdgjogzkp45p0eekkex8rrrxzzbxtol.png" alt="gráfico de distância euclidiana 2d" width="800">
  <p align="center"><b>A imagem acima é a representação em 2D da Distância Euclidiana.</b></p>
</div>
 



## Métricas de validação
Como o algoritmo K-Means não utiliza rótulos verdadeiros durante o treinamento, é preciso avaliar a qualidade dos agrupamentos formados por meio de métricas de validação. Essas métricas verificam se os agrupamentos gerados estão compactos, bem separados entre si e coerentes com os dados.

Existem dois tipos de métricas: internas e externas. As métricas internas avaliam o quão bem agrupado os dados estão baseado apenas nos próprios dados e nos resultados obtidos. Já as métricas externas comparam os agrupamentos resultantes com os verdadeiros, quando existem, permitindo verificar a semelhança dos resultados com a classificação esperada.

### Inércia (Within-Cluster Sum of Squares - WCSS)
A Inércia é uma métrica interna fundamental que quantifica a soma das distâncias quadráticas de cada ponto dentro de um cluster até o seu respectivo centróide. De forma mais detalhada, o cálculo da inércia é feito medindo a distância entre um ponto de dado e seu centróide, elevando essa distância ao quadrado e somando esses quadrados para cada ponto do agrupamento, conforme a fórmula abaixo:

$$
Inércia = Σ_{x ∈ C} ||x - μ||² = ||x_1 - μ||² + ||x_2 - μ||² + ⋯ + ||x_n - μ||²
$$

onde:  
- `C = { x₁, x₂, ..., xₙ }` é o conjunto de `n` pontos do cluster,
- `x_i` representa cada ponto dentro do cluster `C`,
- `μ` é o centróide do cluster,
- `|| · ||` representa a distância Euclidiana.

O somatório termina após somar a distância quadrática de todos os \(n\) pontos do cluster ao centróide. Essa soma é a representação da distância intra-agrupamento, ou seja, quanto menor a soma, melhor, pois isso significa que os pontos dentro dos agrupamentos são mais compactos ou semelhantes. Dessa forma, o objetivo do algoritmo é minimizar essa inércia.

Para o algoritmo como um todo, a inércia total é obtida a partir da soma da inércia de cada cluster, conforme a fórmula abaixo.

$$
Inércia = Σ_{i = 1}^{k} Σ_{x ∈ C_i} |x - μ_i|²
$$

onde:  
- `k` é o número de clusters,  
- `C_i` é o conjunto de pontos pertencentes ao cluster `i`,  
- `x` é um ponto dentro do cluster `C_i`,  
- `μ_i` é o centróide do cluster `i`,  
- `|| · ||` representa a distância Euclidiana.

É importante notar que a inércia sempre diminui com o aumento do número de clusters e assume que os clusters são convexos e isotrópicos - o que pode não refletir a estrutura real dos dados. Assim, a inércia normalmente é utilizada para determinar o número ideal de clusters, conforme apresentado em [4.Como escolher o número de clusters?](https://github.com/mevianna/ISA/tree/k_means/k_means/content/3.how_to_choose_the_number_of_clusters.md). Para determinar a qualidade dos agrupamentos, é indicado utilizar a inércia acompanhada de outras métricas.

### Índice de Dunn
O índice de Dunn, também uma métrica interna, representa a relação entre a menor distância entre clusters e a maior distância dentro de um cluster. Clusters com uma alta distância entre eles indicam melhor qualidade, pois isso significa que os clusters são o mais diferentes possível uns dos outros.

### Índice Davies & Bouldin
O índice de validação Davies-Bouldin (índice DB), proposto em 1979, é uma métrica de validação interna que avalia o quão similares os clusters são internamente e o quão diferentes eles são entre si. Esse índice varia de 0 ao infinito, mas, diferentemente do índice de Dunn, quanto mais próximo de 0 o valor, melhor o agrupamento. 

Assim, esse índice tem resultados melhores em agrupamentos compactos e separados entre si, ou seja, valores baixos do índice DB representam grupos bem ajustados e distantes externamente. 

### Índice Calinski-Harabasz
O índice Calinski-Harabasz, também conhecido como Critério de Razão de Variância, é outro método interno para calcular a qualidade de agrupamentos. Ele é a razão entre a soma da dispersão entre os clusters e a soma da dispersão interna dos clusters, considerando todos os agrupamentos formados.

A dispersão é calculada através da soma ao quadrado das distâncias, ou seja, pelo método dos minímos quadrados, tanto entre os clusters quanto dentro de cada cluster.
Dessa maneira, quanto maior o valor do índice, melhor os agrupamentos, pois  significa que os pontos de cada clusters estão próximos entre si (densidade alta) e cada cluster está distante um dos outros.

### Índice de Rand
O Índice de Rand, proposto em 1971 por William M. Rand, é uma métrica externa utilizada para avaliar a similaridade entre dois agrupamentos distintos. Esse índice mede o quão semelhantes os agrupamentos obtidos pelo algoritmo são do resultado real, quando este é conhecido.

Esse índice é calculado com base na razão entre a soma do número de pares de elementos que estão no mesmo agrupamento em ambos os casos e do número de pares de elementos que estão em agrupamentos diferentes em ambos os casos, e o número possível de agrupamentos.
Os resultados variam entre 0 e 1, sendo que 1 indica um agrupamento perfeito e 0, totalmente diferentes.

> [!IMPORTANT]
> O índice de Rand não leva em consideração agrupamentos gerados aleatoriamente, o que pode gerar a falsa impressão que os agrupamentos estão coerentes. Esse problema ocorre principalmente em grandes conjuntos de dados, pois a maioria dos pares de dados fica em clusters diferentes, o que aumenta o valor do índice. Assim, existe o Índice de Rand Ajustado, que leva em consideração a possibilidade de agrupamentos semelhantes ocorrerem por acaso. Nessa versão, é permitido resultados negativos, o que significa que o agrupamento está pior que o acaso.

***

## Referências
**DATACAMP**. Euclidean Distance: A Complete Guide. Disponível em: <https://www.datacamp.com/pt/tutorial/euclidean-distance>.

**DATACAMP**. K-Means Clustering in Python: A Practical Guide. Disponível em: <https://www.datacamp.com/tutorial/k-means-clustering-python>.

**IBM**. K-means Clustering. Disponível em: <https://www.ibm.com/think/topics/k-means-clustering>.

**OLIVEIRA, Anderson F.**. *Agrupamento de dados utilizando algoritmo K-Modes*. 2019. Dissertação (Mestrado em Ciência da Computação) – Faculdade de Ciências, Universidade Estadual Paulista, São José do Rio Preto, 2019. Disponível em: <https://www.cc.faccamp.br/Dissertacoes/AndersonFranciscoOliveira.pdf>.

**SV**, Misha. Calinski‑Harabasz Index for K‑Means Clustering Evaluation using Python. Towards Data Science, Medium, 2021. Disponível em: <https://towardsdatascience.com/calinski-harabasz-index-for-k-means-clustering-evaluation-using-python-4fefeeb2988e/>.

## Contribuidores
| <img loading="lazy" src="https://avatars.githubusercontent.com/u/207051125?s=400&u=985341a59692bda296ac3e384872f8d5d92fb51a&v=4" width=115><br><sub>Arthur Bogoni (https://github.com/ArthurBogoni) |
| :---: | 
