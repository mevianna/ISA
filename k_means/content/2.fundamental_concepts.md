## English version

## Portuguese version

Para entender como funciona o K-Means, é necessário compreender alguns conceitos fundamentais, que serão abordados ao longo desta pesquisa. A seguir, são apresentados concepções essenciais para facilitar a compreensão do algoritmo e de seu funcionamento.

#### Dados Não Rotulados
O K-means opera em conjuntos de dados onde não há informações pré-existentes sobre a qual "classe" ou "grupo" cada ponto pertence. O objetivo do algoritmo é descobrir essa estrutura inerente nos dados.

#### Pontos de Dados (Data Points)
Cada observação individual no seu conjunto de dados. Pode ser um único número (em uma dimensão), um par de números (em duas dimensões, como em um gráfico x-y), ou uma sequência de números (em múltiplas dimensões, representando diferentes características).

#### Clusters
Os grupos que o algoritmo K-means tenta identificar nos dados. O objetivo é que os pontos dentro do mesmo cluster sejam semelhantes entre si e diferentes dos pontos em outros clusters.

#### Centróides
São os pontos centrais de cada cluster. Inicialmente, eles podem ser escolhidos aleatoriamente ou por algum método heurístico. Durante o processo do K-means, os centróides são recalculados iterativamente como a média de todos os pontos atribuídos àquele cluster.

#### Atribuição de Pontos a Clusters
Cada ponto de dados é atribuído ao cluster cujo centróide está mais próximo dele, com base em alguma medida de distância.

***

## Cálculo da distância
O cálculo da distância entre os pontos de dados e os centróides é um passo crucial para o K-means, pois determina a qual cluster cada ponto será atribuído. Logo, a escolha da métrica de distância influencia diretamente a forma e a estrutura dos clusters resultantes.

### Métrica de Distância Euclidiana
A distância euclidiana é a forma mais comum de medir a distância entre dois pontos. Para calcular a distância euclidiana entre dois pontos, $p$ e $q$, num espaço $n$-dimensional, onde $p$ é definido como $(p_1, p_2, ..., p_n)$ e $q$ como $(q_1, q_2, ..., q_n)$, usamos a seguinte fórmula:

$$d(p, q) = \sqrt{\sum_{i=1}^{n} (q_i - p_i)^2}$$

Nesta fórmula, o símbolo $\sum$ indica o somatório, $n$ é o número de dimensões do espaço e $p_i$ e $q_i$ são as coordenadas dos pontos $p$ e $q$, respectivamente. O cálculo envolve os seguintes passos: primeiro, calcula-se a diferença entre as coordenadas dos dois pontos em cada dimensão; depois, eleva-se ao quadrado cada uma dessas diferenças; em seguida, somam-se todos os valores resultantes; e, finalmente, calcula-se a raiz quadrada desta soma, obtendo-se a distância euclidiana entre os dois pontos.

**Exemplo:**

$$\sqrt{(4 - 1)^2 + (6 - 2)^2} = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = \sqrt{25} = 5$$

<div align="center">
  <img src="https://media.datacamp.com/cms/google/ad_4nxfw85hlpdhvogfbjrxi9jiktdv2memq-leatixmm_u0_jevvnifvpnhfza7b7xxbd3uw_ilm-rqvtf-xe9hxnks01or84mh4hfuy7ejn2dc6j1jzu691cl0f8dfqgdgjogzkp45p0eekkex8rrrxzzbxtol.png" alt="gráfico de distância euclidiana 2d" width="800">
</div>

***

## Métricas de validação
Como o algoritmo K-Means não utiliza rótulos verdadeiros durante o treinamento, é preciso avaliar a qualidade dos agrupamentos formados por meio de métricas de validação. Essas métricas verificam se os agrupamentos gerados estão compactos, bem separados entre si e coerentes com os dados.
Existem dois tipos de métricas: internas e externas. As métricas internas avaliam o quão bem agrupado os dados estão baseado apenas nos próprios dados e nos resultados obtidos. Já as métricas externas comparam os agrupamentos resultantes com os verdadeiros, quando existem, permitindo verificar a semelhança dos resultados com a classificação esperada.

### Inércia (Within-Cluster Sum of Squares - WCSS)
A Inércia é uma métrica interna fundamental que quantifica a soma das distâncias quadráticas de cada ponto dentro de um cluster até o seu respectivo centróide. O principal objetivo do algoritmo é minimizar essa inércia; portanto, essa métrica mede quão bem um conjunto de dados foi agrupado com base em métricas de distância. Sendo assim, a inércia é calculada medindo a distância entre um ponto de dado e seu centróide, elevando essa distância ao quadrado e somando esses quadrados para cada ponto do agrupamento, conforme a fórmula abaixo:

\[
\text{Inércia} = \sum_{x \in C} \| x - \mu \|^2 = \| x_1 - \mu \|^2 + \| x_2 - \mu \|^2 + \cdots + \| x_n - \mu \|^2
\]

onde:  
- \(C = \{ x_1, x_2, \ldots, x_n \}\) é o conjunto de \(n\) pontos do cluster,  
- \(x_i\) representa cada ponto dentro do cluster \(C\),  
- \(\mu\) é o centróide do cluster,
- \(\|\cdot\|\) representa a distância Euclidiana.  

O somatório termina após somar a distância quadrática de todos os \(n\) pontos do cluster ao centróide. Essa soma é a representação da distância intra-agrupamento, ou seja, quanto menor a soma, melhor, pois isso significa que os pontos dentro dos agrupamentos são mais compactos ou semelhantes. Para o algoritmo como um todo, a inércia total é obtida a partir da soma da inércia de cada cluster, conforme a fórmula abaixo.

\[
\text{Inércia} = \sum_{i=1}^{k} \sum_{x \in C_i} \| x - \mu_i \|^2
\]

onde:  
- \(k\) é o número de clusters,  
- \(C_i\) é o conjunto de pontos pertencentes ao cluster \(i\),  
- \(x\) é um ponto dentro do cluster \(C_i\),  
- \(\mu_i\) é o centróide do cluster \(i\),  
- \(\|\cdot\|\) representa a distância Euclidiana.

Valores menores de inércia representam agrupamentos mais compactos. Entretanto, é importante notar que a inércia sempre diminui com o aumento do número de clusters e assume que os clusters são convexos e isotrópicos - o que pode não refletir a estrutura real dos dados.
Assim, a inércia normalmente é utilizada para determinar o número ideal de clusters, conforme apresentado em . Para determinar a qualidade dos agrupamentos, é indicado utilizar a inércia acompanhada de outros métodos.

### Índice de Dunn
O índice de Dunn, também uma métrica interna, representa a relação entre a menor distância entre clusters e a maior distância dentro de um cluster. Clusters com uma alta distância entre eles indicam melhor qualidade, pois isso significa que os clusters são o mais diferentes possível uns dos outros.

### Índice Davies & Bouldin
O índice de validação Davies-Bouldin (índice DB), proposto em 1979, é uma métrica de validação interna que avalia o quão similares os clusters são internamente e o quão diferentes eles são entre si. Esse índice varia de 0 ao infinito, mas, diferentemente do índice de Dunn, quanto mais próximo de 0 o valor, melhor o agrupamento. 
Assim, esse índice tem resultados melhores em agrupamentos compactos e separados entre si, ou seja, valores baixos do índice DB representam grupos bem ajustados e distantes entre si. 

### Índice Calinski-Harabasz
O índice Calinski-Harabasz, também conhecido como Critério de Razão de Variância, é outro método interno para calcular a qualidade de agrupamentos. Ele é calculado como a razão entre a soma da dispersão entre os clusters e a soma da dispersão interna dos clusters, considerando todos os agrupamentos formados.
A dispersão é calculada através da soma ao quadrado das distâncias, ou seja, pelo método dos minímos quadrados, tanto entre os clusters quanto dentro de cada cluster.
Dessa maneira, quanto maior o valor do índice, melhor os agrupamentos, pois  significa que os pontos de cada clusters estão próximos entre si (densidade alta) e cada cluster está distante um dos outros.

### Índice de Rand
O Índice de Rand, proposto em 1971 por William M. Rand, é uma métrica externa utilizada para avaliar a similaridade entre dois agrupamentos distintos. Esse índice mede o quão semelhantes os agrupamentos obtidos pelo algoritmo são do resultado real, quando este é conhecido.
Esse índice é calculado com base na razão entre a soma do número de pares de elementos que estão no mesmo agrupamento em ambos os casos e do número de pares de elementos que estão em agrupamentos diferentes em ambos os casos, e o número possível de agrupamentos.
Os resultados variam entre 0 e 1, sendo que 1 indica um agrupamento perfeito e 0, totalmente diferentes.

> [!IMPORTANT]
> O índice de Rand não leva em consideração agrupamentos gerados aleatoriamente, o que pode gerar a falsa impressão que os agrupamentos estão coerentes. Esse problema ocorre principalmente em grandes conjuntos de dados, pois a maioria dos pares de dados fica em clusters diferentes, o que aumenta o valor do índice. Assim, existe o Índice de Rand Ajustado, que leva em consideração a possibilidade de agrupamentos semelhantes ocorrerem por acaso. Nessa versão, é permitido resultados negativos, o que significa que o agrupamento está pior que o acaso.

***

## Referências
**DATACAMP**. Euclidean Distance: A Complete Guide. Disponível em: <https://www.datacamp.com/pt/tutorial/euclidean-distance>.
**DATACAMP**. K-Means Clustering in Python: A Practical Guide. Disponível em: <https://www.datacamp.com/tutorial/k-means-clustering-python>.
**IBM**. K-means Clustering. Disponível em: <https://www.ibm.com/think/topics/k-means-clustering>.
**OLIVEIRA, Anderson F.**. *Agrupamento de dados utilizando algoritmo K-Modes*. 2019. Dissertação (Mestrado em Ciência da Computação) – Faculdade de Ciências, Universidade Estadual Paulista, São José do Rio Preto, 2019. Disponível em: <https://www.cc.faccamp.br/Dissertacoes/AndersonFranciscoOliveira.pdf>.
**SV**, Misha. Calinski‑Harabasz Index for K‑Means Clustering Evaluation using Python. Towards Data Science, Medium, 2021. Disponível em: <https://towardsdatascience.com/calinski-harabasz-index-for-k-means-clustering-evaluation-using-python-4fefeeb2988e/>.