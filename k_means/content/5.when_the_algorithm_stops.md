## English version

Now that we understand how to define the number of centroids and clusters—essential elements for the algorithm’s operation—a natural question arises: when does the algorithm stop? In other words, what is the stopping criterion for the algorithm? Generally, the algorithm terminates under two main conditions: when convergence is reached or when a predefined maximum number of iterations is completed. These two situations are explained below.

### Convergence

Convergence in K-Means means that the algorithm has reached a stable state where the variation of the centroids is so small that it can be considered negligible. This state indicates that the algorithm has efficiently grouped the data into clusters, and further iterations will not bring significant changes since the results will be practically the same.

However, some factors can influence or hinder the convergence of K-Means. For example, poor initialization of centroids can:

* Increase the time it takes for the algorithm to stabilize;
* Lead to a suboptimal division of the data, that is, reaching a local minimum—a stable configuration that is not necessarily the best possible Thus, although the result may not be the "global optimum" (the best possible solution), it is reasonable but inferior to other potential solutions.

> An example of the second case is imagining a dataset clearly divided into two groups, A and B. However, a poor initialization might select two centroids belonging to group A. In this scenario, the algorithm tends to split group A into two clusters, while group B is ignored or incorrectly clustered. Therefore, even if convergence is achieved, the cluster division is imperfect.

Another issue related to convergence is the presence of outliers (points that do not follow the pattern). These points can cause the centroids to shift during iterations, resulting in potentially low-quality solutions and increasing the convergence time.

> [!NOTE]
> To understand the most common errors, such as the presence of outliers, visit: [7. Possible Problems](https://github.com/mevianna/ISA/tree/k_means/k_means/content/7.possible_problems.md)

Additionally, variable cluster density can also be an issue: centroids tend to adjust more quickly to denser groups, while they take longer to adapt to less dense groups. This can cause the algorithm to assign points belonging to less dense clusters into denser ones, or even merge one cluster into another.

Finally, the non-spherical shape of clusters can also hinder performance. Since K-Means tends to assume clusters are spherical, it may assign points to the nearest centroid even if, in the ideal solution, those points belong to clusters with irregular or overlapping shapes. Consequently, the quality of clustering may be compromised.

### Iteration Limit
In some cases, the algorithm may take a long time to converge or even enter an infinite loop of iterations due to outliers, complex datasets, or a large number of clusters. To prevent excessively long processing times, a maximum number of iterations is set. When this limit is reached, the algorithm stops, even if ideal convergence has not been achieved.

Among the advantages of this method are controlling execution time, avoiding excessive computational resource consumption, and preventing infinite cycles. However, an inappropriate choice of this limit can cause problems:

* Too low a limit: the algorithm may stop before reaching an adequate solution;
* Too high a limit: computational cost may outweigh the quality gained, making the effort unjustifiable.

Therefore, selecting this limit requires evaluating the size and complexity of the dataset, the desired quality level, and available computational resources, since each situation needs a specific assessment. Generally, commonly used limits range between 100 and 300 iterations. It is worth noting that with a high iteration limit, the algorithm may stop earlier if convergence is reached beforehand.

### Other Stopping Criteria
In certain cases, other stopping criteria may be defined. For example, a maximum execution time can be set, forcing K-Means to terminate once reached. In some versions of K-Means, errors such as empty clusters (i.e., no points assigned to a centroid) also cause the algorithm to terminate.

## References:
**BISHOP, C. M.**. Pattern Recognition and Machine Learning. New York: Springer, 2006. Available at: <https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>. Accessed: May 30, 2025.
**ESTADÍSTICAS FÁCILES**. What does k convergence mean? Available at: <https://es.statisticseasily.com/glosario/¿Qué-significa-k-convergencia%3F>. Accessed: May 30, 2025.
**IBM.** K-means clustering. Available at: <https://www.ibm.com/think/topics/k-means-clustering>. Accessed: May 14, 2025.
**SCIKIT-LEARN**. sklearn.cluster.KMeans. Available at: <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html>. Accessed: May 14, 2025.

## Portuguese version

Agora que já sabemos como definir o número de centróides e clusters, elementos essenciais para o funcionamento do algoritmo, pode surgir a seguinte dúvida: até quando o algortimo é executado? Em outras palavras, **qual o critério de parada do algoritmo?** Geralmente, o algoritmo finaliza em duas situações principais: quando é atingido ou a convergência ou o número de iterações previamente estipulado. Estas duas situações são abordadas em seguida.

### Convergência

A convergência no K-Means significa que o algoritmo alcançou um estado de estabilidade em que a variação dos centróides é tão pequena que pode ser considerada insignificante. Esse estado indica que o algoritmo agrupou eficientemente os dados nos clusters e as iterações seguintes não trarão mudanças relevantes, pois os resultados serão praticamente os mesmos.

Entretanto, alguns fatores podem influenciar ou dificultar a convergência do K-Means. Por exemplo, uma má inicialização dos centróides pode:

* Aumentar o tempo de estabilização do algoritmo;
* Levar a uma divisão não tão boa dos dados, isto é, obter um minímo local, que é uma configuração estável mas não necessariamente é a melhor possível. Assim, o resultado embora não seja o "ótimo global" (melhor solução possível), ele é razoável mas inferior a outras possíveis soluções.

> Um exemplo desse segundo caso é imaginar um conjunto de dados bem definidos em 2 grupos, A e B. Entretanto, uma inicialização inadequada escolhe dois centróides pertecentes ao grupo A. Dessa forma, o algorimo tende a dividir o grupo A em dois grupos, enquanto que o grupo B é ignorado ou agrupado incorretamente. Portanto, mesmo que haja convergência, a divisão dos clusters é imperfeita.

Outro problema em relação a convergência é a existência dos outliers (pontos fora do padrão). Esses pontos podem causar deslocamentos nos centróides no decorrer das interações, resultando em possíveis soluções de baixa qualidade e aumentando o tempo de convergência.

> [!NOTE]
> Para entender possíveis erros mais comuns, como a presença de outliers, acesse: [7. Possiveis problemas](https://github.com/mevianna/ISA/tree/k_means/k_means/content/7.possible_problems.md)

Além disso, a densidade variável dos grupos também pode ser um problema: os centróides tendem a se ajustar mais rapidamente aos grupos mais densos, enquanto demora mais em grupos menos densos. Isso pode levar o algoritmo a agrupar pontos pertencentes aos grupos menos densos em grupos mais densos, ou até mesmo englobando um grupo no outro.

Por fim, a forma não esférica dos grupos também pode atrapalhar. Como o K-Means tende a considerar o conjunto de dados como esféricos, ele pode atribuir pontos ao centróide mais próximo, mesmo que, na solução ideal, esses pontos pertençam a clusters de formatos irregulares ou sobrepostos. Assim, a qualidade do agrupamento pode ser prejudicada.

### Limite de iterações

Em alguns casos, o algoritmo pode demorar muito para convergir ou até mesmo entrar em um ciclo infinito de iterações, devido à presença de outliers, ao conjunto complexo de dados ou à presença de muitos clusters. Para evitar que o processo seja prolongado excessivamente, é estipulado um limite de iterações. Quando esse limite é atingido, o algoritmo para, mesmo que a convergência ideal não tenha sido atingida.

Entre as vantagens desse método, há o controle do tempo de execução, evitando o consumo excessivo de recursos computacionais e ciclos infinitos. No entanto, a escolha inadequada de um valor para o limite pode ocasionar problemas:

* Limite muito baixo: algoritmo pode ser finalizado antes de uma solução adequada;
* Limite muito alto: o custo computacional pode ser superior a qualidade atingida, o que pode não justificar o esforço.

Portanto, para a escolha do valor desse limite, é preciso avaliar o tamanho, a complexidade do conjunto de dados, o nível de qualidade desejado e até mesmo os recursos computacionais disponíveis, pois cada situação necessita de uma avaliação específica. Em geral, os valores mais utilizados como limites giram em torno de 100 e 300 iterações. Cabe ressaltar que, em casos de um número muito alto de execuções, é possível que o algoritmo encerre antes, caso a convergência seja atingida previamente.

### Outros critérios de parada

Em determinados casos, outros critérios de parada podem ser determinados. Por exemplo, pode ser definido um tempo máximo de execução do algoritmo que, quando atingido, força a finalização do K-Means. Em algumas versões do K-Means, em casos de erro, como a formação de clusters vazios (isto é, nenhum ponto atribuído a um centróide), o algortimo também é finalizado.

### Referências:
**BISHOP, C. M.**. Pattern Recognition and Machine Learning. New York: Springer, 2006. Disponível em: <https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>. Acesso em: 30 mai. 2025.
**ESTADÍSTICAS FÁCILES**. ¿Qué significa k convergencia? Disponível em: <https://es.statisticseasily.com/glosario/¿Qué-significa-k-convergencia%3F>. Acesso em: 30 maio 2025.
**IBM**. K-means clustering. Disponível em: <https://www.ibm.com/think/topics/k-means-clustering>. Acesso em: 14 maio 2025.
**SCIKIT-LEARN**. sklearn.cluster.KMeans. Disponível em: <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html>. Acesso em: 14 maio 2025.