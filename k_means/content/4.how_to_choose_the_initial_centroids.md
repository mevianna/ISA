## English Version

## How to Choose the Initial Centroids?

The choice of initial centroids in the K-means algorithm is a fundamental step that can strongly impact the final result. A poor choice may lead to local minima or slow convergence; therefore, there are methods for centroid initialization.

## Random Data Points

In this method, $k$ random data points are selected from the dataset and used as the initial centroids. Although each data instance in the dataset must be enumerated and the minimum/maximum value of each attribute recorded, this approach is considered very volatile, as it may result in centroids that are not well positioned.

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/k-means-clustering-2-1.jpg?resize=1020%2C534&ssl=1" alt="k-means-clustering-graph" width="800">
  <p align="center"><b>Data points defined at random locations and used as cluster centroids</b></p>
</div>

The graph above illustrates that the centroid represents the center of a cluster. Initially, the center of these data points is unknown. Therefore, we select random data points and set them as the centroids for each cluster. In this example, there are 3 centroids in the dataset.

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/k-means-clustering-3.jpg?resize=1020%2C534&ssl=1" alt="k-means-clustering-graph" width="800">
  <p align="center"><b>Assigning a data point to the nearest cluster centroid.</b></p>
</div>

With the centroids initialized, the data points $X_n$ are assigned to their nearest cluster centroid $C$. At this stage, it is necessary to calculate the distance between a data point $X$ and the centroid $C$.

$$d(p, q) = \sqrt{\sum_{i=1}^{n} (q_i - p_i)^2}$$

Subsequently, each data point is assigned to the cluster whose centroid has the smallest distance.

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/k-means-clustering-4.jpg?resize=1020%2C534&ssl=1" alt="k-means-clustering-graph" width="800">
</div>

Then, the centroids are recalculated as the mean of all data points within each cluster, according to the formula:

$$C_i = \frac{1}{|N_i|} \sum x_i$$

Where:
* **$C_i$**:is the centroid of group $i$, i.e., the "center" of the cluster – an average vector representing all points belonging to that group.
* **$N_i$**: is the set of points assigned to group $i$.
* **$\sum x_i$**: is the sum of all vectors $x_i$ that belong to group $N_i$.

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/k-means-clustering-5.jpg?resize=1020%2C534&ssl=1" width="800">
</div>

This process is repeated until the optimal number of centroids is reached and the assignment of data points to clusters is stable (i.e., there are no longer significant changes in assignments or centroids).

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/k-means-clustering-6.jpg?resize=1020%2C534&ssl=1" width="800">
  <p align="center"><b>Final assignment cycle completed.</b></p>
</div>

## Naive Partitioning
Naive partitioning is a simple strategy for dividing data into groups (clusters) without considering the actual structure or distribution of the data. Since it does not account for density, distance between points, or variability, it is mainly used in contexts such as: centroid initialization before applying K-Means++, data splitting for testing, and artificial clustering in simulations.

Thus, the idea is to calculate the sum of all attribute values for each instance (row) in a dataset, and use this composite value to order the instances.

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/K-Means-Clustering_12.png?ssl=1" width="800">
</div>

**Step 1:**
Sum the attributes of each instance and add the result as a new column at the beginning of the dataset.

**Step 2:**
Sort the dataset instances by the new sum column in ascending order.

**Step 3:**
Split the dataset horizontally into $k$ equal-sized parts, or fragments.

Finally, the attributes of each fragment are summed and their averages calculated. The average values of the attributes of each fragment are then used as the initial centroids.

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/K-Means-Clustering_3.png?ssl=1" width="800">
</div>

**Step 4:**
For each fragment, calculate the mean of the attribute columns; the mean values become the corresponding values of the new centroid.

Since this occurs in linear time — meaning the execution time grows proportionally with the amount of input data — it is much better than random initialization. However, it still presents the same issue of possibly producing inconsistent results.

## K-Means++

K-Means++ is a variation of the K-means algorithm that optimizes the selection of initial centroids, improving the quality of the final cluster assignment.

The first step of this initialization is to randomly select one centroid from the dataset. For the next centroids, the distance from each data point to the nearest already chosen centroid is computed. Then, the next centroid is chosen based on a probability proportional to this distance — the farther a point is from existing centroids, the higher the chance it will be chosen.

Although similar to random selection, K-Means++ is “random with intelligence,” since it uses distances between points to guide randomness.
**Steps:**
**Step 1:** Randomly choose the first centroid ($C_1$).
**Step 2:** Compute the distance between all data points and the chosen centroid.

$$D_i = \max_{(j:1 \rightarrow k)} \|x_i - c_j\|^2$$

- $D_i$: the maximum squared Euclidean distance between data point $x_i$ and any of the already chosen centroids $c_j$.
- $x_i$: a point from the dataset.
- $c_j$: a centroid already chosen.
- $\|x_i - c_j\|^2$: squared Euclidean distance between $x_i$ and $c_j$.
- max: indicates the maximum value among a set of values.
- $(j:1 \rightarrow k)$: refers to iteration over the $k$ centroids already selected. In K-Means++, $j$ represents a chosen centroid, and $k$ is the number of centroids initialized so far.

**Step 3:** Initialize data point $x$ as a new centroid.
**Step 4:** Repeat steps 2 and 3 until all centroids are selected.


<div align="center">
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20190812011808/Screenshot-2019-08-12-at-1.13.15-AM.png" width="800">
  <p align="center"><b>Poor clustering.</b></p>
</div>

<div align="center">
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20190812011831/Screenshot-2019-08-12-at-1.09.42-AM.png" width="800">
  <p align="center"><b>Good clustering.</b></p>
</div>

***

## References
**DATACAMP**. K-Means Clustering in Python: A Practical Guide. Available at: <https://www.datacamp.com/tutorial/k-means-clustering-python>.
**GEEKSFORGEEKS**. K-Means Algorithm — A Simple Explanation. Available at:<https://www.geeksforgeeks.org/ml-k-means-algorithm/>.
**KDNUGGETS**. Centroid Initialization in K-Means Clustering. Available at: <https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html>.
**NEPTUNE.AI.**. K-Means Clustering — An Introduction. Available at: <https://neptune.ai/blog/k-means-clustering>.

***

## Contributors
| <img loading="lazy" src="https://avatars.githubusercontent.com/u/207051125?s=400&u=985341a59692bda296ac3e384872f8d5d92fb51a&v=4" width=115><br><sub>Arthur Bogoni (https://github.com/ArthurBogoni) |
| :---: | 

***

## Portuguese Version

## Como Escolher os centroides Iniciais?

A escolha dos **centroides iniciais** no algoritmo K-means é um passo fundamental que pode impactar fortemente o resultado final. Uma má escolha pode levar a mínimos locais ou lentidão na convergência, por isso, existem métodos para realizar a inicialização de centroides.

### Pontos de Dados Aleatórios

Nesse método, $k$ **pontos de dados aleatórios** são selecionados do conjunto de dados e usados como centroides iniciais. Embora cada instância de dados no conjunto deva ser enumerada e registrar o valor mínimo/máximo de cada atributo, essa abordagem é considerada muito volátil, pois pode levar a um cenário em que os centroides selecionados não estão bem posicionados.

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/k-means-clustering-2-1.jpg?resize=1020%2C534&ssl=1" alt="k-means-clustering-graph" width="800">
  <p align="center"><b>Pontos de dados definidos em locais aleatórios e definidos como centroides dos clusters</b></p>
</div>

O gráfico acima ilustra que o centroide representa o centro de um cluster. Inicialmente, o centro desses pontos de dados é desconhecido. Portanto, selecionamos pontos de dados aleatórios e os definimos como centroides para cada cluster. Neste exemplo, há 3 centroides no conjunto de dados.

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/k-means-clustering-3.jpg?resize=1020%2C534&ssl=1" alt="k-means-clustering-graph" width="800">
  <p align="center"><b>Atribuição de ponto de dado ao centroide de cluster mais próximo.</b></p>
</div>

Com os centroides inicializados, os pontos de dados $X_n$ são atribuídos ao seu centroide de cluster mais próximo $C$. Ademais, nesta etapa, torna-se necessário calcular a distância entre o ponto de dado $X$ e o centroide $C$.

$$d(p, q) = \sqrt{\sum_{i=1}^{n} (q_i - p_i)^2}$$

Posteriormente, cada ponto de dado é atribuído ao cluster cujo centroide apresente a menor distância.

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/k-means-clustering-4.jpg?resize=1020%2C534&ssl=1" alt="k-means-clustering-graph" width="800">
</div>

Em seguida, os centroides são recalculados pela média de todos os pontos de dados de cada *cluster*, conforme a fórmula:

$$C_i = \frac{1}{|N_i|} \sum x_i$$

Onde:

* **$C_i$**: é o centroide do grupo $i$, ou seja, o "centro" do agrupamento - um vetor médio que representa todos os pontos pertencentes a esse grupo.
* **$N_i$**: é o conjunto de pontos atribuídos ao grupo $i$.
* **$\sum x_i$**: é a soma de todos os vetores $x_i$, que pertencem ao grupo $N_i$.

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/k-means-clustering-5.jpg?resize=1020%2C534&ssl=1" width="800">
</div>

Este processo é repetido até que o número de centroides ideais seja alcançado e as atribuições dos pontos de dados aos *clusters* estejam corretas (ou seja, não haja mais mudanças significativas nas atribuições ou nos centroides).

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2024/04/k-means-clustering-6.jpg?resize=1020%2C534&ssl=1" width="800">
  <p align="center"><b>Ciclo de atribuição finalizado.</b></p>
</div>

## Fragmentação Ingênua

A fragmentação ingênua é uma estratégia simples para dividir dados em grupos (*clusters*) sem considerar a estrutura ou distribuição real dos dados. Por essa razão de não considerar a densidade, distância entre pontos ou variabilidade, é utilizada em contextos como: inicialização de centroides antes de aplicar o K-Means++, divisão de dados para teste e agrupamentos artificiais em simulações. Assim, utiliza-se o valor da soma composta de todos os atributos para uma instância ou linha específica em um conjunto de dados, ou seja, a ideia é calcular o valor composto e usá-lo para ordenar as instâncias dos dados.

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/K-Means-Clustering_12.png?ssl=1" width="800">
</div>

**Passo 1:**
Some os atributos de cada instância e adicione a coluna de resultado no início do conjunto de dados.

**Passo 2:**
Ordene as instâncias do conjunto de dados pela nova coluna de soma, em ordem crescente.

**Passo 3:**
Divida o conjunto de dados horizontalmente em $k$ partes de tamanho igual, ou fragmentos.

Finalizando, os atributos de cada fragmento são somados e suas médias calculadas. Os valores médios dos atributos de cada fragmento serão identificados como o conjunto de centroides que pode ser usado para inicialização.

<div align="center">
<img src="https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/K-Means-Clustering_3.png?ssl=1" width="800">
</div>

**Passo 4:**
Para cada fragmento, calcule a média das colunas de atributos; os valores médios se tornam os valores correspondentes dos atributos do novo centroide.

Por ocorrer em tempo linear, ou seja, o tempo que o algoritmo leva para ser executado cresce proporcionalmente à quantidade de dados de entrada, sendo, portanto, muito melhor que o da inicialização aleatória. Entretanto, ela apresenta os mesmos problemas de ter a chance de apresentar resultados inconsistentes.

## K-Means++

O K-Means++ é uma variação do algoritmo K-means que otimiza a escolha dos centroides iniciais dos grupos, além de melhorar a qualidade da atribuição final dos agrupamentos.

O primeiro passo dessa inicialização é escolher de forma aleatória um centroide a partir do conjunto de dados. Para os próximos centroides, calcula-se a distância de cada ponto de dado até o centroide mais próximo já escolhido. Com isso, o próximo é escolhido a partir da probabilidade proporcional à distância de um ponto até o centroide mais próximo, logo quanto maior essa distância, maior a chance de o ponto ser escolhido.
Apesar de sua semelhança com o método da escolha aleatória, o K-Means++ é uma escolha aleatória com inteligência, por utilizar a distância entre os pontos para guiar essa aleatoriedade.

**Passos:**
**Passo 1:** Escolha aleatoriamente o primeiro centroide ($C_1$).
**Passo 2:** Calcule a distância entre todos os pontos de dados e o centroide selecionado.

$$D_i = \max_{(j:1 \rightarrow k)} \|x_i - c_j\|^2$$

- $D_i$: é a maior distância euclidiana ao quadrado entre o ponto de dado $x_i$ e qualquer um dos centroides $c_j$ já escolhidos.
- $x_i$: o ponto do conjunto de dados.
- $c_j$: um centroide já escolhido.
- $\|x_i - c_j\|^2$: a distância euclidiana ao quadrado entre $x_i$ e $c_j$.
- max: indica o valor máximo entre um conjunto de valores.
- $(j:1 \rightarrow k)$: Refere-se à iteração sobre os $k$ centroides já selecionados. Em um algoritmo de agrupamento como o K-Means++, $j$ representa um centroide já escolhido, e $k$ é o número de centroides que já foram inicializados até o momento.

**Passo 3:** Inicialize o ponto de dado $x$ como novo centroide.
**Passo 4:** Repita os passos 2 e 3 até que todos os clusters sejam encontrados.

<div align="center">
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20190812011808/Screenshot-2019-08-12-at-1.13.15-AM.png" width="800">
  <p align="center"><b>Agrupamento ruim.</b></p>
</div>

<div align="center">
<img src="https://media.geeksforgeeks.org/wp-content/uploads/20190812011831/Screenshot-2019-08-12-at-1.09.42-AM.png" width="800">
  <p align="center"><b>Agrupamento adequado.</b></p>
</div>

***

### Referências:
**DATACAMP**. K-Means Clustering in Python: A Practical Guide. Disponível em: <https://www.datacamp.com/tutorial/k-means-clustering-python>.
**GEEKSFORGEEKS**. K-Means Algorithm — A Simple Explanation. Disponível em: <https://www.geeksforgeeks.org/ml-k-means-algorithm/>.
**KDNUGGETS**. Centroid Initialization in K-Means Clustering. Disponível em: <https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html>.
**NEPTUNE.AI.**. K-Means Clustering — An Introduction. Disponível em: <https://neptune.ai/blog/k-means-clustering>.

## Contribuidores
| <img loading="lazy" src="https://avatars.githubusercontent.com/u/207051125?s=400&u=985341a59692bda296ac3e384872f8d5d92fb51a&v=4" width=115><br><sub>Arthur Bogoni (https://github.com/ArthurBogoni) |
| :---: | 
