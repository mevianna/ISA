# English Version

The K-Means algorithm offers interesting advantages, such as:

* **Simplicity:** It is an easy algorithm to understand and implement, even without prior knowledge of machine learning.

* **Computational efficiency:** Compared to other algorithms, K-Means stands out for its efficiency. Its complexity depends on the multiplication of the number of data points, the number of clusters, the number of iterations until convergence, and the number of dimensions. Since the number of clusters and iterations is generally significantly smaller than the total number of points, the algorithm becomes balanced, meaning it can process a large amount of data in a short time.

* **Scalability:** Similar to the previous point, K-Means tends to handle the addition of new data well. The algorithm generalizes clusters into various sizes, which is ideal for analyzing complex datasets. Thus, K-Means is more suitable for working with large datasets compared to other models in terms of scalability.

However, K-Means also presents disadvantages that must be considered before its implementation, such as:

### Need for initialization:
One of the main problems with K-Means is the need to define the initial centroids and the number of clusters. As seen previously, poor initialization of these parameters can lead to suboptimal results.

### Assumption of spherical and similarly sized clusters:
K-Means assumes that clusters have approximately spherical shapes, as it considers the shortest Euclidean distance between centroids and points for assignment. While this characteristic can be considered an advantage, given its generalization capability and simplicity, it also represents a limitation: if the data is presented in non-spherical shapes or in distributions of different sizes, the algorithm may have difficulty grouping them correctly.

### Sensitivity to outliers and noise:
Since centroid updates depend on the average positions of the points belonging to each cluster, the presence of outliers and noise can significantly distort the result. For example, if an outlier is assigned to a specific cluster, it can unduly shift the centroid, as its distant position affects the calculation of the average positions for centroid update. Thus, the algorithm's result can be considerably affected.

### Curse of dimensionality:
K-Means relies on the calculation of Euclidean distance to measure similarity between points, which can become a challenge when data has many dimensions (parameters). As the number of dimensions increases, the difference between distances tends to smooth out, making it difficult to assign points.

> For example, in a dataset with 6 parameters (dimensions). For 3 dimensions, a certain point is closer to centroid A, while for another 3 dimensions, it is closer to centroid B. Which centroid is the point closest to? To solve this, one strategy is to standardize the data so that it has fewer dimensions.

### Does not handle categorical data well:
K-Means is designed to work with numerical and continuous data, where the concept of distance makes sense. When using categorical data - data that does not represent numbers, such as colors and fruits - there is no real distance between them. For example, how do you measure the distance between "green" and "red"? How far are they from "blue"?

One way to deal with this problem is to transform categorical data into numerical data, such as:
* Blue = 1;
* Red = 2;
* Green = 3;
However, in most cases, the distance would still not make sense. In the given example, the algorithm would assume that the distance between "blue" and "red" is shorter than between "blue" and "green". But how can we ensure that this is correct?

Another alternative is to transform categorical data into binary columns. For example:
* Blue = [1, 0, 0]
* Red = [0, 1, 0]
* Green = [0, 0, 1]
In this way, the data becomes numerical. However, there can still be problems in calculating the Euclidean distance, because the more categories (in the case of the example, the more colors) there are, the number of dimensions will be increasingly larger, falling back into the curse of dimensionality. Thus, for categorical data, the best strategy might be to use specific K-Means adaptations for these types of data.

> K-Modes is an example of a K-Means variation for categorical data. Instead of using Euclidean distance, the algorithm counts how many attributes are different between the data points and represents each group with the most common value (modes) for each characteristic. For example, in a clustering of people, with the attributes "Favorite Color" and "Favorite Animal", if most people prefer the color blue and the animal dog, these values form one cluster, just as the least preferred color and animal form another cluster. When new data is analyzed, the algorithm will see how similar it is to each cluster to assign it to the most similar one.

## References:
**BISHOP, C. M.**. Pattern Recognition and Machine Learning. New York: Springer, 2006. Disponível em: <https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>. Acesso em: 30 mai. 2025.
**GEEKSFORGEEKS**. K-Mode Clustering in Python. Disponível em: <https://www.geeksforgeeks.org/k-mode-clustering-in-python/>. Acesso em: 5 jun. 2025.
**IBM**. K-means clustering. Disponível em: <https://www.ibm.com/think/topics/k-means-clustering>. Acesso em: 14 maio 2025.
**OLIVEIRA, Anderson F.**. *Agrupamento de dados utilizando algoritmo K-Modes*. 2019. Dissertação (Mestrado em Ciência da Computação) – Faculdade de Ciências, Universidade Estadual Paulista, São José do Rio Preto, 2019. Disponível em: <https://www.cc.faccamp.br/Dissertacoes/AndersonFranciscoOliveira.pdf>. Acesso em: 5 jun. 2025.
**SCIKIT-LEARN**. sklearn.cluster.KMeans. Disponível em: <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html>. Acesso em: 14 maio 2025.

***

# Portuguese Version

O algoritmo K-Means apresenta vantagens interessentes, como:

* **Simplicidade:** é um algoritmo fácil de compreender e implementar, mesmo sem conhecimentos prévios em aprendizagem de máquina;

* **Eficiência computacional:** em comparação com outros algoritmos, o K-Means se destaca quanto a sua eficiência. Sua complexidade depende da  multiplicação do número de pontos, do número de clusters, n\do úmero de iterações até a convergência e do número de dimensões. Como, geralmente, o número de clusters e iterações é siginificativamente menor que o número total de pontos, o algoritmo se torna equilibrado, ou seja, consegue processar uma grande quantidade de dados em um tempo pequeno. 

* **Esacabilidade:** de forma semelhante ao tópico anterior, o K-Means tende a lidar bem com a adição de novos dados. O algoritmo generaliza os clusters em tamanhos variados, o que é ideal para a análise de conjuntos complexos de dados. Assim, o K-Means torna-se mais indicado para trabalhos com grandes conjuntos de dados, em comparação a outros modelos em termos de escabilidade.

Entretanto, o K-Means apresenta desvantagens que devem ser consideradas antes de sua impementação, tais como:

### Necessidade de inicialização:
Um dos principais problemas do K-Means é a necessidade de definir os centróides iniciais e o número de clusters. Como visto anteriormente, a má inicialização desses parâmetros pode levar a resultados subótimos.

### Presunção de clusters esféricos e de tamanho similiar:
O K-Means assume que os clusters possuem formatos aproximadamente esféricos, já que é considerada a menor distância euclidiana entre os centróides e os pontos para realizar a atribuição. Embora essa caracteristíca possa ser considerada uma vantagem, dada a sua capacidade de generalização e simplicidade, ela também representa uma limitação: caso os dados se apresentem em formatos não esféricos ou em distribuições de tamanhos diferentes, o algoritmo pode ter dificuldade em agrupá-los corretamente.

### Sensibilidade a outliers e ruídos:
Como a atualização dos centróides depende da média das posições dos pontos pertencentes a cada cluster, a presença de outliers e ruídos pode distorcer significativamente o resultado. Por exemplo, se um outlier estiver atribuído a determinado cluster, ele pode deslocar o centróide de maneira indevida, uma vez que a sua posição distante afeta o cálculo da média das posições para atualização do centróide. Assim, o resultado do algoritmo pode ser consideravelmente afetado. 

### Maldição da dimensionalidade:
O K-Means depende do cálculo da distância euclidiana para medir similaridade entre os pontos, o que pode se tornar um desafio em casos que os dados possuem muitas dimensões (parâmetros). À medida que o número de dimensões aumenta, a diferença entre as distâncias tende a suavizar, dificultando a atribuição dos pontos. 

> Por exemplo, em um conjunto de dados há 6 paramêtros (dimensões). Por 3 dimensões, um determinado ponto está mais perto do centróide A, enquanto que por outras 3 dimensões, ele está mais perto do centróide B. De qual centróide o ponto está mais próximo? Assim, para resolver isso, uma das estratégias é padronizar os dados, de forma que apresentem menos dimensões. 

### Não lida bem com dados categóricos:
O K-Means é planejado para trabalhar com dados númericos e contínuos, em que o conceito de distância faz sentido. Na utilização de dados categóricos - dados que não representam números, como cores e frutas - não há uma distância real entre eles. Por exemplo, como medir a distância entre "verde" e "vermelho"? O quão longe eles estão de "azul"?

Uma forma que é possível de lidar para resolver esse problema, é transformar os dados categóricos em dados numéricos, como:
* Azul = 1;
* Vermelho = 2;
* Verde = 3;
Porém, na maioria dos casos, a distância ainda não faria sentido. No exemplo dado, o algoritmo assumiria que a distância entre "azul" e "vermelho" é menor do que entre "azul" e "verde". Mas como garantir que isso está certo?

Outra alternativa é transformar os dados categóricos em colunas binárias. Por exemplo:
* Azul = [1, 0, 0]
* Vermelho = [0, 1, 0]
* Verde = [0, 0, 1]
Dessa forma, os dados se tornam numéricos. Entretanto, ainda pode haver problemas no cálculo da distância euclidiana, pois quanto mais categorias (no caso do exemplo, quanto mais cores) houverem, o número de dimensões será cada vez maior, recaindo na maldição da dimensionalidade. Assim, para dados categóricos, a melhor estratégia talvez seja utilizar adaptações específicas do K-Means para esses tipos de dados.

> O K-Modes é um exemplo de variação do K-Means para dados categóricos. Em vez de utilizar a distância euclidiana, o algoritmo conta quantos atributos são diferentes entre os dados e representa cada grupo com o valor mais comuns (modos) de cada caracteristíca. Por exemplo, num agrupamento de pessoas, com os atributos "Cor favorita" e "Animal Favorito", se a maioria das pessoas prefere a cor azul e o animal cachorro, esses valores formam um cluster, assim como a cor e o animal menos preferidos, formam outro cluster. Quando um novo dado for analisado, o algoritmo verá o quão similar ele é de cada cluster para atribuí-lo ao mais parecido.

***

### Referências:
**BISHOP, C. M.**. Pattern Recognition and Machine Learning. New York: Springer, 2006. Disponível em: <https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf>. Acesso em: 30 mai. 2025.
**GEEKSFORGEEKS**. K-Mode Clustering in Python. Disponível em: <https://www.geeksforgeeks.org/k-mode-clustering-in-python/>. Acesso em: 5 jun. 2025.
**IBM**. K-means clustering. Disponível em: <https://www.ibm.com/think/topics/k-means-clustering>. Acesso em: 14 maio 2025.
**OLIVEIRA, Anderson F.**. *Agrupamento de dados utilizando algoritmo K-Modes*. 2019. Dissertação (Mestrado em Ciência da Computação) – Faculdade de Ciências, Universidade Estadual Paulista, São José do Rio Preto, 2019. Disponível em: <https://www.cc.faccamp.br/Dissertacoes/AndersonFranciscoOliveira.pdf>. Acesso em: 5 jun. 2025.
**SCIKIT-LEARN**. sklearn.cluster.KMeans. Disponível em: <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html>. Acesso em: 14 maio 2025.